{
  "trace": {
    "id": "ac29be521ce79d5df06d9081b50f2cb3",
    "projectId": "clkpwwm0m000gmm094odg11gi",
    "name": "QA-Chatbot",
    "timestamp": "2025-09-19T06:51:14.794Z",
    "environment": "default",
    "tags": [],
    "bookmarked": false,
    "release": null,
    "version": null,
    "userId": "u-KAD72D7_veypWq6gCXArG",
    "sessionId": "chat_53668515-211a-4a42-b069-554b7e1fcc60",
    "public": false,
    "input": "\"How to use langfuse for N+1 evaluations? \"",
    "output": "\"# N+1 evaluations in Langfuse\\n\\nDo you want to compare N model/prompt variants on the same dataset (A/B/N), each scored by one or more evaluators? And which SDK are you using (Python or JS/TS)?\\n\\n## Pattern: A/B/N (N variants) on one dataset\\nRun one experiment per variant on the same dataset, attach identical evaluators, then compare runs in the UI.\\n\\n_Prerequisites_\\n- Create/choose a dataset: see Datasets ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\\n\\n### Python (minimal)\\n_python_example.py_\\n```python\\nfrom langfuse import get_client, Evaluation\\n\\nlf = get_client()\\ndataset = lf.get_dataset(\\\"my-eval-dataset\\\")\\n\\ndef task(*, item, variant, **_):\\n    # call your app/model with the variant (prompt/model/params)\\n    # return plain output string/object\\n    return f\\\"answer by {variant} to: {item['input']}\\\"\\n\\ndef exact_match(*, input, output, expected_output, **_):\\n    v = 1.0 if expected_output and str(expected_output).lower() in str(output).lower() else 0.0\\n    return Evaluation(name=\\\"accuracy\\\", value=v)\\n\\nfor variant in [\\\"prompt_v1\\\", \\\"prompt_v2\\\", \\\"prompt_v3\\\"]:\\n    result = dataset.run_experiment(\\n        name=f\\\"abn-{variant}\\\",\\n        description=\\\"A/B/N on same dataset\\\",\\n        task=lambda *, item, **kw: task(item=item, variant=variant),\\n        evaluators=[exact_match],\\n        metadata={\\\"variant\\\": variant},\\n    )\\n    print(result.format())\\n```\\n\\n### JS/TS (minimal)\\n_abn.ts_\\n```ts\\nimport { LangfuseClient, ExperimentItem } from \\\"@langfuse/client\\\";\\n\\nconst lf = new LangfuseClient();\\nconst dataset = await lf.dataset.get(\\\"my-eval-dataset\\\");\\n\\nconst task = async (item: ExperimentItem, variant: string) => {\\n  return `answer by ${variant} to: ${item.input}`;\\n};\\n\\nconst accuracy = async ({ output, expectedOutput }: any) => ({\\n  name: \\\"accuracy\\\",\\n  value: expectedOutput && String(output).toLowerCase()\\n    .includes(String(expectedOutput).toLowerCase()) ? 1.0 : 0.0,\\n});\\n\\nfor (const variant of [\\\"prompt_v1\\\", \\\"prompt_v2\\\", \\\"prompt_v3\\\"]) {\\n  const res = await dataset.runExperiment({\\n    name: `abn-${variant}`,\\n    description: \\\"A/B/N on same dataset\\\",\\n    task: (item) => task(item, variant),\\n    evaluators: [accuracy],\\n    metadata: { variant },\\n  });\\n  console.log(await res.format());\\n}\\n```\\n\\n- This creates N dataset runs that are directly comparable. See Remote Dataset Runs ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\\n\\n## Add an ‚Äú+1‚Äù judge\\nAttach one or more LLM-as-a-judge or custom evaluators to all runs.\\n\\n- Python evaluator example: [Evaluations (Python)](https://langfuse.com/docs/observability/sdk/python/evaluation)\\n- JS Autoevals integration: [Remote Runs ‚Üí Autoevals](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)\\n\\nJS Autoevals (optional):\\n```ts\\nimport { Factuality } from \\\"autoevals\\\";\\nimport { createEvaluatorFromAutoevals } from \\\"@langfuse/client\\\";\\nconst factuality = createEvaluatorFromAutoevals(Factuality());\\n/* pass evaluators: [factuality, ...] in runExperiment */\\n```\\n\\n## Notes\\n- Running multiple predictions per item in a single run is not the recommended path; prefer one dataset run per variant and compare runs in the UI. The SDKs are built to ‚Äúrun multiple experiments on the same dataset in parallel‚Äù and attach the same evaluators for apples-to-apples comparison ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\\n\\n## Links\\n- Datasets: create and populate ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\\n- Remote Dataset Runs and evaluators ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run))\\n- Python evaluations API ([docs](https://langfuse.com/docs/observability/sdk/python/evaluation))\"",
    "metadata": "{\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"langfuse-sdk\",\"version\":\"4.0.0\",\"attributes\":{}}}",
    "createdAt": "2025-09-19T06:51:20.000Z",
    "updatedAt": "2025-09-19T06:52:04.274Z",
    "scores": [
      {
        "id": "99fcaa00-2a71-4f76-9571-9c007e7c2063",
        "timestamp": "2025-09-19T06:52:07.645Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "error-analysis_tool_getLangfuseDocsPage",
        "source": "EVAL",
        "authorUserId": null,
        "comment": "The user is not asking specifically about an integration guide or requesting documentation for a specific integration, but rather about a use case (N+1 evaluations). Therefore, using a RAG tool is acceptable since there may not be a single dedicated integration page for this topic.",
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-19T06:52:08.000Z",
        "updatedAt": "2025-09-19T06:52:07.714Z",
        "queueId": null,
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 1,
        "stringValue": null,
        "dataType": "NUMERIC"
      },
      {
        "id": "eaae5fdd-8e8a-4c76-92e7-fb4fcc9c10c5",
        "timestamp": "2025-09-19T06:52:06.236Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "error-analysis_OffTopic",
        "source": "EVAL",
        "authorUserId": null,
        "comment": "The agent's answer is fully focused on how to use Langfuse for N+1 evaluations, providing detailed and relevant information about Langfuse's features and usage, thus adhering to the scope criteria.",
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-19T06:52:07.000Z",
        "updatedAt": "2025-09-19T06:52:06.357Z",
        "queueId": null,
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 1,
        "stringValue": null,
        "dataType": "NUMERIC"
      },
      {
        "id": "b49127b6-60da-48ee-8f98-72ea83541bd6",
        "timestamp": "2025-09-19T06:52:06.042Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "is_question",
        "source": "EVAL",
        "authorUserId": null,
        "comment": "The message is asking for instructions on using langfuse for N+1 evaluations, which is a clear question.",
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-19T06:52:07.000Z",
        "updatedAt": "2025-09-19T06:52:06.126Z",
        "queueId": null,
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 1,
        "stringValue": null,
        "dataType": "NUMERIC"
      },
      {
        "id": "3fe829f5-8f43-4d2a-b5d5-43694333ed59",
        "timestamp": "2025-09-19T06:52:06.021Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "is_same_language",
        "source": "EVAL",
        "authorUserId": null,
        "comment": "Both texts discuss using Langfuse for N+1 evaluations, with Text 2 providing a detailed explanation and code examples, indicating they are about the same language/topic.",
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-19T06:52:07.000Z",
        "updatedAt": "2025-09-19T06:52:06.089Z",
        "queueId": null,
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 1,
        "stringValue": null,
        "dataType": "NUMERIC"
      },
      {
        "id": "069c1831-c0bd-42a1-be23-d36d6a31f730",
        "timestamp": "2025-09-19T06:52:05.620Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "helpfulness",
        "source": "EVAL",
        "authorUserId": null,
        "comment": "The generation directly repeats the user's query without providing any additional information or guidance. It does not address the query effectively or in a friendly and engaging manner.",
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-19T06:52:06.000Z",
        "updatedAt": "2025-09-19T06:52:05.685Z",
        "queueId": null,
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 0,
        "stringValue": null,
        "dataType": "NUMERIC"
      },
      {
        "id": "417046cd-aa15-4b91-878b-6d8afa90fcdd",
        "timestamp": "2025-09-19T06:51:45.584Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "contains-pii",
        "source": "EVAL",
        "authorUserId": null,
        "comment": "No PII is present in the text",
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-19T06:51:46.000Z",
        "updatedAt": "2025-09-19T06:51:45.687Z",
        "queueId": null,
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 0,
        "stringValue": null,
        "dataType": "NUMERIC"
      }
    ],
    "latency": 49.269,
    "observations": [
      {
        "id": "acf76ba316194a49",
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "GENERATION",
        "environment": "default",
        "parentObservationId": "2126822ce3db7c07",
        "startTime": "2025-09-19T06:51:15.777Z",
        "endTime": "2025-09-19T06:52:04.063Z",
        "name": "ai.streamText",
        "metadata": "{\"attributes\":{\"operation.name\":\"ai.streamText\",\"ai.operationId\":\"ai.streamText\",\"ai.model.provider\":\"openai.responses\",\"ai.model.id\":\"gpt-5\",\"ai.settings.maxRetries\":\"2\",\"ai.telemetry.metadata.langfusePrompt\":\"{\\\"name\\\":\\\"langfuse-docs-assistant-text\\\",\\\"prompt\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"version\\\":10,\\\"isFallback\\\":false,\\\"tags\\\":[],\\\"labels\\\":[\\\"production\\\",\\\"latest\\\"],\\\"type\\\":\\\"text\\\",\\\"config\\\":{\\\"model\\\":\\\"gpt-5\\\",\\\"reasoningSummary\\\":\\\"detailed\\\",\\\"textVerbosity\\\":\\\"low\\\",\\\"reasoningEffort\\\":\\\"low\\\"}}\",\"ai.prompt\":\"{\\\"system\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"messages\\\":[{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"**üëã Do you have any questions about Langfuse? Ask me!**\\\\n\\\\n_‚ö†Ô∏è Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\\\"}]},{\\\"role\\\":\\\"user\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"How to use langfuse for N+1 evaluations? \\\"}]}]}\",\"ai.response.finishReason\":\"stop\",\"ai.response.text\":\"# N+1 evaluations in Langfuse\\n\\nDo you want to compare N model/prompt variants on the same dataset (A/B/N), each scored by one or more evaluators? And which SDK are you using (Python or JS/TS)?\\n\\n## Pattern: A/B/N (N variants) on one dataset\\nRun one experiment per variant on the same dataset, attach identical evaluators, then compare runs in the UI.\\n\\n_Prerequisites_\\n- Create/choose a dataset: see Datasets ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\\n\\n### Python (minimal)\\n_python_example.py_\\n```python\\nfrom langfuse import get_client, Evaluation\\n\\nlf = get_client()\\ndataset = lf.get_dataset(\\\"my-eval-dataset\\\")\\n\\ndef task(*, item, variant, **_):\\n    # call your app/model with the variant (prompt/model/params)\\n    # return plain output string/object\\n    return f\\\"answer by {variant} to: {item['input']}\\\"\\n\\ndef exact_match(*, input, output, expected_output, **_):\\n    v = 1.0 if expected_output and str(expected_output).lower() in str(output).lower() else 0.0\\n    return Evaluation(name=\\\"accuracy\\\", value=v)\\n\\nfor variant in [\\\"prompt_v1\\\", \\\"prompt_v2\\\", \\\"prompt_v3\\\"]:\\n    result = dataset.run_experiment(\\n        name=f\\\"abn-{variant}\\\",\\n        description=\\\"A/B/N on same dataset\\\",\\n        task=lambda *, item, **kw: task(item=item, variant=variant),\\n        evaluators=[exact_match],\\n        metadata={\\\"variant\\\": variant},\\n    )\\n    print(result.format())\\n```\\n\\n### JS/TS (minimal)\\n_abn.ts_\\n```ts\\nimport { LangfuseClient, ExperimentItem } from \\\"@langfuse/client\\\";\\n\\nconst lf = new LangfuseClient();\\nconst dataset = await lf.dataset.get(\\\"my-eval-dataset\\\");\\n\\nconst task = async (item: ExperimentItem, variant: string) => {\\n  return `answer by ${variant} to: ${item.input}`;\\n};\\n\\nconst accuracy = async ({ output, expectedOutput }: any) => ({\\n  name: \\\"accuracy\\\",\\n  value: expectedOutput && String(output).toLowerCase()\\n    .includes(String(expectedOutput).toLowerCase()) ? 1.0 : 0.0,\\n});\\n\\nfor (const variant of [\\\"prompt_v1\\\", \\\"prompt_v2\\\", \\\"prompt_v3\\\"]) {\\n  const res = await dataset.runExperiment({\\n    name: `abn-${variant}`,\\n    description: \\\"A/B/N on same dataset\\\",\\n    task: (item) => task(item, variant),\\n    evaluators: [accuracy],\\n    metadata: { variant },\\n  });\\n  console.log(await res.format());\\n}\\n```\\n\\n- This creates N dataset runs that are directly comparable. See Remote Dataset Runs ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\\n\\n## Add an ‚Äú+1‚Äù judge\\nAttach one or more LLM-as-a-judge or custom evaluators to all runs.\\n\\n- Python evaluator example: [Evaluations (Python)](https://langfuse.com/docs/observability/sdk/python/evaluation)\\n- JS Autoevals integration: [Remote Runs ‚Üí Autoevals](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)\\n\\nJS Autoevals (optional):\\n```ts\\nimport { Factuality } from \\\"autoevals\\\";\\nimport { createEvaluatorFromAutoevals } from \\\"@langfuse/client\\\";\\nconst factuality = createEvaluatorFromAutoevals(Factuality());\\n/* pass evaluators: [factuality, ...] in runExperiment */\\n```\\n\\n## Notes\\n- Running multiple predictions per item in a single run is not the recommended path; prefer one dataset run per variant and compare runs in the UI. The SDKs are built to ‚Äúrun multiple experiments on the same dataset in parallel‚Äù and attach the same evaluators for apples-to-apples comparison ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\\n\\n## Links\\n- Datasets: create and populate ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\\n- Remote Dataset Runs and evaluators ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run))\\n- Python evaluations API ([docs](https://langfuse.com/docs/observability/sdk/python/evaluation))\",\"ai.response.providerMetadata\":\"{\\\"openai\\\":{\\\"responseId\\\":\\\"resp_68ccfd6dc2108194813f993e99c0af9805b0adca4249e773\\\",\\\"logprobs\\\":[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]}}\",\"ai.usage.inputTokens\":\"47743\",\"ai.usage.outputTokens\":\"1655\",\"ai.usage.totalTokens\":\"49398\",\"ai.usage.reasoningTokens\":\"640\",\"ai.usage.cachedInputTokens\":\"1024\"},\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"ai\",\"attributes\":{}}}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "input": "{\"system\":\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\n\\n## Guidelines\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\n- Answer the question using only this and only this information, outputted in markdown format.\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\n\\n## Scope & Safety\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\n\\n## Clarifying Questions\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\n\\n## Pricing & Discounts\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\n\\n## Markdown Style Guide\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\",\"messages\":[{\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"**üëã Do you have any questions about Langfuse? Ask me!**\\n\\n_‚ö†Ô∏è Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\"}]},{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"How to use langfuse for N+1 evaluations? \"}]}]}",
        "output": "# N+1 evaluations in Langfuse\n\nDo you want to compare N model/prompt variants on the same dataset (A/B/N), each scored by one or more evaluators? And which SDK are you using (Python or JS/TS)?\n\n## Pattern: A/B/N (N variants) on one dataset\nRun one experiment per variant on the same dataset, attach identical evaluators, then compare runs in the UI.\n\n_Prerequisites_\n- Create/choose a dataset: see Datasets ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\n\n### Python (minimal)\n_python_example.py_\n```python\nfrom langfuse import get_client, Evaluation\n\nlf = get_client()\ndataset = lf.get_dataset(\"my-eval-dataset\")\n\ndef task(*, item, variant, **_):\n    # call your app/model with the variant (prompt/model/params)\n    # return plain output string/object\n    return f\"answer by {variant} to: {item['input']}\"\n\ndef exact_match(*, input, output, expected_output, **_):\n    v = 1.0 if expected_output and str(expected_output).lower() in str(output).lower() else 0.0\n    return Evaluation(name=\"accuracy\", value=v)\n\nfor variant in [\"prompt_v1\", \"prompt_v2\", \"prompt_v3\"]:\n    result = dataset.run_experiment(\n        name=f\"abn-{variant}\",\n        description=\"A/B/N on same dataset\",\n        task=lambda *, item, **kw: task(item=item, variant=variant),\n        evaluators=[exact_match],\n        metadata={\"variant\": variant},\n    )\n    print(result.format())\n```\n\n### JS/TS (minimal)\n_abn.ts_\n```ts\nimport { LangfuseClient, ExperimentItem } from \"@langfuse/client\";\n\nconst lf = new LangfuseClient();\nconst dataset = await lf.dataset.get(\"my-eval-dataset\");\n\nconst task = async (item: ExperimentItem, variant: string) => {\n  return `answer by ${variant} to: ${item.input}`;\n};\n\nconst accuracy = async ({ output, expectedOutput }: any) => ({\n  name: \"accuracy\",\n  value: expectedOutput && String(output).toLowerCase()\n    .includes(String(expectedOutput).toLowerCase()) ? 1.0 : 0.0,\n});\n\nfor (const variant of [\"prompt_v1\", \"prompt_v2\", \"prompt_v3\"]) {\n  const res = await dataset.runExperiment({\n    name: `abn-${variant}`,\n    description: \"A/B/N on same dataset\",\n    task: (item) => task(item, variant),\n    evaluators: [accuracy],\n    metadata: { variant },\n  });\n  console.log(await res.format());\n}\n```\n\n- This creates N dataset runs that are directly comparable. See Remote Dataset Runs ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\n\n## Add an ‚Äú+1‚Äù judge\nAttach one or more LLM-as-a-judge or custom evaluators to all runs.\n\n- Python evaluator example: [Evaluations (Python)](https://langfuse.com/docs/observability/sdk/python/evaluation)\n- JS Autoevals integration: [Remote Runs ‚Üí Autoevals](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)\n\nJS Autoevals (optional):\n```ts\nimport { Factuality } from \"autoevals\";\nimport { createEvaluatorFromAutoevals } from \"@langfuse/client\";\nconst factuality = createEvaluatorFromAutoevals(Factuality());\n/* pass evaluators: [factuality, ...] in runExperiment */\n```\n\n## Notes\n- Running multiple predictions per item in a single run is not the recommended path; prefer one dataset run per variant and compare runs in the UI. The SDKs are built to ‚Äúrun multiple experiments on the same dataset in parallel‚Äù and attach the same evaluators for apples-to-apples comparison ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\n\n## Links\n- Datasets: create and populate ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\n- Remote Dataset Runs and evaluators ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run))\n- Python evaluations API ([docs](https://langfuse.com/docs/observability/sdk/python/evaluation))",
        "modelParameters": {
          "maxSteps": null,
          "toolChoice": null,
          "maxTokens": null,
          "finishReason": null,
          "system": "openai.responses",
          "maxRetries": "2",
          "mode": null,
          "temperature": null
        },
        "completionStartTime": null,
        "promptId": "5422a9e9-19cf-4ba8-bbd4-2fe0108eefc5",
        "createdAt": "2025-09-19T06:52:04.200Z",
        "updatedAt": "2025-09-19T06:52:04.204Z",
        "usageDetails": {},
        "costDetails": {},
        "providedCostDetails": {},
        "model": null,
        "internalModelId": null,
        "promptName": "langfuse-docs-assistant-text",
        "promptVersion": 10,
        "latency": 48286,
        "timeToFirstToken": null,
        "inputCost": null,
        "outputCost": null,
        "totalCost": 0,
        "inputUsage": 0,
        "outputUsage": 0,
        "totalUsage": 0,
        "internalModel": null
      },
      {
        "id": "fe102fec4ed78754",
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "GENERATION",
        "environment": "default",
        "parentObservationId": "acf76ba316194a49",
        "startTime": "2025-09-19T06:51:25.036Z",
        "endTime": "2025-09-19T06:52:04.056Z",
        "name": "ai.streamText.doStream",
        "metadata": "{}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "modelParameters": {
          "maxSteps": null,
          "toolChoice": "{\"type\":\"auto\"}",
          "maxTokens": null,
          "finishReason": "stop",
          "system": "openai.responses",
          "maxRetries": "2",
          "mode": null,
          "temperature": null
        },
        "completionStartTime": "2025-09-19T06:51:26.084Z",
        "promptId": "5422a9e9-19cf-4ba8-bbd4-2fe0108eefc5",
        "createdAt": "2025-09-19T06:52:04.200Z",
        "updatedAt": "2025-09-19T06:52:04.202Z",
        "usageDetails": {
          "input": 46619,
          "output": 1576,
          "input_cached_tokens": 0,
          "total": 48195
        },
        "costDetails": {
          "input": 0.05827375,
          "output": 0.01576,
          "input_cached_tokens": 0,
          "total": 0.07403375
        },
        "providedCostDetails": {},
        "model": "gpt-5",
        "internalModelId": "38c3822a-09a3-457b-b200-2c6f17f7cf2f",
        "promptName": "langfuse-docs-assistant-text",
        "promptVersion": 10,
        "latency": 39020,
        "timeToFirstToken": 1.048,
        "inputCost": 0.05827375,
        "outputCost": 0.01576,
        "totalCost": 0.07403375,
        "inputUsage": 46619,
        "outputUsage": 1576,
        "totalUsage": 48195
      },
      {
        "id": "2126822ce3db7c07",
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "GENERATION",
        "environment": "default",
        "parentObservationId": null,
        "startTime": "2025-09-19T06:51:14.794Z",
        "endTime": "2025-09-19T06:52:04.062Z",
        "name": "handle-chatbot-message",
        "metadata": "{\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"langfuse-sdk\",\"version\":\"4.0.0\",\"attributes\":{}}}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "input": "How to use langfuse for N+1 evaluations? ",
        "output": "# N+1 evaluations in Langfuse\n\nDo you want to compare N model/prompt variants on the same dataset (A/B/N), each scored by one or more evaluators? And which SDK are you using (Python or JS/TS)?\n\n## Pattern: A/B/N (N variants) on one dataset\nRun one experiment per variant on the same dataset, attach identical evaluators, then compare runs in the UI.\n\n_Prerequisites_\n- Create/choose a dataset: see Datasets ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\n\n### Python (minimal)\n_python_example.py_\n```python\nfrom langfuse import get_client, Evaluation\n\nlf = get_client()\ndataset = lf.get_dataset(\"my-eval-dataset\")\n\ndef task(*, item, variant, **_):\n    # call your app/model with the variant (prompt/model/params)\n    # return plain output string/object\n    return f\"answer by {variant} to: {item['input']}\"\n\ndef exact_match(*, input, output, expected_output, **_):\n    v = 1.0 if expected_output and str(expected_output).lower() in str(output).lower() else 0.0\n    return Evaluation(name=\"accuracy\", value=v)\n\nfor variant in [\"prompt_v1\", \"prompt_v2\", \"prompt_v3\"]:\n    result = dataset.run_experiment(\n        name=f\"abn-{variant}\",\n        description=\"A/B/N on same dataset\",\n        task=lambda *, item, **kw: task(item=item, variant=variant),\n        evaluators=[exact_match],\n        metadata={\"variant\": variant},\n    )\n    print(result.format())\n```\n\n### JS/TS (minimal)\n_abn.ts_\n```ts\nimport { LangfuseClient, ExperimentItem } from \"@langfuse/client\";\n\nconst lf = new LangfuseClient();\nconst dataset = await lf.dataset.get(\"my-eval-dataset\");\n\nconst task = async (item: ExperimentItem, variant: string) => {\n  return `answer by ${variant} to: ${item.input}`;\n};\n\nconst accuracy = async ({ output, expectedOutput }: any) => ({\n  name: \"accuracy\",\n  value: expectedOutput && String(output).toLowerCase()\n    .includes(String(expectedOutput).toLowerCase()) ? 1.0 : 0.0,\n});\n\nfor (const variant of [\"prompt_v1\", \"prompt_v2\", \"prompt_v3\"]) {\n  const res = await dataset.runExperiment({\n    name: `abn-${variant}`,\n    description: \"A/B/N on same dataset\",\n    task: (item) => task(item, variant),\n    evaluators: [accuracy],\n    metadata: { variant },\n  });\n  console.log(await res.format());\n}\n```\n\n- This creates N dataset runs that are directly comparable. See Remote Dataset Runs ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\n\n## Add an ‚Äú+1‚Äù judge\nAttach one or more LLM-as-a-judge or custom evaluators to all runs.\n\n- Python evaluator example: [Evaluations (Python)](https://langfuse.com/docs/observability/sdk/python/evaluation)\n- JS Autoevals integration: [Remote Runs ‚Üí Autoevals](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)\n\nJS Autoevals (optional):\n```ts\nimport { Factuality } from \"autoevals\";\nimport { createEvaluatorFromAutoevals } from \"@langfuse/client\";\nconst factuality = createEvaluatorFromAutoevals(Factuality());\n/* pass evaluators: [factuality, ...] in runExperiment */\n```\n\n## Notes\n- Running multiple predictions per item in a single run is not the recommended path; prefer one dataset run per variant and compare runs in the UI. The SDKs are built to ‚Äúrun multiple experiments on the same dataset in parallel‚Äù and attach the same evaluators for apples-to-apples comparison ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\n\n## Links\n- Datasets: create and populate ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\n- Remote Dataset Runs and evaluators ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run))\n- Python evaluations API ([docs](https://langfuse.com/docs/observability/sdk/python/evaluation))",
        "modelParameters": {},
        "completionStartTime": null,
        "promptId": null,
        "createdAt": "2025-09-19T06:52:04.200Z",
        "updatedAt": "2025-09-19T06:52:04.200Z",
        "usageDetails": {},
        "costDetails": {},
        "providedCostDetails": {},
        "model": null,
        "internalModelId": null,
        "promptName": null,
        "promptVersion": null,
        "latency": 49268,
        "timeToFirstToken": null,
        "inputCost": null,
        "outputCost": null,
        "totalCost": 0,
        "inputUsage": 0,
        "outputUsage": 0,
        "totalUsage": 0,
        "internalModel": null
      },
      {
        "id": "741b099d7586a62f",
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "GENERATION",
        "environment": "default",
        "parentObservationId": "acf76ba316194a49",
        "startTime": "2025-09-19T06:51:15.780Z",
        "endTime": "2025-09-19T06:51:25.033Z",
        "name": "ai.streamText.doStream",
        "metadata": "{\"tools\":[\"{\\\"type\\\":\\\"function\\\",\\\"name\\\":\\\"searchLangfuseDocs\\\",\\\"description\\\":\\\"Semantic search (RAG) over the Langfuse documentation. Use this whenever the user asks a broader question that cannot be answered by a specific single page. Returns a concise answer synthesized from relevant docs. The raw provider response is included in _meta. Prefer this before guessing. If a specific page is needed call getLangfuseDocsPage first.\\\",\\\"inputSchema\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The user‚Äôs question in natural language. Include helpful context like SDK/language (e.g., Python v3, JS v4), self-hosted vs cloud, and short error messages (trim long stack traces). Keep under ~600 characters.\\\"}},\\\"required\\\":[\\\"query\\\"],\\\"additionalProperties\\\":false,\\\"$schema\\\":\\\"http://json-schema.org/draft-07/schema#\\\"}}\",\"{\\\"type\\\":\\\"function\\\",\\\"name\\\":\\\"getLangfuseDocsPage\\\",\\\"description\\\":\\\"Fetch the raw Markdown for a single Langfuse docs page. Accepts a docs path (e.g., /docs/observability/overview) or a full https://langfuse.com URL. Returns the exact Markdown (may include front matter). Use when you need a specific page content (Integration, Features, API, etc.) or code samples. Prefer searchLangfuseDocs for broader questions where there is not one specific page about it.\\\",\\\"inputSchema\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"pathOrUrl\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"Docs path starting with ‚Äú/‚Äù (e.g., /docs/observability/overview) or a full URL on https://langfuse.com. Do not include anchors (#...) or queries (?foo=bar) ‚Äî they will be ignored.\\\"}},\\\"required\\\":[\\\"pathOrUrl\\\"],\\\"additionalProperties\\\":false,\\\"$schema\\\":\\\"http://json-schema.org/draft-07/schema#\\\"}}\",\"{\\\"type\\\":\\\"function\\\",\\\"name\\\":\\\"getLangfuseOverview\\\",\\\"description\\\":\\\"Get a high-level, machine-readable index by downloading https://langfuse.com/llms.txt. Use this at the start of a session when needed to discover key docs endpoints or to seed follow-up calls to searchLangfuseDocs or getLangfuseDocsPage.Returns the plain text contents of llms.txt. Avoid repeated calls within the same session.\\\",\\\"inputSchema\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{},\\\"additionalProperties\\\":false,\\\"$schema\\\":\\\"http://json-schema.org/draft-07/schema#\\\"}}\"],\"attributes\":{\"operation.name\":\"ai.streamText.doStream\",\"ai.operationId\":\"ai.streamText.doStream\",\"ai.model.provider\":\"openai.responses\",\"ai.model.id\":\"gpt-5\",\"ai.settings.maxRetries\":\"2\",\"ai.telemetry.metadata.langfusePrompt\":\"{\\\"name\\\":\\\"langfuse-docs-assistant-text\\\",\\\"prompt\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"version\\\":10,\\\"isFallback\\\":false,\\\"tags\\\":[],\\\"labels\\\":[\\\"production\\\",\\\"latest\\\"],\\\"type\\\":\\\"text\\\",\\\"config\\\":{\\\"model\\\":\\\"gpt-5\\\",\\\"reasoningSummary\\\":\\\"detailed\\\",\\\"textVerbosity\\\":\\\"low\\\",\\\"reasoningEffort\\\":\\\"low\\\"}}\",\"ai.prompt.messages\":\"[{\\\"role\\\":\\\"system\\\",\\\"content\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\\\"},{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"**üëã Do you have any questions about Langfuse? Ask me!**\\\\n\\\\n_‚ö†Ô∏è Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\\\"}]},{\\\"role\\\":\\\"user\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"How to use langfuse for N+1 evaluations? \\\"}]}]\",\"ai.prompt.tools\":\"[\\\"{\\\\\\\"type\\\\\\\":\\\\\\\"function\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"searchLangfuseDocs\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Semantic search (RAG) over the Langfuse documentation. Use this whenever the user asks a broader question that cannot be answered by a specific single page. Returns a concise answer synthesized from relevant docs. The raw provider response is included in _meta. Prefer this before guessing. If a specific page is needed call getLangfuseDocsPage first.\\\\\\\",\\\\\\\"inputSchema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"query\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The user‚Äôs question in natural language. Include helpful context like SDK/language (e.g., Python v3, JS v4), self-hosted vs cloud, and short error messages (trim long stack traces). Keep under ~600 characters.\\\\\\\"}},\\\\\\\"required\\\\\\\":[\\\\\\\"query\\\\\\\"],\\\\\\\"additionalProperties\\\\\\\":false,\\\\\\\"$schema\\\\\\\":\\\\\\\"http://json-schema.org/draft-07/schema#\\\\\\\"}}\\\",\\\"{\\\\\\\"type\\\\\\\":\\\\\\\"function\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"getLangfuseDocsPage\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Fetch the raw Markdown for a single Langfuse docs page. Accepts a docs path (e.g., /docs/observability/overview) or a full https://langfuse.com URL. Returns the exact Markdown (may include front matter). Use when you need a specific page content (Integration, Features, API, etc.) or code samples. Prefer searchLangfuseDocs for broader questions where there is not one specific page about it.\\\\\\\",\\\\\\\"inputSchema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"pathOrUrl\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Docs path starting with ‚Äú/‚Äù (e.g., /docs/observability/overview) or a full URL on https://langfuse.com. Do not include anchors (#...) or queries (?foo=bar) ‚Äî they will be ignored.\\\\\\\"}},\\\\\\\"required\\\\\\\":[\\\\\\\"pathOrUrl\\\\\\\"],\\\\\\\"additionalProperties\\\\\\\":false,\\\\\\\"$schema\\\\\\\":\\\\\\\"http://json-schema.org/draft-07/schema#\\\\\\\"}}\\\",\\\"{\\\\\\\"type\\\\\\\":\\\\\\\"function\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"getLangfuseOverview\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Get a high-level, machine-readable index by downloading https://langfuse.com/llms.txt. Use this at the start of a session when needed to discover key docs endpoints or to seed follow-up calls to searchLangfuseDocs or getLangfuseDocsPage.Returns the plain text contents of llms.txt. Avoid repeated calls within the same session.\\\\\\\",\\\\\\\"inputSchema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{},\\\\\\\"additionalProperties\\\\\\\":false,\\\\\\\"$schema\\\\\\\":\\\\\\\"http://json-schema.org/draft-07/schema#\\\\\\\"}}\\\"]\",\"ai.prompt.toolChoice\":\"{\\\"type\\\":\\\"auto\\\"}\",\"gen_ai.system\":\"openai.responses\",\"gen_ai.request.model\":\"gpt-5\",\"ai.response.msToFirstChunk\":\"773.1660110000521\",\"ai.response.msToFinish\":\"9247.992744000047\",\"ai.response.avgOutputTokensPerSecond\":\"8.542394245632812\",\"ai.response.finishReason\":\"tool-calls\",\"ai.response.text\":\"I‚Äôll look up the Langfuse docs for ‚ÄúN+1 evaluations‚Äù to share the recommended approach and a minimal example.\",\"ai.response.toolCalls\":\"[{\\\"type\\\":\\\"tool-call\\\",\\\"toolCallId\\\":\\\"call_AOk8le1i4tNgzuTtytNcqD5Q\\\",\\\"toolName\\\":\\\"searchLangfuseDocs\\\",\\\"input\\\":{\\\"query\\\":\\\"N+1 evaluations Langfuse how to implement n+1 evaluation pattern batch eval per prediction vs per dataset Python JS\\\"},\\\"providerMetadata\\\":{\\\"openai\\\":{\\\"itemId\\\":\\\"fc_68ccfd67f814819498c9c64fbb7e948c05b0adca4249e773\\\"}},\\\"dynamic\\\":true}]\",\"ai.response.id\":\"resp_68ccfd645cb08194a7464f25c6b45fe705b0adca4249e773\",\"ai.response.model\":\"gpt-5-2025-08-07\",\"ai.response.timestamp\":\"2025-09-19T06:51:16.000Z\",\"ai.response.providerMetadata\":\"{\\\"openai\\\":{\\\"responseId\\\":\\\"resp_68ccfd645cb08194a7464f25c6b45fe705b0adca4249e773\\\",\\\"logprobs\\\":[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]}}\",\"ai.usage.inputTokens\":\"1124\",\"ai.usage.outputTokens\":\"79\",\"ai.usage.totalTokens\":\"1203\",\"ai.usage.reasoningTokens\":\"{\\\"intValue\\\":0}\",\"ai.usage.cachedInputTokens\":\"1024\",\"gen_ai.response.finish_reasons\":\"[\\\"tool-calls\\\"]\",\"gen_ai.response.id\":\"resp_68ccfd645cb08194a7464f25c6b45fe705b0adca4249e773\",\"gen_ai.response.model\":\"gpt-5-2025-08-07\",\"gen_ai.usage.input_tokens\":\"1124\",\"gen_ai.usage.output_tokens\":\"79\"},\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"ai\",\"attributes\":{}}}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "input": "[{\"role\":\"system\",\"content\":\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\n\\n## Guidelines\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\n- Answer the question using only this and only this information, outputted in markdown format.\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\n\\n## Scope & Safety\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\n\\n## Clarifying Questions\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\n\\n## Pricing & Discounts\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\n\\n## Markdown Style Guide\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\"},{\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"**üëã Do you have any questions about Langfuse? Ask me!**\\n\\n_‚ö†Ô∏è Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\"}]},{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"How to use langfuse for N+1 evaluations? \"}]}]",
        "output": "I‚Äôll look up the Langfuse docs for ‚ÄúN+1 evaluations‚Äù to share the recommended approach and a minimal example.",
        "modelParameters": {
          "maxSteps": null,
          "toolChoice": "{\"type\":\"auto\"}",
          "maxTokens": null,
          "finishReason": "tool-calls",
          "system": "openai.responses",
          "maxRetries": "2",
          "mode": null,
          "temperature": null
        },
        "completionStartTime": "2025-09-19T06:51:16.554Z",
        "promptId": "5422a9e9-19cf-4ba8-bbd4-2fe0108eefc5",
        "createdAt": "2025-09-19T06:51:30.170Z",
        "updatedAt": "2025-09-19T06:51:30.172Z",
        "usageDetails": {
          "input": 1124,
          "output": 79,
          "output_reasoning_tokens": 0,
          "total": 1203
        },
        "costDetails": {
          "input": 0.001405,
          "output": 0.00079,
          "output_reasoning_tokens": 0,
          "total": 0.002195
        },
        "providedCostDetails": {},
        "model": "gpt-5",
        "internalModelId": "38c3822a-09a3-457b-b200-2c6f17f7cf2f",
        "promptName": "langfuse-docs-assistant-text",
        "promptVersion": 10,
        "latency": 9253,
        "timeToFirstToken": 0.774,
        "inputCost": 0.001405,
        "outputCost": 0.00079,
        "totalCost": 0.002195,
        "inputUsage": 1124,
        "outputUsage": 79,
        "totalUsage": 1203,
        "internalModel": "38c3822a-09a3-457b-b200-2c6f17f7cf2f"
      },
      {
        "id": "e704e7eaa8a70d4d",
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "TOOL",
        "environment": "default",
        "parentObservationId": "acf76ba316194a49",
        "startTime": "2025-09-19T06:51:20.208Z",
        "endTime": "2025-09-19T06:51:25.008Z",
        "name": "ai.toolCall",
        "metadata": "{\"attributes\":{\"operation.name\":\"ai.toolCall\",\"ai.operationId\":\"ai.toolCall\",\"ai.toolCall.name\":\"searchLangfuseDocs\",\"ai.toolCall.id\":\"call_AOk8le1i4tNgzuTtytNcqD5Q\",\"ai.toolCall.args\":\"{\\\"query\\\":\\\"N+1 evaluations Langfuse how to implement n+1 evaluation pattern batch eval per prediction vs per dataset Python JS\\\"}\",\"ai.toolCall.result\":\"{\\\"_meta\\\":{\\\"id\\\":\\\"inkeep-chatcmpl-40fc3a6d-a341-4906-ba8f-eecf1570e0e3\\\",\\\"model\\\":\\\"inkeep-rag\\\",\\\"choices\\\":[{\\\"finish_reason\\\":\\\"stop\\\",\\\"log_probs\\\":null,\\\"index\\\":0,\\\"message\\\":{\\\"content\\\":\\\"{\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Troubleshooting and FAQ\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[API - /scores not respecting value when operator '='](https://github.com/orgs/langfuse/discussions/8770 \\\\\\\\\\\\\\\"Langfuse Support: API - /scores not respecting value when operator '='\\\\\\\\\\\\\\\")\\\\\\\\n[Does LangFuse support evaluations on an existing dataset (.csv)](https://github.com/orgs/langfuse/discussions/8665 \\\\\\\\\\\\\\\"Langfuse Support: Does LangFuse support evaluations on an existing dataset \\\\\\\\\\\\\\\\(.csv\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[deleting evaluators](https://github.com/orgs/langfuse/discussions/8640 \\\\\\\\\\\\\\\"Langfuse Support: deleting evaluators\\\\\\\\\\\\\\\")\\\\\\\\n[Not able to import my LLM-as-a-Judge evals](https://github.com/orgs/langfuse/discussions/8636 \\\\\\\\\\\\\\\"Langfuse Support: Not able to import my LLM-as-a-Judge evals\\\\\\\\\\\\\\\")\\\\\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590 \\\\\\\\\\\\\\\"Langfuse Support: How to get experiment run scores programmatically?\\\\\\\\\\\\\\\")\\\\\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556 \\\\\\\\\\\\\\\"Langfuse Support: Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset runs restore from backups](https://github.com/orgs/langfuse/discussions/8534 \\\\\\\\\\\\\\\"Langfuse Support: Dataset runs restore from backups\\\\\\\\\\\\\\\")\\\\\\\\n[Getting scores efficiently via API for analytics purposes](https://github.com/orgs/langfuse/discussions/8520 \\\\\\\\\\\\\\\"Langfuse Support: Getting scores efficiently via API for analytics purposes\\\\\\\\\\\\\\\")\\\\\\\\n[run experiment on dataset](https://github.com/orgs/langfuse/discussions/8433 \\\\\\\\\\\\\\\"Langfuse Support: run experiment on dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Experiments on Datasets with Human Annotated Labels?](https://github.com/orgs/langfuse/discussions/8414 \\\\\\\\\\\\\\\"Langfuse Support: Experiments on Datasets with Human Annotated Labels?\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[How to Recalculate Total Score on Dashboard After Updating User-Defined Model?](https://github.com/orgs/langfuse/discussions/8375 \\\\\\\\\\\\\\\"Langfuse Support: How to Recalculate Total Score on Dashboard After Updating User-Defined Model?\\\\\\\\\\\\\\\")\\\\\\\\n[How to filter by Categorical Scores in custom dashboard?](https://github.com/orgs/langfuse/discussions/8356 \\\\\\\\\\\\\\\"Langfuse Support: How to filter by Categorical Scores in custom dashboard?\\\\\\\\\\\\\\\")\\\\\\\\n[Running scheduled evals utilising LangFuse Datasets & Evaluators](https://github.com/orgs/langfuse/discussions/8355 \\\\\\\\\\\\\\\"Langfuse Support: Running scheduled evals utilising LangFuse Datasets & Evaluators\\\\\\\\\\\\\\\")\\\\\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\\\\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\\\\\\\\\(\\\\\\\\\\\\\\\\)` method\\\\\\\\\\\\\\\")\\\\\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\\\\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\\\\\\\\\")\\\\\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\\\\\\\\\"Langfuse Support: \\\\\\\\\\\\\\\\[Experiment\\\\\\\\\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\\\\\\\\\")\\\\\\\\n[Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\\\\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing\\\\\\\\\\\\\\\")\\\\\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\\\\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[2 traces generated instead of 1](https://github.com/orgs/langfuse/discussions/2244 \\\\\\\\\\\\\\\"Langfuse Support: 2 traces generated instead of 1\\\\\\\\\\\\\\\")\\\\\\\\n[Evaluations Not Available in Self-Hosted Version?](https://github.com/orgs/langfuse/discussions/2130 \\\\\\\\\\\\\\\"Langfuse Support: Evaluations Not Available in Self-Hosted Version?\\\\\\\\\\\\\\\")\\\\\\\\n[Deleting Duplicate Items in a Dataset](https://github.com/orgs/langfuse/discussions/2099 \\\\\\\\\\\\\\\"Langfuse Support: Deleting Duplicate Items in a Dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Availability of evals when self-hosting](https://github.com/orgs/langfuse/discussions/2042 \\\\\\\\\\\\\\\"Langfuse Support: Availability of evals when self-hosting\\\\\\\\\\\\\\\")\\\\\\\\n[How to utilize a dataset w/ typescript and langchain integration](https://github.com/orgs/langfuse/discussions/1969 \\\\\\\\\\\\\\\"Langfuse Support: How to utilize a dataset w/ typescript and langchain integration\\\\\\\\\\\\\\\")\\\\\\\\n[Scoring a trace after the LLM chain returns](https://github.com/orgs/langfuse/discussions/1610 \\\\\\\\\\\\\\\"Langfuse Support: Scoring a trace after the LLM chain returns\\\\\\\\\\\\\\\")\\\\\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486 \\\\\\\\\\\\\\\"Langfuse Support: Update/delete score using python sdk\\\\\\\\\\\\\\\")\\\\\\\\n[Linking dataset run items with existing callback handler](https://github.com/orgs/langfuse/discussions/1445 \\\\\\\\\\\\\\\"Langfuse Support: Linking dataset run items with existing callback handler\\\\\\\\\\\\\\\")\\\\\\\\n[Datasets list / by id](https://github.com/orgs/langfuse/discussions/1420 \\\\\\\\\\\\\\\"Langfuse Support: Datasets list / by id\\\\\\\\\\\\\\\")\\\\\\\\n[Run items not appearing when linking to a trace and not a span or a generation](https://github.com/orgs/langfuse/discussions/1357 \\\\\\\\\\\\\\\"Langfuse Support: Run items not appearing when linking to a trace and not a span or a generation\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Option to run Experiments without traces](https://github.com/orgs/langfuse/discussions/8133 \\\\\\\\\\\\\\\"Langfuse Ideas: Option to run Experiments without traces\\\\\\\\\\\\\\\")\\\\\\\\n[Auto-generate dataset items](https://github.com/orgs/langfuse/discussions/8126 \\\\\\\\\\\\\\\"Langfuse Ideas: Auto-generate dataset items\\\\\\\\\\\\\\\")\\\\\\\\n[feature: support for creation of custom) model adapters](https://github.com/orgs/langfuse/discussions/8123 \\\\\\\\\\\\\\\"Langfuse Ideas: feature: support for creation of custom\\\\\\\\\\\\\\\\) model adapters\\\\\\\\\\\\\\\")\\\\\\\\n[Prompt experiment result download.](https://github.com/orgs/langfuse/discussions/8120 \\\\\\\\\\\\\\\"Langfuse Ideas: Prompt experiment result download.\\\\\\\\\\\\\\\")\\\\\\\\n[Multiple predictions per-item in a single run](https://github.com/orgs/langfuse/discussions/8040 \\\\\\\\\\\\\\\"Langfuse Ideas: Multiple predictions per-item in a single run\\\\\\\\\\\\\\\")\\\\\\\\n[Add evaluator(s) to python SDK (FernLangfuse)](https://github.com/orgs/langfuse/discussions/8018 \\\\\\\\\\\\\\\"Langfuse Ideas: Add evaluator\\\\\\\\\\\\\\\\(s\\\\\\\\\\\\\\\\) to python SDK \\\\\\\\\\\\\\\\(FernLangfuse\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[feat(dataset-runs): return all dataset run item scores in a given run](https://github.com/orgs/langfuse/discussions/8011 \\\\\\\\\\\\\\\"Langfuse Ideas: feat\\\\\\\\\\\\\\\\(dataset-runs\\\\\\\\\\\\\\\\): return all dataset run item scores in a given run\\\\\\\\\\\\\\\")\\\\\\\\n[Delete an evaluator from the evaluator library](https://github.com/orgs/langfuse/discussions/7960 \\\\\\\\\\\\\\\"Langfuse Ideas: Delete an evaluator from the evaluator library\\\\\\\\\\\\\\\")\\\\\\\\n[Archive or Delete Evaluators from the Evaluator Library](https://github.com/orgs/langfuse/discussions/7868 \\\\\\\\\\\\\\\"Langfuse Ideas: Archive or Delete Evaluators from the Evaluator Library\\\\\\\\\\\\\\\")\\\\\\\\n[Edit dataset run name, description](https://github.com/orgs/langfuse/discussions/7814 \\\\\\\\\\\\\\\"Langfuse Ideas: Edit dataset run name, description\\\\\\\\\\\\\\\")\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Datasets\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Datasets - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")Dataset RunsDatasets\\\\\\\\nCopy as Markdown\\\\\\\\n# Datasets\\\\\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\\\\\n## Why use datasets?\\\\\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\\\\\n  * Create test cases for your application with real production traces\\\\\\\\n  * Collaboratively create and collect dataset items with your team\\\\\\\\n  * Have a single source of truth for your test data\\\\\\\\n## Get Started\\\\\\\\n### Creating a dataset\\\\\\\\nDatasets have a name which is unique within a project.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset(\\\\\\\\n name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # optional description\\\\\\\\n description=\\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n # optional metadata\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"author\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"date\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { LangfuseClient } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\nconst langfuse = new LangfuseClient()\\\\\\\\n \\\\\\\\nawait langfuse.api.datasets.create({\\\\\\\\n name: \\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n // optional description\\\\\\\\n description: \\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n // optional metadata\\\\\\\\n metadata: {\\\\\\\\n author: \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n date: \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n type: \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\",\\\\\\\\n },\\\\\\\\n});\\\\\\\\n```\\\\\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\\\\\n### Create new dataset items\\\\\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset_item(\\\\\\\\n dataset_name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # any python object or value, optional\\\\\\\\n input={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # any python object or value, optional\\\\\\\\n expected_output={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # metadata, optional\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"llama3\\\\\\\\\\\\\\\",\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/observability/sdk/python/evaluation\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Evaluations\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Evaluations with the Langfuse Python SDK - Langfuse\\\\\\\\nDocs[Observability](/docs/observability/overview \\\\\\\\\\\\\\\"Observability\\\\\\\\\\\\\\\")\\\\\\\\n[SDKs](/docs/observability/sdk/overview \\\\\\\\\\\\\\\"SDKs\\\\\\\\\\\\\\\")\\\\\\\\n[Python](/docs/observability/sdk/python/overview \\\\\\\\\\\\\\\"Python\\\\\\\\\\\\\\\")Evaluation\\\\\\\\nCopy as Markdown\\\\\\\\n# Evaluations\\\\\\\\nThe Python SDK provides ways to evaluate your application. You can add custom scores to your traces and observations, or use the SDK to execute Dataset Runs.\\\\\\\\nThis page shows the evaluation methods that are supported by the Python SDK. Please refer to the [Evaluation documentation](/docs/evaluation/overview) for more information on how to evaluate your application in Langfuse.\\\\\\\\n## Create Scores\\\\\\\\nLangfuseSpan / LangfuseGeneration object methodsContext-aware methodsLow-level method\\\\\\\\n  * `span_or_generation_obj.score()`: Scores the specific observation object.\\\\\\\\n  * `span_or_generation_obj.score_trace()`: Scores the entire trace to which the object belongs.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nwith langfuse.start_as_current_generation(name=\\\\\\\\\\\\\\\"summary_generation\\\\\\\\\\\\\\\") as gen:\\\\\\\\n # ... LLM call ...\\\\\\\\n gen.update(output=\\\\\\\\\\\\\\\"summary text...\\\\\\\\\\\\\\\")\\\\\\\\n # Score this specific generation\\\\\\\\n gen.score(name=\\\\\\\\\\\\\\\"conciseness\\\\\\\\\\\\\\\", value=0.8, data_type=\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\")\\\\\\\\n # Score the overall trace\\\\\\\\n gen.score_trace(name=\\\\\\\\\\\\\\\"user_feedback_rating\\\\\\\\\\\\\\\", value=\\\\\\\\\\\\\\\"positive\\\\\\\\\\\\\\\", data_type=\\\\\\\\\\\\\\\"CATEGORICAL\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\n  * `langfuse.score_current_span()`: Scores the currently active observation in the context.\\\\\\\\n  * `langfuse.score_current_trace()`: Scores the trace of the currently active observation.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nwith langfuse.start_as_current_span(name=\\\\\\\\\\\\\\\"complex_task\\\\\\\\\\\\\\\") as task_span:\\\\\\\\n # ... perform task ...\\\\\\\\n langfuse.score_current_span(name=\\\\\\\\\\\\\\\"task_component_quality\\\\\\\\\\\\\\\", value=True, data_type=\\\\\\\\\\\\\\\"BOOLEAN\\\\\\\\\\\\\\\")\\\\\\\\n # ...\\\\\\\\n if task_is_fully_successful:\\\\\\\\n langfuse.score_current_trace(name=\\\\\\\\\\\\\\\"overall_success\\\\\\\\\\\\\\\", value=1.0, data_type=\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n  * Creates a score for a specified `trace_id` and optionally `observation_id`.\\\\\\\\n  * Useful when IDs are known, or for scoring after the trace/observation has completed.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nlangfuse.create_score(\\\\\\\\n name=\\\\\\\\\\\\\\\"fact_check_accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value=0.95, ///# Can be float for NUMERIC/BOOLEAN, string for CATEGORICAL\\\\\\\\n trace_id=\\\\\\\\\\\\\\\"abcdef1234567890abcdef1234567890\\\\\\\\\\\\\\\",\\\\\\\\n observation_id=\\\\\\\\\\\\\\\"1234567890abcdef\\\\\\\\\\\\\\\", ///# Optional: if scoring a specific observation\\\\\\\\n session_id=\\\\\\\\\\\\\\\"session_123\\\\\\\\\\\\\\\", ///# Optional: if scoring a specific session\\\\\\\\n data_type=\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\", ///# \\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"BOOLEAN\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"CATEGORICAL\\\\\\\\\\\\\\\"\\\\\\\\n comment=\\\\\\\\\\\\\\\"Source verified for 95% of claims.\\\\\\\\\\\\\\\"\\\\\\\\n)\\\\\\\\n```\\\\\\\\n**Score Parameters:**\\\\\\\\nParameter| Type| Description  \\\\\\\\n---|---|---  \\\\\\\\n`name`| `str`| Name of the score (e.g., \\\\\\\\u201crelevance\\\\\\\\u201d, \\\\\\\\u201caccuracy\\\\\\\\u201d). **Required.**  \\\\\\\\n`value`| `Union[float, str]`| Score value. Float for `NUMERIC`/`BOOLEAN`, string for `CATEGORICAL`. **Required.**  \\\\\\\\n`trace_id`| `str`| ID of the trace to associate with (for `create_score`). **Required.**  \\\\\\\\n`observation_id`| `Optional[str]`| ID of the specific observation to score (for `create_score`).  \\\\\\\\n`session_id`| `Optional[str]`| ID of the specific session to score (for `create_score`).  \\\\\\\\n`score_id`| `Optional[str]`| Custom ID for the score (auto-generated if None).  \\\\\\\\n`data_type`| `Optional[ScoreDataType]`| `\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\"`, `\\\\\\\\\\\\\\\"BOOLEAN\\\\\\\\\\\\\\\"`, or `\\\\\\\\\\\\\\\"CATEGORICAL\\\\\\\\\\\\\\\"`. Inferred if not provided based on value type and score config on server.  \\\\\\\\n`comment`| `Optional[str]`| Optional comment or explanation for the score.  \\\\\\\\n`config_id`| `Optional[str]`| Optional ID of a pre-defined score configuration in Langfuse.  \\\\\\\\n  \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThe most powerful way to use datasets is by linking your application\\\\\\\\u2019s executions (traces) to specific dataset items when performing an evaluation run. The `DatasetItemClient.run()` method provides a context manager to streamline this process.\\\\\\\\n**How`item.run()` works:**\\\\\\\\nWhen you use `with item.run(run_name=\\\\\\\\\\\\\\\"your_eval_run_name\\\\\\\\\\\\\\\") as root_span:`:\\\\\\\\n  1. **Trace Creation** : A new Langfuse trace is initiated specifically for processing this dataset item within the context of the named run.\\\\\\\\n  2. **Trace Naming & Metadata**: \\\\\\\\n    * The trace is automatically named (e.g., \\\\\\\\u201cDataset run: your_eval_run_name\\\\\\\\u201d).\\\\\\\\n    * Essential metadata is added to this trace, including `dataset_item_id` (the ID of `item`), `run_name`, and `dataset_id`.\\\\\\\\n  3. **DatasetRunItem Linking** : The SDK makes an API call to Langfuse to create a `DatasetRunItem`. This backend object formally links: \\\\\\\\n    * The `dataset_item_id`\\\\\\\\n    * The `trace_id` of the newly created trace\\\\\\\\n    * The provided `run_name`\\\\\\\\n    * Any `run_metadata` or `run_description` you pass to `item.run()`. This linkage is what populates the \\\\\\\\u201cRuns\\\\\\\\u201d tab for your dataset in the Langfuse UI, allowing you to see all traces associated with a particular evaluation run.\\\\\\\\n  4. **Contextual Span** : The context manager yields `root_span`, which is a `LangfuseSpan` object representing the root span of this new trace.\\\\\\\\n  5. **Automatic Nesting** : Any Langfuse observations (spans or generations) created _inside_ the `with` block will automatically become children of `root_span` and thus part of the trace linked to this dataset item and run.\\\\\\\\n**Example:**\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\ndataset_name = \\\\\\\\\\\\\\\"qna-eval\\\\\\\\\\\\\\\"\\\\\\\\ncurrent_run_name = \\\\\\\\\\\\\\\"qna_model_v3_run_05_20\\\\\\\\\\\\\\\" # Identifies this specific evaluation run\\\\\\\\n \\\\\\\\n///# Assume 'my_qna_app' is your instrumented application function\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"def my_qna_app(question: str, context: str, item_id: str, run_name: str):\\\\\\\\n with langfuse.start_as_current_generation(\\\\\\\\n name=\\\\\\\\\\\\\\\"qna-llm-call\\\\\\\\\\\\\\\",\\\\\\\\n input={\\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\": question, \\\\\\\\\\\\\\\"context\\\\\\\\\\\\\\\": context},\\\\\\\\n metadata={\\\\\\\\\\\\\\\"item_id\\\\\\\\\\\\\\\": item_id, \\\\\\\\\\\\\\\"run\\\\\\\\\\\\\\\": run_name}, ///# Example metadata for the generation\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\"\\\\\\\\n ) as generation:\\\\\\\\n # Simulate LLM call\\\\\\\\n answer = f\\\\\\\\\\\\\\\"Answer to '{question}' using context.\\\\\\\\\\\\\\\" # Replace with actual LLM call\\\\\\\\n generation.update(output={\\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer})\\\\\\\\n \\\\\\\\n # Update the trace with the input and output\\\\\\\\n generation.update_trace(\\\\\\\\n input={\\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\": question, \\\\\\\\\\\\\\\"context\\\\\\\\\\\\\\\": context},\\\\\\\\n output={\\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer},\\\\\\\\n )\\\\\\\\n \\\\\\\\n return answer\\\\\\\\n \\\\\\\\ndataset = langfuse.get_dataset(name=dataset_name) ///# Fetch your pre-populated dataset\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n print(f\\\\\\\\\\\\\\\"Running evaluation for item: {item.id} (Input: {item.input})\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # Use the item.run() context manager\\\\\\\\n with item.run(\\\\\\\\n run_name=current_run_name,\\\\\\\\n run_metadata={\\\\\\\\\\\\\\\"model_provider\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"OpenAI\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"temperature_setting\\\\\\\\\\\\\\\": 0.7},\\\\\\\\n run_description=\\\\\\\\\\\\\\\"Evaluation run for Q&A model v3 on May 20th\\\\\\\\\\\\\\\"\\\\\\\\n ) as root_span: ///# root_span is the root span of the new trace for this item and run.\\\\\\\\n # All subsequent langfuse operations within this block are part of this trace.\\\\\\\\n \\\\\\\\n # Call your application logic\\\\\\\\n generated_answer = my_qna_app(\\\\\\\\n question=item.input[\\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\"],\\\\\\\\n context=item.input[\\\\\\\\\\\\\\\"context\\\\\\\\\\\\\\\"],\\\\\\\\n item_id=item.id,\\\\\\\\n run_name=current_run_name\\\\\\\\n )\\\\\\\\n \\\\\\\\n print(f\\\\\\\\\\\\\\\" Item {item.id} processed. Trace ID: {root_span.trace_id}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # Optionally, score the result against the expected output\\\\\\\\n if item.expected_output and generated_answer == item.expected_output.get(\\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\"):\\\\\\\\n root_span.score_trace(name=\\\\\\\\\\\\\\\"exact_match\\\\\\\\\\\\\\\", value=1.0)\\\\\\\\n else:\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n root_span.score_trace(name=\\\\\\\\\\\\\\\"exact_match\\\\\\\\\\\\\\\", value=0.0)\\\\\\\\n \\\\\\\\nprint(f\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\nFinished processing dataset '{dataset_name}' for run '{current_run_name}'.\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nBy using `item.run()`, you ensure each dataset item\\\\\\\\u2019s processing is neatly encapsulated in its own trace, and these traces are aggregated under the specified `run_name` in the Langfuse UI. This allows for systematic review of results, comparison across runs, and deep dives into individual processing traces.\\\\\\\\nLast updated on August 20, 2025\\\\\\\\n[Instrumentation](/docs/observability/sdk/python/instrumentation \\\\\\\\\\\\\\\"Instrumentation\\\\\\\\\\\\\\\")\\\\\\\\n[Advanced usage](/docs/observability/sdk/python/advanced-usage \\\\\\\\\\\\\\\"Advanced usage\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"All new Datasets, Experimentation and Evaluation documentation\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"## New Datasets and Evaluation documentation\\\\\\\\nBased on this week\\\\\\\\u2019s changes, we have have restructured the documentation to:\\\\\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\\\\\n### Evaluation\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Evaluation documentation](/docs/scores)\\\\\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nPOST /api/public/scores\\\\\\\\n```\\\\\\\\nAdd scores via Python or JS SDK.\\\\\\\\nExample (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.score(\\\\\\\\n trace_id=\\\\\\\\\\\\\\\"123\\\\\\\\\\\\\\\",\\\\\\\\n name=\\\\\\\\\\\\\\\"my_custom_evaluator\\\\\\\\\\\\\\\",\\\\\\\\n value=0.5,\\\\\\\\n)\\\\\\\\n```\\\\\\\\n### Datasets\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Remote Dataset Runs\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Task function that answers geography questions\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n question = item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]\\\\\\\\n response = OpenAI().chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": question}]\\\\\\\\n )\\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n if expected_output and expected_output.lower() in output.lower():\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=1.0)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=0.0)\\\\\\\\n \\\\\\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n accuracies = [\\\\\\\\n eval.value for result in item_results\\\\\\\\n for eval in result.evaluations if eval.name == \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\"\\\\\\\\n ]\\\\\\\\n \\\\\\\\n if not accuracies:\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=None)\\\\\\\\n \\\\\\\\n avg = sum(accuracies) / len(accuracies)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=avg, comment=f\\\\\\\\\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n@pytest.fixture\\\\\\\\ndef langfuse_client():\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Initialize Langfuse client for testing\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n return get_client()\\\\\\\\n \\\\\\\\ndef test_geography_accuracy_passes(langfuse_client):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Test that passes when accuracy is above threshold\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n result = langfuse_client.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Geography Test - Should Pass\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=geography_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy_evaluator]\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Access the run evaluator result directly\\\\\\\\n avg_accuracy = next(\\\\\\\\n eval.value for eval in result.run_evaluations\\\\\\\\n if eval.name == \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\"\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Assert minimum accuracy threshold\\\\\\\\n assert avg_accuracy >= 0.8, f\\\\\\\\\\\\\\\"Average accuracy {avg_accuracy:.2f} below threshold 0.8\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\ndef test_geography_accuracy_fails(langfuse_client):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Example test that demonstrates failure conditions\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n # Use a weaker model or harder questions to demonstrate test failure\\\\\\\\n def failing_task(*, item, **kwargs):\\\\\\\\n # Simulate a task that gives wrong answers\\\\\\\\n return \\\\\\\\\\\\\\\"I don't know\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n result = langfuse_client.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Geography Test - Should Fail\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=failing_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy_evaluator]\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Access the run evaluator result directly\\\\\\\\n avg_accuracy = next(\\\\\\\\n eval.value for eval in result.run_evaluations\\\\\\\\n if eval.name == \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\"\\\\\\\\n )\\\\\\\\n \\\\\\\\n # This test will fail because the task gives wrong answers\\\\\\\\n with pytest.raises(AssertionError):\\\\\\\\n assert avg_accuracy >= 0.8, f\\\\\\\\\\\\\\\"Expected test to fail with low accuracy: {avg_accuracy:.2f}\\\\\\\\\\\\\\\"\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n// test/geography-experiment.test.ts\\\\\\\\nimport { describe, it, expect, beforeAll, afterAll } from 'vitest';\\\\\\\\nimport { OpenAI } from \\\\\\\\\\\\\\\"openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { NodeSDK } from \\\\\\\\\\\\\\\"@opentelemetry/sdk-node\\\\\\\\\\\\\\\";\\\\\\\\nimport { LangfuseClient, ExperimentItem } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\nimport { observeOpenAI } from \\\\\\\\\\\\\\\"@langfuse/openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { LangfuseSpanProcessor } from \\\\\\\\\\\\\\\"@langfuse/otel\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Test data for European capitals\\\\\\\\nconst testData: ExperimentItem[] = [\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\", expected: \\\\\\\\\\\\\\\"Paris\\\\\\\\\\\\\\\" },\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\", expected: \\\\\\\\\\\\\\\"Berlin\\\\\\\\\\\\\\\" },\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of Spain?\\\\\\\\\\\\\\\", expected: \\\\\\\\\\\\\\\"Madrid\\\\\\\\\\\\\\\" },\\\\\\\\n];\\\\\\\\n \\\\\\\\nlet otelSdk: NodeSDK;\\\\\\\\nlet langfuse: LangfuseClient;\\\\\\\\n \\\\\\\\nbeforeAll(async () => {\\\\\\\\n // Initialize OpenTelemetry\\\\\\\\n otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\\\\\n otelSdk.start();\\\\\\\\n \\\\\\\\n // Initialize Langfuse client\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { Factuality, Levenshtein } from \\\\\\\\\\\\\\\"autoevals\\\\\\\\\\\\\\\";\\\\\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\\\\\n \\\\\\\\n// Use with additional parameters\\\\\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\\\\\n Factuality,\\\\\\\\n { model: \\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\" } // Additional AutoEvals parameters\\\\\\\\n);\\\\\\\\n \\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"AutoEvals Integration Test\\\\\\\\\\\\\\\",\\\\\\\\n data: testDataset,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [\\\\\\\\n factualityEvaluator,\\\\\\\\n levenshteinEvaluator,\\\\\\\\n customFactualityEvaluator\\\\\\\\n ]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Remote Dataset Runs\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Remote Dataset Runs via the SDK - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\\\\\\\\\"Dataset Runs\\\\\\\\\\\\\\\")Remote Run\\\\\\\\nCopy as Markdown\\\\\\\\n# Remote Dataset Runs\\\\\\\\nOnce you [created a dataset](/docs/evaluation/dataset-runs/datasets), you can use the dataset to test how your application performs on different inputs. Remote Dataset Runs are used to programmatically loop your applications or prompts through a dataset and optionally apply Evaluation Methods to the results.\\\\\\\\nThey are called \\\\\\\\u201cRemote Dataset Runs\\\\\\\\u201d because they can make use of \\\\\\\\u201cremote\\\\\\\\u201d or external logic and code.\\\\\\\\nOptionally, you can also trigger Remote Dataset Runs via the Langfuse UI which will call them via a webhook.\\\\\\\\n## Why use Remote Dataset Runs?\\\\\\\\n  * Full flexibility to use your own application logic\\\\\\\\n  * Use custom scoring functions to evaluate the outputs\\\\\\\\n  * Run multiple experiments on the same dataset in parallel\\\\\\\\n  * Easy to integrate with your existing evaluation infrastructure\\\\\\\\nSequence Diagram\\\\\\\\n## Experiment runner SDK\\\\\\\\nBoth the Python and JS/TS SDKs provide a high-level abstraction for running an experiment on a dataset. The dataset can be both local or hosted on Langfuse. Using the Experiment runner is the recommended way to run an experiment on a dataset with our SDK.\\\\\\\\nThe experiment runner automatically handles:\\\\\\\\n  * **Concurrent execution** of tasks with configurable limits\\\\\\\\n  * **Automatic tracing** of all executions for observability\\\\\\\\n  * **Flexible evaluation** with both item-level and run-level evaluators\\\\\\\\n  * **Error isolation** so individual failures don\\\\\\\\u2019t stop the experiment\\\\\\\\n  * **Dataset integration** for easy comparison and tracking\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThe experiment runner SDK supports both datasets hosted on Langfuse and datasets hosted locally. If you are using a dataset hosted on Langfuse for your experiment, the SDK will automatically create a dataset run for you that you can inspect and compare in the Langfuse UI. For locally hosted datasets not on Langfuse, only traces and scores (if evaluations are used) are tracked in Langfuse.\\\\\\\\n### Basic Usage\\\\\\\\nStart with the simplest possible experiment to test your task function on local data. If you already have a dataset in Langfuse, see here.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\nfrom langfuse.openai import OpenAI\\\\\\\\n \\\\\\\\n///# Initialize client\\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\n///# Define your task function\\\\\\\\ndef my_task(*, item, **kwargs):\\\\\\\\n question = item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]\\\\\\\\n response = OpenAI().chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4.1\\\\\\\\\\\\\\\", messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": question}]\\\\\\\\n )\\\\\\\\n \\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\n \\\\\\\\n///# Run experiment on local data\\\\\\\\nlocal_data = [\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\"},\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\"},\\\\\\\\n]\\\\\\\\n \\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Geography Quiz\\\\\\\\\\\\\\\",\\\\\\\\n description=\\\\\\\\\\\\\\\"Testing basic functionality\\\\\\\\\\\\\\\",\\\\\\\\n data=local_data,\\\\\\\\n task=my_task,\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# Use format method to display results\\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\nMake sure that OpenTelemetry is properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { OpenAI } from \\\\\\\\\\\\\\\"openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { NodeSDK } from \\\\\\\\\\\\\\\"@opentelemetry/sdk-node\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\nimport {\\\\\\\\n LangfuseClient,\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n ExperimentTask,\\\\\\\\n ExperimentItem,\\\\\\\\n} from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\nimport { observeOpenAI } from \\\\\\\\\\\\\\\"@langfuse/openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { LangfuseSpanProcessor } from \\\\\\\\\\\\\\\"@langfuse/otel\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Initialize OpenTelemetry\\\\\\\\nconst otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\\\\\notelSdk.start();\\\\\\\\n \\\\\\\\n// Initialize client\\\\\\\\nconst langfuse = new LangfuseClient();\\\\\\\\n \\\\\\\\n// Run experiment on local data\\\\\\\\nconst localData: ExperimentItem[] = [\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\" },\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\" },\\\\\\\\n];\\\\\\\\n \\\\\\\\n// Define your task function\\\\\\\\nconst myTask: ExperimentTask = async (item) => {\\\\\\\\n const question = item.input;\\\\\\\\n \\\\\\\\n const response = await observeOpenAI(new OpenAI()).chat.completions.create({\\\\\\\\n model: \\\\\\\\\\\\\\\"gpt-4.1\\\\\\\\\\\\\\\",\\\\\\\\n messages: [\\\\\\\\n {\\\\\\\\n role: \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\",\\\\\\\\n content: question,\\\\\\\\n },\\\\\\\\n ],\\\\\\\\n });\\\\\\\\n \\\\\\\\n return response;\\\\\\\\n};\\\\\\\\n \\\\\\\\n// Run the experiment\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Geography Quiz\\\\\\\\\\\\\\\",\\\\\\\\n description: \\\\\\\\\\\\\\\"Testing basic functionality\\\\\\\\\\\\\\\",\\\\\\\\n data: localData,\\\\\\\\n task: myTask,\\\\\\\\n});\\\\\\\\n \\\\\\\\n// Print formatted result\\\\\\\\nconsole.log(await result.format());\\\\\\\\n \\\\\\\\n// Important: shut down OTEL SDK to deliver traces\\\\\\\\nawait otelSdk.shutdown();\\\\\\\\n```\\\\\\\\n **Note for JS/TS SDK** : OpenTelemetry must be properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\\\\\nWhen running experiments on local data, only traces are created in Langfuse - no dataset runs are generated. Each task execution creates an individual trace for observability and debugging.\\\\\\\\n### Usage with Langfuse Datasets\\\\\\\\nRun experiments directly on datasets stored in Langfuse for automatic tracing and comparison.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n///# Get dataset from Langfuse\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"my-evaluation-dataset\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n///# Run experiment directly on the dataset\\\\\\\\nresult = dataset.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Production Model Test\\\\\\\\\\\\\\\",\\\\\\\\n description=\\\\\\\\\\\\\\\"Monthly evaluation of our production model\\\\\\\\\\\\\\\",\\\\\\\\n task=my_task\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# Use format method to display results\\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n// Get dataset from Langfuse\\\\\\\\nconst dataset = await langfuse.dataset.get(\\\\\\\\\\\\\\\"my-evaluation-dataset\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\n// Run experiment directly on the dataset\\\\\\\\nconst result = await dataset.runExperiment({\\\\\\\\n name: \\\\\\\\\\\\\\\"Production Model Test\\\\\\\\\\\\\\\",\\\\\\\\n description: \\\\\\\\\\\\\\\"Monthly evaluation of our production model\\\\\\\\\\\\\\\",\\\\\\\\n task: myTask\\\\\\\\n});\\\\\\\\n \\\\\\\\n// Use format method to display results\\\\\\\\nconsole.log(await result.format());\\\\\\\\n \\\\\\\\n// Important: shut down OpenTelemetry to ensure traces are sent to Langfuse\\\\\\\\nawait otelSdk.shutdown()\\\\\\\\n```\\\\\\\\nWhen using Langfuse datasets, dataset runs are automatically created in Langfuse and are available for comparison in the UI. This enables tracking experiment performance over time and comparing different approaches on the same dataset.\\\\\\\\n### Advanced Features\\\\\\\\nEnhance your experiments with evaluators and advanced configuration options.\\\\\\\\n#### Evaluators\\\\\\\\nEvaluators assess the quality of task outputs at the item level. They receive the input, metadata, output, and expected output for each item and return evaluation metrics that are reported as scores on the traces in Langfuse.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import Evaluation\\\\\\\\n \\\\\\\\n///# Define evaluation functions\\\\\\\\ndef accuracy_evaluator(*, input, output, expected_output, metadata, **kwargs):\\\\\\\\n if expected_output and expected_output.lower() in output.lower():\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=1.0, comment=\\\\\\\\\\\\\\\"Correct answer found\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=0.0, comment=\\\\\\\\\\\\\\\"Incorrect answer\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\ndef length_evaluator(*, input, output, **kwargs):\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\", value=len(output), comment=f\\\\\\\\\\\\\\\"Response has {len(output)} characters\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n///# Use multiple evaluators\\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Multi-metric Evaluation\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[accuracy_evaluator, length_evaluator]\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n// Define evaluation functions\\\\\\\\nconst accuracyEvaluator = async ({ input, output, expectedOutput }) => {\\\\\\\\n if (expectedOutput && output.toLowerCase().includes(expectedOutput.toLowerCase())) {\\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value: 1.0,\\\\\\\\n comment: \\\\\\\\\\\\\\\"Correct answer found\\\\\\\\\\\\\\\"\\\\\\\\n };\\\\\\\\n }\\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value: 0.0,\\\\\\\\n comment: \\\\\\\\\\\\\\\"Incorrect answer\\\\\\\\\\\\\\\"\\\\\\\\n };\\\\\\\\n};\\\\\\\\n \\\\\\\\nconst lengthEvaluator = async ({ input, output }) => {\\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\",\\\\\\\\n value: output.length,\\\\\\\\n comment: `Response has ${output.length} characters`\\\\\\\\n };\\\\\\\\n};\\\\\\\\n \\\\\\\\n// Use multiple evaluators\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Multi-metric Evaluation\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [accuracyEvaluator, lengthEvaluator]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Run-level Evaluators\\\\\\\\nRun-level evaluators assess the full experiment results and compute aggregate metrics. When run on Langfuse datasets, these scores are attached to the full dataset run for tracking overall experiment performance.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import Evaluation\\\\\\\\n \\\\\\\\ndef average_accuracy(*, item_results, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Calculate average accuracy across all items\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n accuracies = [\\\\\\\\n eval.value for result in item_results\\\\\\\\n for eval in result.evaluations\\\\\\\\n if eval.name == \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\"\\\\\\\\n ]\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n if not accuracies:\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=None)\\\\\\\\n \\\\\\\\n avg = sum(accuracies) / len(accuracies)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=avg, comment=f\\\\\\\\\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Comprehensive Analysis\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy]\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst averageAccuracy = async ({ itemResults }) => {\\\\\\\\n // Calculate average accuracy across all items\\\\\\\\n const accuracies = itemResults\\\\\\\\n .flatMap(result => result.evaluations)\\\\\\\\n .filter(evaluation => evaluation.name === \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\")\\\\\\\\n .map(evaluation => evaluation.value as number);\\\\\\\\n \\\\\\\\n if (accuracies.length === 0) {\\\\\\\\n return { name: \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value: null };\\\\\\\\n }\\\\\\\\n \\\\\\\\n const avg = accuracies.reduce((sum, val) => sum + val, 0) / accuracies.length;\\\\\\\\n \\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value: avg,\\\\\\\\n comment: `Average accuracy: ${(avg * 100).toFixed(1)}%`\\\\\\\\n };\\\\\\\\n};\\\\\\\\n \\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Comprehensive Analysis\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [accuracyEvaluator],\\\\\\\\n runEvaluators: [averageAccuracy]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Async Tasks and Evaluators\\\\\\\\nBoth task functions and evaluators can be asynchronous.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport asyncio\\\\\\\\nfrom langfuse.openai import AsyncOpenAI\\\\\\\\n \\\\\\\\nasync def async_llm_task(*, item, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Async task using OpenAI\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n client = AsyncOpenAI()\\\\\\\\n response = await client.chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]}]\\\\\\\\n )\\\\\\\\n \\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\n///# Works seamlessly with async functions\\\\\\\\nresult = langfuse.run_experiment(\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n name=\\\\\\\\\\\\\\\"Async Experiment\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=async_llm_task,\\\\\\\\n max_concurrency=5 # Control concurrent API calls\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport OpenAI from \\\\\\\\\\\\\\\"openai\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\nconst asyncLlmTask = async (item) => {\\\\\\\\n // Async task using OpenAI\\\\\\\\n const client = new OpenAI();\\\\\\\\n const response = await client.chat.completions.create({\\\\\\\\n model: \\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages: [{ role: \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", content: item.input }]\\\\\\\\n });\\\\\\\\n \\\\\\\\n return response.choices[0].message.content;\\\\\\\\n};\\\\\\\\n \\\\\\\\n// Works seamlessly with async functions\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Async Experiment\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: asyncLlmTask,\\\\\\\\n maxConcurrency: 5 // Control concurrent API calls\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Configuration Options\\\\\\\\nCustomize experiment behavior with various configuration options.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Configurable Experiment\\\\\\\\\\\\\\\",\\\\\\\\n run_name=\\\\\\\\\\\\\\\"Custom Run Name\\\\\\\\\\\\\\\", ///# will be dataset run name if dataset is used\\\\\\\\n description=\\\\\\\\\\\\\\\"Experiment with custom configuration\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy],\\\\\\\\n max_concurrency=10, ///# Max concurrent executions\\\\\\\\n metadata={ ///# Attached to all traces\\\\\\\\n \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"temperature\\\\\\\\\\\\\\\": 0.7,\\\\\\\\n \\\\\\\\\\\\\\\"version\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"v1.2.0\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Configurable Experiment\\\\\\\\\\\\\\\",\\\\\\\\n runName: \\\\\\\\\\\\\\\"Custom Run Name\\\\\\\\\\\\\\\", // will be dataset run name if dataset is used\\\\\\\\n description: \\\\\\\\\\\\\\\"Experiment with custom configuration\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [accuracyEvaluator],\\\\\\\\n runEvaluators: [averageAccuracy],\\\\\\\\n maxConcurrency: 10, // Max concurrent executions\\\\\\\\n metadata: { // Attached to all traces\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n model: \\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n temperature: 0.7,\\\\\\\\n version: \\\\\\\\\\\\\\\"v1.2.0\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Testing in CI Environments\\\\\\\\nIntegrate the experiment runner with testing frameworks like Pytest and Vitest to run automated evaluations in your CI pipeline. Use evaluators to create assertions that can fail tests based on evaluation results.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n///# test_geography_experiment.py\\\\\\\\nimport pytest\\\\\\\\nfrom langfuse import get_client, Evaluation\\\\\\\\nfrom langfuse.openai import OpenAI\\\\\\\\n \\\\\\\\n///# Test data for European capitals\\\\\\\\ntest_data = [\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Paris\\\\\\\\\\\\\\\"},\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Berlin\\\\\\\\\\\\\\\"},\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of Spain?\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Madrid\\\\\\\\\\\\\\\"},\\\\\\\\n]\\\\\\\\n \\\\\\\\ndef geography_task(*, item, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Task function that answers geography questions\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n question = item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]\\\\\\\\n response = OpenAI().chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": question}]\\\\\\\\n )\\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n if expected_output and expected_output.lower() in output.lower():\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=1.0)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=0.0)\\\\\\\\n \\\\\\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n accuracies = [\\\\\\\\n eval.value for result in item_results\\\\\\\\n for eval in result.evaluations if eval.name == \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\"\\\\\\\\n ]\\\\\\\\n \\\\\\\\n if not accuracies:\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=None)\\\\\\\\n \\\\\\\\n avg = sum(accuracies) / len(accuracies)\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThese examples show how to use the experiment runner\\\\\\\\u2019s evaluation results to create meaningful test assertions in your CI pipeline. Tests can fail when accuracy drops below acceptable thresholds, ensuring model quality standards are maintained automatically.\\\\\\\\n### Autoevals Integration\\\\\\\\nAccess pre-built evaluation functions through the [autoevals library](https://github.com/braintrustdata/autoevals) integration.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\nThe Python SDK supports AutoEvals evaluators through direct integration:\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse.experiment import create_evaluator_from_autoevals\\\\\\\\nfrom autoevals.llm import Factuality\\\\\\\\n \\\\\\\\nevaluator = create_evaluator_from_autoevals(Factuality())\\\\\\\\n \\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Autoevals Integration Test\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[evaluator]\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { Factuality, Levenshtein } from \\\\\\\\\\\\\\\"autoevals\\\\\\\\\\\\\\\";\\\\\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\\\\\n \\\\\\\\n// Use with additional parameters\\\\\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\\\\\n Factuality,\\\\\\\\n { model: \\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\" } // Additional AutoEvals parameters\\\\\\\\n);\\\\\\\\n \\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"AutoEvals Integration Test\\\\\\\\\\\\\\\",\\\\\\\\n data: testDataset,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [\\\\\\\\n factualityEvaluator,\\\\\\\\n levenshteinEvaluator,\\\\\\\\n customFactualityEvaluator\\\\\\\\n ]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n## Low-level SDK methods\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\\\\\n comment: \\\\\\\\\\\\\\\"This is a comment\\\\\\\\\\\\\\\", // optional, useful to add reasoning\\\\\\\\n });\\\\\\\\n}\\\\\\\\n \\\\\\\\n// Flush the langfuse client to ensure all score data is sent to the server at the end of the experiment run\\\\\\\\nawait langfuse.flush();\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\nfrom langfuse.langchain import CallbackHandler\\\\\\\\n///#from .app import my_llm_application\\\\\\\\n \\\\\\\\n///# Load the dataset\\\\\\\\ndataset = get_client().get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n///# Initialize the Langfuse handler\\\\\\\\nlangfuse_handler = CallbackHandler()\\\\\\\\n \\\\\\\\n///# Loop over the dataset items\\\\\\\\nfor item in dataset.items:\\\\\\\\n # Use the item.run() context manager for automatic trace linking\\\\\\\\n with item.run(\\\\\\\\n run_name=\\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\",\\\\\\\\n run_description=\\\\\\\\\\\\\\\"My first run\\\\\\\\\\\\\\\",\\\\\\\\n run_metadata={\\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"llama3\\\\\\\\\\\\\\\"},\\\\\\\\n ) as root_span:\\\\\\\\n # Execute your LLM-app against the dataset item input\\\\\\\\n output = my_langchain_chain(item.input, langfuse_handler)\\\\\\\\n \\\\\\\\n # Update top-level trace input and output\\\\\\\\n root_span.update_trace(input=item.input, output=output.content)\\\\\\\\n \\\\\\\\n # Optionally: Add scores computed in your experiment runner, e.g. json equality check\\\\\\\\n root_span.score_trace(\\\\\\\\n name=\\\\\\\\\\\\\\\"<example_eval>\\\\\\\\\\\\\\\",\\\\\\\\n value=my_eval_fn(item.input, output, item.expected_output),\\\\\\\\n comment=\\\\\\\\\\\\\\\"This is a comment\\\\\\\\\\\\\\\", ///# optional, useful to add reasoning\\\\\\\\n )\\\\\\\\n \\\\\\\\n///# Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run\\\\\\\\nget_client().flush()\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { LangfuseClient } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\nimport { CallbackHandler } from \\\\\\\\\\\\\\\"@langfuse/langchain\\\\\\\\\\\\\\\";\\\\\\\\n...\\\\\\\\n \\\\\\\\nconst langfuse = new LangfuseClient()\\\\\\\\nconst runName = \\\\\\\\\\\\\\\"my-dataset-run\\\\\\\\\\\\\\\";\\\\\\\\nfor (const item of dataset.items) {\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/guides/cookbook/example_external_evaluation_pipelines\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline - Langfuse\\\\\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\\\\\\\\\"Cookbooks\\\\\\\\\\\\\\\")External Evaluation Pipelines\\\\\\\\nCopy as Markdown\\\\\\\\nThis is a Jupyter notebook\\\\\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\\\\\n# Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\\\\\\nThis cookbook explains how to build an external evaluation pipeline to measure the performance of your production LLM application using Langfuse.\\\\\\\\nAs a rule of thumb, we encourage you to check first if the [evaluations in the Langfuse UI](https://langfuse.com/docs/scores/model-based-evals) cover your use case. If your needs go beyond these, you can still implement in Langfuse custom evaluation templates without code.\\\\\\\\nConsider implementing an external evaluation pipeline if you need:\\\\\\\\n  * More control over **when** traces get evaluated. You could schedule the pipeline to run at specific times or responding to event-based triggers like Webhooks.\\\\\\\\n  * Greater flexibility with your custom evaluations, when your needs go beyond what\\\\\\\\u2019s possible with the Langfuse UI\\\\\\\\n  * Version control for your custom evaluations\\\\\\\\n  * The ability to evaluate data using existing evaluation frameworks\\\\\\\\nIf your use case meets any of this situations, let\\\\\\\\u2019s go ahead and implement your first external evaluation pipeline!\\\\\\\\n* * *\\\\\\\\nBy the end of this cookbook, you\\\\\\\\u2019ll be able to:\\\\\\\\n  * Create a synthetic dataset to test your models.\\\\\\\\n  * Use the Langfuse client to gather and filter traces of previous model runs\\\\\\\\n  * Evaluate these traces offline and incrementally\\\\\\\\n  * Add scores to existing Langfuse traces\\\\\\\\nConceptually, we will implement the following architecture:\\\\\\\\n* * *\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n**Note** : While we\\\\\\\\u2019re using a Jupyter notebook for this cookbook, in production you\\\\\\\\u2019d use your preferred orchestration tool. Just make sure to extract the code into a .py file and ensure all dependencies are available at runtime.\\\\\\\\n## (Prep-work) Loading synthetic traces to Langfuse\\\\\\\\nIn this demo, we\\\\\\\\u2019ll build a mock application: a science communicator LLM that explains any topic in an engaging and approachable way.\\\\\\\\nSince we don\\\\\\\\u2019t have real user data, our first step is to create a synthetic dataset. We\\\\\\\\u2019ll generate a variety of potential questions that real users might ask. While this is a great way to kickstart your LLM development, collecting real user queries as soon as possible is invaluable.\\\\\\\\nYou can get your Langfuse API keys [here](https://cloud.langfuse.com/) and OpenAI API key [here](https://platform.openai.com/api-keys)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n%pip install langfuse openai deepeval --upgrade\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport os\\\\\\\\n \\\\\\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"pk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddea\\\\\\\\ud83c\\\\\\\\uddfa EU region\\\\\\\\n///# os.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://us.cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddfa\\\\\\\\ud83c\\\\\\\\uddf8 US region\\\\\\\\n \\\\\\\\n///# Your openai key\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"OPENAI_API_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-proj-...\\\\\\\\\\\\\\\"\\\\\\\\n```\\\\\\\\nLet\\\\\\\\u2019s go ahead and generate a list of topic suggestions that we can later query to our application.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport openai\\\\\\\\n \\\\\\\\ntopic_suggestion = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\" You're a world-class journalist, specialized\\\\\\\\nin figuring out which are the topics that excite people the most.\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nBATCH_SIZE = 10\\\\\\\\nTOTAL_TRACES = 50\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nnow = datetime.now()\\\\\\\\nfive_am_today = datetime(now.year, now.month, now.day, 5, 0)\\\\\\\\nfive_am_yesterday = five_am_today - timedelta(days=1)\\\\\\\\n \\\\\\\\ntraces_batch = langfuse.api.trace.list(page=1,\\\\\\\\n limit=BATCH_SIZE,\\\\\\\\n tags=\\\\\\\\\\\\\\\"ext_eval_pipelines\\\\\\\\\\\\\\\",\\\\\\\\n from_timestamp=five_am_yesterday,\\\\\\\\n to_timestamp=datetime.now()\\\\\\\\n ).data\\\\\\\\n \\\\\\\\nprint(f\\\\\\\\\\\\\\\"Traces in first batch: {len(traces_batch)}\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nTraces in first batch: 10\\\\\\\\n## 2\\\\\\\\\\\\\\\\. Run your evaluations\\\\\\\\nLangfuse can handle numerical, boolean and categorical (`string`) scores. Wrapping your custom evaluation logic in a function is often a good practice. Evaluation functions should take a `trace` as input and yield a valid score. Let\\\\\\\\u2019s begin with a simple example using a categorical score.\\\\\\\\n### 2.1. Categoric Evaluations\\\\\\\\nWhen analyzing the outputs of your LLM applications, you may want to evaluate traits that are best defined qualitatively, such as sentiment, tonality or text complexity (Grade level).\\\\\\\\nWe\\\\\\\\u2019re building a science educator LLM that should sound engaging and positive. To ensure it hits the right notes, we\\\\\\\\u2019ll evaluate the tone of its outputs to see if they match our intent. We\\\\\\\\u2019ll draft an evaluation prompt ourselves (no library) to identify the three main tones in each model output.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\ntemplate_tone_eval = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nYou're an expert in human emotional intelligence. You can identify with ease the\\\\\\\\n tone in human-written text. Your task is to identify the tones present in a\\\\\\\\n piece of <text/> with precission. Your output is a comma separated list of three\\\\\\\\n tones. PRINT THE LIST ALONE, NOTHING ELSE.\\\\\\\\n \\\\\\\\n<possible_tones>\\\\\\\\nneutral, confident, joyful, optimistic, friendly, urgent, analytical, respectful\\\\\\\\n</possible_tones>\\\\\\\\n \\\\\\\\n<example_1>\\\\\\\\nInput: Citizen science plays a crucial role in research by involving everyday\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\npeople in scientific projects. This collaboration allows researchers to collect\\\\\\\\nvast amounts of data that would be impossible to gather on their own. Citizen\\\\\\\\nscientists contribute valuable observations and insights that can lead to new\\\\\\\\ndiscoveries and advancements in various fields. By participating in citizen\\\\\\\\nscience projects, individuals can actively contribute to scientific research\\\\\\\\nand make a meaningful impact on our understanding of the world around us.\\\\\\\\n \\\\\\\\nOutput: respectful,optimistic,confident\\\\\\\\n</example_1>\\\\\\\\n \\\\\\\\n<example_2>\\\\\\\\nInput: Bionics is a field that combines biology and engineering to create\\\\\\\\ndevices that can enhance human abilities. By merging humans and machines,\\\\\\\\nbionics aims to improve quality of life for individuals with disabilities\\\\\\\\nor enhance performance for others. These technologies often mimic natural\\\\\\\\nprocesses in the body to create seamless integration. Overall, bionics holds\\\\\\\\ngreat potential for revolutionizing healthcare and technology in the future.\\\\\\\\n \\\\\\\\nOutput: optimistic,confident,analytical\\\\\\\\n</example_2>\\\\\\\\n \\\\\\\\n<example_3>\\\\\\\\nInput: Social media can have both positive and negative impacts on mental\\\\\\\\nhealth. On the positive side, it can help people connect, share experiences,\\\\\\\\nand find support. However, excessive use of social media can also lead to\\\\\\\\nfeelings of inadequacy, loneliness, and anxiety. It's important to find a\\\\\\\\nbalance and be mindful of how social media affects your mental well-being.\\\\\\\\nRemember, it's okay to take breaks and prioritize your mental health.\\\\\\\\n \\\\\\\\nOutput: friendly,neutral,respectful\\\\\\\\n</example_3>\\\\\\\\n \\\\\\\\n<text>\\\\\\\\n{text}\\\\\\\\n</text>\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n \\\\\\\\ntest_tone_score = openai.chat.completions.create(\\\\\\\\n messages=[\\\\\\\\n {\\\\\\\\n \\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": template_tone_eval.format(\\\\\\\\n text=traces_batch[1].output),\\\\\\\\n }\\\\\\\\n ],\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\n temperature=0\\\\\\\\n).choices[0].message.content\\\\\\\\nprint(f\\\\\\\\\\\\\\\"User query: {traces_batch[1].input['args'][0]}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Model answer: {traces_batch[1].output}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Dominant tones: {test_tone_score}\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nIdentifying human intents and tones can be tricky for language models. To handle this, we used a multi-shot prompt, which means giving the model several examples to learn from. Now let\\\\\\\\u2019s wrap our code in an evaluation function for convenience.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\ndef tone_score(trace):\\\\\\\\n return openai.chat.completions.create(\\\\\\\\n messages=[\\\\\\\\n {\\\\\\\\n \\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": template_tone_eval.format(text=trace.output),\\\\\\\\n }\\\\\\\\n ],\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\",\\\\\\\\n temperature=0\\\\\\\\n ).choices[0].message.content\\\\\\\\n \\\\\\\\ntone_score(traces_batch[1])\\\\\\\\n```\\\\\\\\nGreat! Now let\\\\\\\\u2019s go ahead and create a numeric evaluation score.\\\\\\\\n### 2.2. Numeric Evaluations\\\\\\\\nIn this cookbook, we\\\\\\\\u2019ll use the `Deepeval` framework ([docs](https://docs.confident-ai.com/docs/getting-started)) to handle our numeric evaluations. Deepeval provides scores ranging from zero to one for many common LLM metrics. Plus, you can create custom metrics by simply describing them in plain language. To ensure our app\\\\\\\\u2019s responses are joyful and engaging, we\\\\\\\\u2019ll define a custom \\\\\\\\u2018joyfulness\\\\\\\\u2019 score.\\\\\\\\nYou can use any evaluation library. These are popular ones:\\\\\\\\n  * OpenAI Evals ([GitHub](https://github.com/openai/evals))\\\\\\\\n  * Langchain Evaluators\\\\\\\\n  * [RAGAS](https://docs.ragas.io/en/latest/concepts/metrics/index.html) for RAG applications\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom deepeval.metrics import GEval\\\\\\\\nfrom deepeval.test_case import LLMTestCaseParams, LLMTestCase\\\\\\\\n \\\\\\\\ndef joyfulness_score(trace):\\\\\\\\n joyfulness_metric = GEval(\\\\\\\\n name=\\\\\\\\\\\\\\\"Correctness\\\\\\\\\\\\\\\",\\\\\\\\n criteria=\\\\\\\\\\\\\\\"Determine whether the output is engaging and fun.\\\\\\\\\\\\\\\",\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\\\\\\\\n )\\\\\\\\n test_case = LLMTestCase(\\\\\\\\n input=trace.input[\\\\\\\\\\\\\\\"args\\\\\\\\\\\\\\\"],\\\\\\\\n actual_output=trace.output)\\\\\\\\n \\\\\\\\n joyfulness_metric.measure(test_case)\\\\\\\\n \\\\\\\\n print(f\\\\\\\\\\\\\\\"Score: {joyfulness_metric.score}\\\\\\\\\\\\\\\")\\\\\\\\n print(f\\\\\\\\\\\\\\\"Reason: {joyfulness_metric.reason}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n return {\\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": joyfulness_metric.score, \\\\\\\\\\\\\\\"reason\\\\\\\\\\\\\\\": joyfulness_metric.reason}\\\\\\\\n \\\\\\\\njoyfulness_score(traces_batch[1])\\\\\\\\n```\\\\\\\\nUnder the hood, GEval uses chain of thought (CoT) prompting to formulate a set of criteria for scoring prompts. When developing your own metrics, it\\\\\\\\u2019s important to review the reasoning behind these scores. This helps ensure that the model evaluates the traces just as you intended when you wrote the evaluation prompt.\\\\\\\\nOur eval function returns a dictionary with both the score and the model\\\\\\\\u2019s reasoning. We do this as we\\\\\\\\u2019ll persist the reasoning with every langfuse score, ensuring interpretability.\\\\\\\\nNow we\\\\\\\\u2019re done with defining our evaluation functions. Let\\\\\\\\u2019s push those scores back to Langfuse!\\\\\\\\n## 3\\\\\\\\\\\\\\\\. Pushing Scores to Langfuse\\\\\\\\nNow that we have our evaluation functions ready, it\\\\\\\\u2019s time to put them to work. Use the Langfuse client to add scores to existing traces.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_score(\\\\\\\\n trace_id=traces_batch[1].id,\\\\\\\\n name=\\\\\\\\\\\\\\\"tone\\\\\\\\\\\\\\\",\\\\\\\\n value=joyfulness_score(traces_batch[1])[\\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\"],\\\\\\\\n comment=joyfulness_score(traces_batch[1])[\\\\\\\\\\\\\\\"reason\\\\\\\\\\\\\\\"]\\\\\\\\n)\\\\\\\\n```\\\\\\\\nAnd thus, you\\\\\\\\u2019ve added your first externally-evaluated score to Langfuse! Just 49 more to go \\\\\\\\ud83d\\\\\\\\ude01. But don\\\\\\\\u2019t worry \\\\\\\\u2014 our solutions are easy to scale.\\\\\\\\n## 4\\\\\\\\\\\\\\\\. Putting everything together\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nUntil now, we went through each of the necessary steps to build an external evaluation pipeline: Fetching traces, running the evaluations, and persisting the scores to Langfuse. Let\\\\\\\\u2019s sum it up into a compact script that you could run in your evaluation pipeline.\\\\\\\\nWe\\\\\\\\u2019ll fetch the data in batches of 10 traces and then iterate through each trace to score it and push the scores back to Langfuse. Note that this batch size is for demonstration purposes. In a production setup, you might want to process multiple batches in parallel to speed things up. Batching not only reduces the memory load on your system but also allows you to create checkpoints, so you can easily resume if something goes wrong.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport math\\\\\\\\n \\\\\\\\nfor page_number in range(1, math.ceil(TOTAL_TRACES/BATCH_SIZE)):\\\\\\\\n \\\\\\\\n traces_batch = langfuse.api.trace.list(\\\\\\\\n tags=\\\\\\\\\\\\\\\"ext_eval_pipelines\\\\\\\\\\\\\\\",\\\\\\\\n page=page_number,\\\\\\\\n from_timestamp=five_am_yesterday,\\\\\\\\n to_timestamp=five_am_today,\\\\\\\\n limit=BATCH_SIZE\\\\\\\\n ).data\\\\\\\\n \\\\\\\\n for trace in traces_batch:\\\\\\\\n print(f\\\\\\\\\\\\\\\"Processing {trace.name}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n if trace.output is None:\\\\\\\\n print(f\\\\\\\\\\\\\\\"Warning: \\\\\\\\\\\\\\\\n Trace {trace.name} had no generated output, \\\\\\\\\\\\\\\\\\\\\\\\n it was skipped\\\\\\\\\\\\\\\")\\\\\\\\n continue\\\\\\\\n \\\\\\\\n langfuse.create_score(\\\\\\\\n trace_id=trace.id,\\\\\\\\n name=\\\\\\\\\\\\\\\"tone\\\\\\\\\\\\\\\",\\\\\\\\n value=tone_score(trace)\\\\\\\\n )\\\\\\\\n \\\\\\\\n jscore = joyfulness_score(trace)\\\\\\\\n langfuse.create_score(\\\\\\\\n trace_id=trace.id,\\\\\\\\n name=\\\\\\\\\\\\\\\"joyfulness\\\\\\\\\\\\\\\",\\\\\\\\n value=jscore[\\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\"],\\\\\\\\n comment=jscore[\\\\\\\\\\\\\\\"reason\\\\\\\\\\\\\\\"]\\\\\\\\n )\\\\\\\\n \\\\\\\\n print(f\\\\\\\\\\\\\\\"Batch {page_number} processed \\\\\\\\ud83d\\\\\\\\ude80 \\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nIf your pipeline ran successfully, you should see your score in the Langfuse UI. \\\\\\\\nAnd that\\\\\\\\u2019s it! You\\\\\\\\u2019re now ready to integrate these lines into your preferred orchestration tool to ensure they run at the right times.\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nTo achieve our original goal of running the script every day at 5 am, simply schedule a Cron task in your chosen environment with the rule `cron(0 5 * * ? *)`.\\\\\\\\nThanks for coding along! I hope you enjoyed the tutorial and found it helpful.\\\\\\\\nLast updated on August 20, 2025\\\\\\\\n[Example - Tracing and Evaluation for the OpenAI-Agents SDK](/guides/cookbook/example_evaluating_openai_agents \\\\\\\\\\\\\\\"Example - Tracing and Evaluation for the OpenAI-Agents SDK\\\\\\\\\\\\\\\")\\\\\\\\n[Guide - Building an intent classification pipeline](/guides/cookbook/example_intent_classification_pipeline \\\\\\\\\\\\\\\"Guide - Building an intent classification pipeline\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"github_discussion\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://github.com/orgs/langfuse/discussions/7572\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"LLM-as-a-Judge Evaluators for Dataset doesn't work as expected.\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\\\\\n\\\\\\\\n---\\\\\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=hallucination) | [Report \\\\\\\\ud83d\\\\\\\\udc1b](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\\\\\n\\\\\\\\n        Reply #5 to comment #1 from dosubot on June 30, 2025:\\\\\\\\n\\\\\\\\n        I am using ui and variable mapping is as follows:\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n{{search_query}} is mapped to Object: Dataset item, Object Variable: Input, JsonPath: <empty>\\\\\\\\r\\\\\\\\n{{summary}} is mapped to Object: Dataset item, Object Variable: Metadata, JsonPath: <empty>\\\\\\\\r\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"{{expected}} is mapped to Object: Dataset item,  Object Variable: Expected Output, JsonPath: <empty>\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\nA single row format example from my dataset: \\\\\\\\r\\\\\\\\nInput column: \\\\\\\\\\\\\\\"Test input\\\\\\\\\\\\\\\", Expected Output column: \\\\\\\\\\\\\\\"Test expected output\\\\\\\\\\\\\\\", Metadata column: \\\\\\\\\\\\\\\"Test metadata\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n        Reply #6 to comment #1 from dosubot on June 30, 2025:\\\\\\\\n\\\\\\\\n        <!-- Answer -->\\\\\\\\nYour mapping setup looks correct, but the backend eligibility logic is strict: if any mapped variable can't be resolved (for example, if the field is empty or the type doesn't match expectations), the item is silently skipped and you get \\\\\\\\\\\\\\\"0 elements\\\\\\\\\\\\\\\" processed. \\\\\\\\n\\\\\\\\nOne likely issue is the Metadata field\\\\\\\\u2014if it's a plain string (like \\\\\\\\\\\\\\\"Test metadata\\\\\\\\\\\\\\\"), but the system expects a JSON object (since many Langfuse examples use objects for metadata), this can cause the mapping to fail even if JsonPath is empty. When JsonPath is empty, the backend uses the top-level value as-is, but if the field is a string and the mapping expects an object (or vice versa), the item is excluded from evaluation\\\\\\\\u00a0[see code logic](https://github.com/langfuse/langfuse/blob/98cc1bb7a1759e29de3dd818f6a500df48fb5e82/worker/src/features/evaluation/evalService.ts).\\\\\\\\n\\\\\\\\nTry this: map all variables to fields you know are plain strings (like Input or Expected Output) and see if the evaluation processes any items. If it works, the issue is with the Metadata field's type. To confirm, create a dataset item where Metadata is a JSON object (e.g., {\\\\\\\\\\\\\\\"meta\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"value\\\\\\\\\\\\\\\"}) and try mapping with and without a JsonPath. This will help isolate the problem.\\\\\\\\n\\\\\\\\nIf you still get 0 elements, export a sample of your dataset as JSON and share it\\\\\\\\u2014there may be a subtle data or mapping mismatch. \\\\\\\\n\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nMore details on dataset item structure and mapping: [Langfuse docs](https://langfuse.com/docs/datasets/get-started).\\\\\\\\n\\\\\\\\n\\\\\\\\n<!-- Dosu Comment Footer -->\\\\\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\\\\\n\\\\\\\\n---\\\\\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=hallucination) | [Report \\\\\\\\ud83d\\\\\\\\udc1b](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\\\\\n\\\\\\\\n        Reply #7 to comment #1 from dosubot on June 30, 2025:\\\\\\\\n\\\\\\\\n        I have created a new dataset with single row in json format, so columns Input, Expected Output and Metadata have following values respectively but still the same issue:\\\\\\\\r\\\\\\\\n{ \\\\\\\\\\\\\\\"search_query\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What's MLflow?\\\\\\\\\\\\\\\" }\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n{ \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"MLflow tracks ML experiments.\\\\\\\\\\\\\\\" } \\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2023-09-25-datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Datasets (beta)\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Datasets (beta) - Langfuse\\\\\\\\n[\\\\\\\\u2190 Back to changelog](/changelog#2023-09-25-datasets)\\\\\\\\nSeptember 25, 2023\\\\\\\\n# Datasets (beta)\\\\\\\\n[Marc Klingen](https://twitter.com/marcklingen)\\\\\\\\nCollect sets of inputs and expected outputs in Langfuse to evaluate your LLM app. Use evaluations to benchmark different experiments.\\\\\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\\\\\n_Run experiment on dataset_\\\\\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n \\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n # output also returned as it is used to evaluate the run\\\\\\\\n generation, output = my_llm_application.run(item.input)\\\\\\\\n \\\\\\\\n # link the execution trace to the dataset item and give it a run_name\\\\\\\\n item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # optionally, evaluate the output to compare different runs more easily\\\\\\\\n generation.score(\\\\\\\\n name=\\\\\\\\\\\\\\\"<example_eval>\\\\\\\\\\\\\\\",\\\\\\\\n # any float value\\\\\\\\n value=my_eval_fn(\\\\\\\\n item.input,\\\\\\\\n output,\\\\\\\\n item.expected_output\\\\\\\\n )\\\\\\\\n )\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst dataset = await langfuse.getDataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\nfor (const item of dataset.items) {\\\\\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n // output also returned as it is used to evaluate the run\\\\\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\\\\\n \\\\\\\\n // link the execution trace to the dataset item and give it a run_name\\\\\\\\n await item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\n // optionally, evaluate the output to compare different runs more easily\\\\\\\\n generation.score({\\\\\\\\n name: \\\\\\\\\\\\\\\"<score_name>\\\\\\\\\\\\\\\",\\\\\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\\\\\n });\\\\\\\\n}\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Troubleshooting and FAQ\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\\\\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\\\\\\\\\(\\\\\\\\\\\\\\\\)` method\\\\\\\\\\\\\\\")\\\\\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\\\\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\\\\\\\\\")\\\\\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\\\\\\\\\"Langfuse Support: \\\\\\\\\\\\\\\\[Experiment\\\\\\\\\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\\\\\\\\\")\\\\\\\\n[Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\\\\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing\\\\\\\\\\\\\\\")\\\\\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\\\\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\\\\\\\\\")\\\\\\\\n[Results for some data items not present when comparing experiments](https://github.com/orgs/langfuse/discussions/5928 \\\\\\\\\\\\\\\"Langfuse Support: Results for some data items not present when comparing experiments\\\\\\\\\\\\\\\")\\\\\\\\n[Deleting Metrics for Langfuse](https://github.com/orgs/langfuse/discussions/5849 \\\\\\\\\\\\\\\"Langfuse Support: Deleting Metrics for Langfuse\\\\\\\\\\\\\\\")\\\\\\\\n[Discrepancies between dataset items found in the UI vs retrieved from the SDK/API](https://github.com/orgs/langfuse/discussions/5822 \\\\\\\\\\\\\\\"Langfuse Support: Discrepancies between dataset items found in the UI vs retrieved from the SDK/API\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Request batch adding of multiple traces to datasets](https://github.com/orgs/langfuse/discussions/8526 \\\\\\\\\\\\\\\"Langfuse Ideas: Request batch adding of multiple traces to datasets\\\\\\\\\\\\\\\")\\\\\\\\n[How to categorize topics and create a pie chart?](https://github.com/orgs/langfuse/discussions/8512 \\\\\\\\\\\\\\\"Langfuse Ideas: How to categorize topics and create a pie chart?\\\\\\\\\\\\\\\")\\\\\\\\n[Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window](https://github.com/orgs/langfuse/discussions/8485 \\\\\\\\\\\\\\\"Langfuse Ideas: Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window\\\\\\\\\\\\\\\")\\\\\\\\n[feat(LLM-as-a-judge): support stratified sampling by trace property](https://github.com/orgs/langfuse/discussions/8480 \\\\\\\\\\\\\\\"Langfuse Ideas: feat\\\\\\\\\\\\\\\\(LLM-as-a-judge\\\\\\\\\\\\\\\\): support stratified sampling by trace property\\\\\\\\\\\\\\\")\\\\\\\\n[Bulk Evaluation Does not Displays the status of the dataset (Like for E.g. processing, evaluated, executed)](https://github.com/orgs/langfuse/discussions/8410 \\\\\\\\\\\\\\\"Langfuse Ideas: Bulk Evaluation Does not Displays the status of the dataset \\\\\\\\\\\\\\\\(Like for E.g. processing, evaluated, executed\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[Annotation Queue Creation API](https://github.com/orgs/langfuse/discussions/8372 \\\\\\\\\\\\\\\"Langfuse Ideas: Annotation Queue Creation API\\\\\\\\\\\\\\\")\\\\\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\\\\\\\\\"Langfuse Ideas: \\\\\\\\\\\\\\\\[Langfuse Cloud\\\\\\\\\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\\\\\\\\\")\\\\\\\\n[feat(evals): allow canceling a running evaluator with pending evaluation jobs](https://github.com/orgs/langfuse/discussions/8310 \\\\\\\\\\\\\\\"Langfuse Ideas: feat\\\\\\\\\\\\\\\\(evals\\\\\\\\\\\\\\\\): allow canceling a running evaluator with pending evaluation jobs\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Support Description on Dataset Items](https://github.com/orgs/langfuse/discussions/6011 \\\\\\\\\\\\\\\"Langfuse Ideas: Support Description on Dataset Items\\\\\\\\\\\\\\\")\\\\\\\\n[Alta Integration](https://github.com/orgs/langfuse/discussions/5957 \\\\\\\\\\\\\\\"Langfuse Ideas: Alta Integration\\\\\\\\\\\\\\\")\\\\\\\\n[Add run name to columns when looking at a specific dataset item](https://github.com/orgs/langfuse/discussions/5929 \\\\\\\\\\\\\\\"Langfuse Ideas: Add run name to columns when looking at a specific dataset item\\\\\\\\\\\\\\\")\\\\\\\\n[Delete multiple dataset runs](https://github.com/orgs/langfuse/discussions/5893 \\\\\\\\\\\\\\\"Langfuse Ideas: Delete multiple dataset runs\\\\\\\\\\\\\\\")\\\\\\\\n[Navigation between items in a dataset run is confusing - context of the selected dataset run is lost](https://github.com/orgs/langfuse/discussions/5892 \\\\\\\\\\\\\\\"Langfuse Ideas: Navigation between items in a dataset run is confusing - context of the selected dataset run is lost\\\\\\\\\\\\\\\")\\\\\\\\n[Enhanced score distribution visualization in experiment analysis](https://github.com/orgs/langfuse/discussions/5819 \\\\\\\\\\\\\\\"Langfuse Ideas: Enhanced score distribution visualization in experiment analysis\\\\\\\\\\\\\\\")\\\\\\\\n[Multi-step Prompt Experiments and Playground](https://github.com/orgs/langfuse/discussions/5812 \\\\\\\\\\\\\\\"Langfuse Ideas: Multi-step Prompt Experiments and Playground\\\\\\\\\\\\\\\")\\\\\\\\n[Option to add trace to new dataset](https://github.com/orgs/langfuse/discussions/5756 \\\\\\\\\\\\\\\"Langfuse Ideas: Option to add trace to new dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Simplified UI for Scoring](https://github.com/orgs/langfuse/discussions/5721 \\\\\\\\\\\\\\\"Langfuse Ideas: Simplified UI for Scoring\\\\\\\\\\\\\\\")\\\\\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\\\\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Datasets: Add selection of traces to a dataset](https://github.com/orgs/langfuse/discussions/4353 \\\\\\\\\\\\\\\"Langfuse Ideas: Datasets: Add selection of traces to a dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Multi-user annotation capability in Annotation Queues](https://github.com/orgs/langfuse/discussions/4348 \\\\\\\\\\\\\\\"Langfuse Ideas: Multi-user annotation capability in Annotation Queues\\\\\\\\\\\\\\\")\\\\\\\\n[Multi-turn / session experiments in datasets](https://github.com/orgs/langfuse/discussions/4208 \\\\\\\\\\\\\\\"Langfuse Ideas: Multi-turn / session experiments in datasets\\\\\\\\\\\\\\\")\\\\\\\\n[Enable to use variable of prompt on evaluator.](https://github.com/orgs/langfuse/discussions/4121 \\\\\\\\\\\\\\\"Langfuse Ideas: Enable to use variable of prompt on evaluator.\\\\\\\\\\\\\\\")\\\\\\\\n[Sessions Table: Scores Column](https://github.com/orgs/langfuse/discussions/4120 \\\\\\\\\\\\\\\"Langfuse Ideas: Sessions Table: Scores Column\\\\\\\\\\\\\\\")\\\\\\\\n[Add new filters for the LLM as a Judge Evaluation (other scores and cost)](https://github.com/orgs/langfuse/discussions/4106 \\\\\\\\\\\\\\\"Langfuse Ideas: Add new filters for the LLM as a Judge Evaluation \\\\\\\\\\\\\\\\(other scores and cost\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[Export dataset run table](https://github.com/orgs/langfuse/discussions/4077 \\\\\\\\\\\\\\\"Langfuse Ideas: Export dataset run table\\\\\\\\\\\\\\\")\\\\\\\\n[feat: support adding trace tags in annotation queue view](https://github.com/orgs/langfuse/discussions/4037 \\\\\\\\\\\\\\\"Langfuse Ideas: feat: support adding trace tags in annotation queue view\\\\\\\\\\\\\\\")\\\\\\\\n[Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times](https://github.com/orgs/langfuse/discussions/4035 \\\\\\\\\\\\\\\"Langfuse Ideas: Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times\\\\\\\\\\\\\\\")\\\\\\\\n[Diff support for dataset runs view](https://github.com/orgs/langfuse/discussions/4025 \\\\\\\\\\\\\\\"Langfuse Ideas: Diff support for dataset runs view\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Create Support for gemini models in playground](https://github.com/orgs/langfuse/discussions/4019 \\\\\\\\\\\\\\\"Langfuse Ideas: Create Support for gemini models in playground\\\\\\\\\\\\\\\")\\\\\\\\n[Change AWS access pattern for Bedrock LLM usage, assume role](https://github.com/orgs/langfuse/discussions/3988 \\\\\\\\\\\\\\\"Langfuse Ideas: Change AWS access pattern for Bedrock LLM usage, assume role\\\\\\\\\\\\\\\")\\\\\\\\n[Add ability to export and import evaluators between projects](https://github.com/orgs/langfuse/discussions/3970 \\\\\\\\\\\\\\\"Langfuse Ideas: Add ability to export and import evaluators between projects\\\\\\\\\\\\\\\")\\\\\\\\n[feat: Folder structure for dataset organisation](https://github.com/orgs/langfuse/discussions/3935 \\\\\\\\\\\\\\\"Langfuse Ideas: feat: Folder structure for dataset organisation\\\\\\\\\\\\\\\")\\\\\\\\n[Model-based evaluations triggered by observations](https://github.com/orgs/langfuse/discussions/3918 \\\\\\\\\\\\\\\"Langfuse Ideas: Model-based evaluations triggered by observations\\\\\\\\\\\\\\\")\\\\\\\\n[Scores: Conditional Annotation](https://github.com/orgs/langfuse/discussions/3842 \\\\\\\\\\\\\\\"Langfuse Ideas: Scores: Conditional Annotation\\\\\\\\\\\\\\\")\\\\\\\\n[Annotation Queues: define optional/mandatory score configs by queue](https://github.com/orgs/langfuse/discussions/3841 \\\\\\\\\\\\\\\"Langfuse Ideas: Annotation Queues: define optional/mandatory score configs by queue\\\\\\\\\\\\\\\")\\\\\\\\n[Scores: support for recording multiple choice selection as score value](https://github.com/orgs/langfuse/discussions/3840 \\\\\\\\\\\\\\\"Langfuse Ideas: Scores: support for recording multiple choice selection as score value\\\\\\\\\\\\\\\")\\\\\\\\n[Filter by status in dataset items table](https://github.com/orgs/langfuse/discussions/3818 \\\\\\\\\\\\\\\"Langfuse Ideas: Filter by status in dataset items table\\\\\\\\\\\\\\\")\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/overview\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Evaluation Overview\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Evaluation of LLM Applications - Langfuse\\\\\\\\nDocsEvaluationOverview\\\\\\\\nCopy as Markdown\\\\\\\\n# Evaluation Overview\\\\\\\\nEvaluation is a critical aspect of developing and deploying LLM applications. Usually, teams use a multitude of different evaluation methods to score the performance of their AI application depending on the use case and the stage of the development process.\\\\\\\\n\\\\\\\\ud83c\\\\\\\\udfa5\\\\\\\\n[ **Watch this walkthrough**](/watch-demo?tab=evaluation) of Langfuse Evaluation and how to use it to improve your LLM application.\\\\\\\\n## Why use LLM Evaluation?\\\\\\\\nLLM evaluation is crucial for improving the accuracy and robustness of language models, ultimately enhancing the user experience and trust in your AI application. Here are the key benefits:\\\\\\\\n  * **Quality Assurance** : Detect hallucinations, factual inaccuracies, and inconsistent outputs to ensure your AI app delivers reliable results\\\\\\\\n  * **Performance Monitoring** : Measure response quality, relevance, and user satisfaction across different scenarios and edge cases\\\\\\\\n  * **Continuous Improvement** : Identify areas for enhancement and track improvements over time through structured evaluation metrics\\\\\\\\n  * **User Trust** : Build confidence in your AI application by demonstrating consistent, high-quality outputs through systematic evaluation\\\\\\\\n  * **Risk Mitigation** : Catch potential issues before they reach production users, reducing the likelihood of poor user experiences or reputational damage\\\\\\\\n## Online & Offline Evaluation\\\\\\\\n**Offline Evaluation involves**\\\\\\\\n  * Evaluating the application in a controlled setting\\\\\\\\n  * Typically using curated test Datasets instead of live user queries\\\\\\\\n  * Heavily used during development (can be part of CI/CD pipelines) to measure improvements / regressions\\\\\\\\n  * Repeatable and you can get clear accuracy metrics since you have ground truth.\\\\\\\\n**Online Evaluation involves**\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Python SDK: Pass ScoreConfig instead of config_id when creating scores](https://github.com/orgs/langfuse/discussions/8623 \\\\\\\\\\\\\\\"Langfuse Ideas: Python SDK: Pass ScoreConfig instead of config_id when creating scores\\\\\\\\\\\\\\\")\\\\\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\\\\\\\\\"Langfuse Ideas: \\\\\\\\\\\\\\\\[Langfuse Cloud\\\\\\\\\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\\\\\\\\\")\\\\\\\\n[Score Configs: Allow editing the categories of a categorical score](https://github.com/orgs/langfuse/discussions/8259 \\\\\\\\\\\\\\\"Langfuse Ideas: Score Configs: Allow editing the categories of a categorical score\\\\\\\\\\\\\\\")\\\\\\\\n[UI-LLM as a Jury](https://github.com/orgs/langfuse/discussions/8195 \\\\\\\\\\\\\\\"Langfuse Ideas: UI-LLM as a Jury\\\\\\\\\\\\\\\")\\\\\\\\n[Enable Immediate Score Management for User Feedback](https://github.com/orgs/langfuse/discussions/7686 \\\\\\\\\\\\\\\"Langfuse Ideas: Enable Immediate Score Management for User Feedback\\\\\\\\\\\\\\\")\\\\\\\\n[Filter by scores in session view](https://github.com/orgs/langfuse/discussions/7528 \\\\\\\\\\\\\\\"Langfuse Ideas: Filter by scores in session view\\\\\\\\\\\\\\\")\\\\\\\\n[Support break lines on evaluation run tooltip hint](https://github.com/orgs/langfuse/discussions/7452 \\\\\\\\\\\\\\\"Langfuse Ideas: Support break lines on evaluation run tooltip hint\\\\\\\\\\\\\\\")\\\\\\\\n[Support new lines when storing / displaying score comments](https://github.com/orgs/langfuse/discussions/6473 \\\\\\\\\\\\\\\"Langfuse Ideas: Support new lines when storing / displaying score comments\\\\\\\\\\\\\\\")\\\\\\\\n[Evaluator: Filter for Scores](https://github.com/orgs/langfuse/discussions/6236 \\\\\\\\\\\\\\\"Langfuse Ideas: Evaluator: Filter for Scores\\\\\\\\\\\\\\\")\\\\\\\\n[Code-based custom evaluators](https://github.com/orgs/langfuse/discussions/6087 \\\\\\\\\\\\\\\"Langfuse Ideas: Code-based custom evaluators\\\\\\\\\\\\\\\")\\\\\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\\\\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Session-level scores](https://github.com/orgs/langfuse/discussions/2728 \\\\\\\\\\\\\\\"Langfuse Ideas: Session-level scores\\\\\\\\\\\\\\\")\\\\\\\\n[Scoring dataset runs, e.g. precision, recall, f-value](https://github.com/orgs/langfuse/discussions/2511 \\\\\\\\\\\\\\\"Langfuse Ideas: Scoring dataset runs, e.g. precision, recall, f-value\\\\\\\\\\\\\\\")\\\\\\\\n[Adding userId / author to score (custom metadata)](https://github.com/orgs/langfuse/discussions/2469 \\\\\\\\\\\\\\\"Langfuse Ideas: Adding userId / author to score \\\\\\\\\\\\\\\\(custom metadata\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[Add string data type in score config](https://github.com/orgs/langfuse/discussions/2402 \\\\\\\\\\\\\\\"Langfuse Ideas: Add string data type in score config\\\\\\\\\\\\\\\")\\\\\\\\n[API to delete scores](https://github.com/orgs/langfuse/discussions/1133 \\\\\\\\\\\\\\\"Langfuse Ideas: API to delete scores\\\\\\\\\\\\\\\")\\\\\\\\nGitHubSupportGitHubIdeas\\\\\\\\nUpvotes[GitHubNew](https://github.com/orgs/langfuse/discussions/new/choose)\\\\\\\\n  * 9votes\\\\\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486)\\\\\\\\nmsanand\\\\\\\\u20223/25/2024\\\\\\\\u2022\\\\\\\\n2Resolved\\\\\\\\n  * 4votes\\\\\\\\n[Filter Categorical Score Values](https://github.com/orgs/langfuse/discussions/3797)\\\\\\\\nalabrashJr\\\\\\\\u202210/17/2024\\\\\\\\u2022\\\\\\\\n3Resolved\\\\\\\\n  * 3votes\\\\\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590)\\\\\\\\nanuras\\\\\\\\u20228/18/2025\\\\\\\\u2022\\\\\\\\n1Resolved\\\\\\\\n  * 3votes\\\\\\\\n[Support for Metric Calculation (Precision@K, Recall@K) and Adding Custom Metrics Use Case Overview](https://github.com/orgs/langfuse/discussions/5215)\\\\\\\\nsrimantacse\\\\\\\\u20221/27/2025\\\\\\\\u2022\\\\\\\\n2Resolved\\\\\\\\n  * 2votes\\\\\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556)\\\\\\\\nflabbergastedbd\\\\\\\\u20228/15/2025\\\\\\\\u2022\\\\\\\\n1\\\\\\\\n  * 1votes\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}]}\\\",\\\"role\\\":\\\"assistant\\\",\\\"tool_calls\\\":null,\\\"function_call\\\":null}}],\\\"usage\\\":{\\\"prompt_tokens\\\":23,\\\"completion_tokens\\\":20274,\\\"total_tokens\\\":20297},\\\"system_fingerprint\\\":\\\"inkeep-fp-9a382b06-43e9-4c7a-b61b-7b35190c514a\\\",\\\"object\\\":\\\"chat.completion\\\",\\\"service_tier\\\":null,\\\"created\\\":1758264684},\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"{\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Troubleshooting and FAQ\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[API - /scores not respecting value when operator '='](https://github.com/orgs/langfuse/discussions/8770 \\\\\\\\\\\\\\\"Langfuse Support: API - /scores not respecting value when operator '='\\\\\\\\\\\\\\\")\\\\\\\\n[Does LangFuse support evaluations on an existing dataset (.csv)](https://github.com/orgs/langfuse/discussions/8665 \\\\\\\\\\\\\\\"Langfuse Support: Does LangFuse support evaluations on an existing dataset \\\\\\\\\\\\\\\\(.csv\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[deleting evaluators](https://github.com/orgs/langfuse/discussions/8640 \\\\\\\\\\\\\\\"Langfuse Support: deleting evaluators\\\\\\\\\\\\\\\")\\\\\\\\n[Not able to import my LLM-as-a-Judge evals](https://github.com/orgs/langfuse/discussions/8636 \\\\\\\\\\\\\\\"Langfuse Support: Not able to import my LLM-as-a-Judge evals\\\\\\\\\\\\\\\")\\\\\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590 \\\\\\\\\\\\\\\"Langfuse Support: How to get experiment run scores programmatically?\\\\\\\\\\\\\\\")\\\\\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556 \\\\\\\\\\\\\\\"Langfuse Support: Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset runs restore from backups](https://github.com/orgs/langfuse/discussions/8534 \\\\\\\\\\\\\\\"Langfuse Support: Dataset runs restore from backups\\\\\\\\\\\\\\\")\\\\\\\\n[Getting scores efficiently via API for analytics purposes](https://github.com/orgs/langfuse/discussions/8520 \\\\\\\\\\\\\\\"Langfuse Support: Getting scores efficiently via API for analytics purposes\\\\\\\\\\\\\\\")\\\\\\\\n[run experiment on dataset](https://github.com/orgs/langfuse/discussions/8433 \\\\\\\\\\\\\\\"Langfuse Support: run experiment on dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Experiments on Datasets with Human Annotated Labels?](https://github.com/orgs/langfuse/discussions/8414 \\\\\\\\\\\\\\\"Langfuse Support: Experiments on Datasets with Human Annotated Labels?\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[How to Recalculate Total Score on Dashboard After Updating User-Defined Model?](https://github.com/orgs/langfuse/discussions/8375 \\\\\\\\\\\\\\\"Langfuse Support: How to Recalculate Total Score on Dashboard After Updating User-Defined Model?\\\\\\\\\\\\\\\")\\\\\\\\n[How to filter by Categorical Scores in custom dashboard?](https://github.com/orgs/langfuse/discussions/8356 \\\\\\\\\\\\\\\"Langfuse Support: How to filter by Categorical Scores in custom dashboard?\\\\\\\\\\\\\\\")\\\\\\\\n[Running scheduled evals utilising LangFuse Datasets & Evaluators](https://github.com/orgs/langfuse/discussions/8355 \\\\\\\\\\\\\\\"Langfuse Support: Running scheduled evals utilising LangFuse Datasets & Evaluators\\\\\\\\\\\\\\\")\\\\\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\\\\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\\\\\\\\\(\\\\\\\\\\\\\\\\)` method\\\\\\\\\\\\\\\")\\\\\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\\\\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\\\\\\\\\")\\\\\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\\\\\\\\\"Langfuse Support: \\\\\\\\\\\\\\\\[Experiment\\\\\\\\\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\\\\\\\\\")\\\\\\\\n[Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\\\\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing\\\\\\\\\\\\\\\")\\\\\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\\\\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[2 traces generated instead of 1](https://github.com/orgs/langfuse/discussions/2244 \\\\\\\\\\\\\\\"Langfuse Support: 2 traces generated instead of 1\\\\\\\\\\\\\\\")\\\\\\\\n[Evaluations Not Available in Self-Hosted Version?](https://github.com/orgs/langfuse/discussions/2130 \\\\\\\\\\\\\\\"Langfuse Support: Evaluations Not Available in Self-Hosted Version?\\\\\\\\\\\\\\\")\\\\\\\\n[Deleting Duplicate Items in a Dataset](https://github.com/orgs/langfuse/discussions/2099 \\\\\\\\\\\\\\\"Langfuse Support: Deleting Duplicate Items in a Dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Availability of evals when self-hosting](https://github.com/orgs/langfuse/discussions/2042 \\\\\\\\\\\\\\\"Langfuse Support: Availability of evals when self-hosting\\\\\\\\\\\\\\\")\\\\\\\\n[How to utilize a dataset w/ typescript and langchain integration](https://github.com/orgs/langfuse/discussions/1969 \\\\\\\\\\\\\\\"Langfuse Support: How to utilize a dataset w/ typescript and langchain integration\\\\\\\\\\\\\\\")\\\\\\\\n[Scoring a trace after the LLM chain returns](https://github.com/orgs/langfuse/discussions/1610 \\\\\\\\\\\\\\\"Langfuse Support: Scoring a trace after the LLM chain returns\\\\\\\\\\\\\\\")\\\\\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486 \\\\\\\\\\\\\\\"Langfuse Support: Update/delete score using python sdk\\\\\\\\\\\\\\\")\\\\\\\\n[Linking dataset run items with existing callback handler](https://github.com/orgs/langfuse/discussions/1445 \\\\\\\\\\\\\\\"Langfuse Support: Linking dataset run items with existing callback handler\\\\\\\\\\\\\\\")\\\\\\\\n[Datasets list / by id](https://github.com/orgs/langfuse/discussions/1420 \\\\\\\\\\\\\\\"Langfuse Support: Datasets list / by id\\\\\\\\\\\\\\\")\\\\\\\\n[Run items not appearing when linking to a trace and not a span or a generation](https://github.com/orgs/langfuse/discussions/1357 \\\\\\\\\\\\\\\"Langfuse Support: Run items not appearing when linking to a trace and not a span or a generation\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Option to run Experiments without traces](https://github.com/orgs/langfuse/discussions/8133 \\\\\\\\\\\\\\\"Langfuse Ideas: Option to run Experiments without traces\\\\\\\\\\\\\\\")\\\\\\\\n[Auto-generate dataset items](https://github.com/orgs/langfuse/discussions/8126 \\\\\\\\\\\\\\\"Langfuse Ideas: Auto-generate dataset items\\\\\\\\\\\\\\\")\\\\\\\\n[feature: support for creation of custom) model adapters](https://github.com/orgs/langfuse/discussions/8123 \\\\\\\\\\\\\\\"Langfuse Ideas: feature: support for creation of custom\\\\\\\\\\\\\\\\) model adapters\\\\\\\\\\\\\\\")\\\\\\\\n[Prompt experiment result download.](https://github.com/orgs/langfuse/discussions/8120 \\\\\\\\\\\\\\\"Langfuse Ideas: Prompt experiment result download.\\\\\\\\\\\\\\\")\\\\\\\\n[Multiple predictions per-item in a single run](https://github.com/orgs/langfuse/discussions/8040 \\\\\\\\\\\\\\\"Langfuse Ideas: Multiple predictions per-item in a single run\\\\\\\\\\\\\\\")\\\\\\\\n[Add evaluator(s) to python SDK (FernLangfuse)](https://github.com/orgs/langfuse/discussions/8018 \\\\\\\\\\\\\\\"Langfuse Ideas: Add evaluator\\\\\\\\\\\\\\\\(s\\\\\\\\\\\\\\\\) to python SDK \\\\\\\\\\\\\\\\(FernLangfuse\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[feat(dataset-runs): return all dataset run item scores in a given run](https://github.com/orgs/langfuse/discussions/8011 \\\\\\\\\\\\\\\"Langfuse Ideas: feat\\\\\\\\\\\\\\\\(dataset-runs\\\\\\\\\\\\\\\\): return all dataset run item scores in a given run\\\\\\\\\\\\\\\")\\\\\\\\n[Delete an evaluator from the evaluator library](https://github.com/orgs/langfuse/discussions/7960 \\\\\\\\\\\\\\\"Langfuse Ideas: Delete an evaluator from the evaluator library\\\\\\\\\\\\\\\")\\\\\\\\n[Archive or Delete Evaluators from the Evaluator Library](https://github.com/orgs/langfuse/discussions/7868 \\\\\\\\\\\\\\\"Langfuse Ideas: Archive or Delete Evaluators from the Evaluator Library\\\\\\\\\\\\\\\")\\\\\\\\n[Edit dataset run name, description](https://github.com/orgs/langfuse/discussions/7814 \\\\\\\\\\\\\\\"Langfuse Ideas: Edit dataset run name, description\\\\\\\\\\\\\\\")\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Datasets\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Datasets - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")Dataset RunsDatasets\\\\\\\\nCopy as Markdown\\\\\\\\n# Datasets\\\\\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\\\\\n## Why use datasets?\\\\\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\\\\\n  * Create test cases for your application with real production traces\\\\\\\\n  * Collaboratively create and collect dataset items with your team\\\\\\\\n  * Have a single source of truth for your test data\\\\\\\\n## Get Started\\\\\\\\n### Creating a dataset\\\\\\\\nDatasets have a name which is unique within a project.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset(\\\\\\\\n name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # optional description\\\\\\\\n description=\\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n # optional metadata\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"author\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"date\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { LangfuseClient } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\nconst langfuse = new LangfuseClient()\\\\\\\\n \\\\\\\\nawait langfuse.api.datasets.create({\\\\\\\\n name: \\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n // optional description\\\\\\\\n description: \\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n // optional metadata\\\\\\\\n metadata: {\\\\\\\\n author: \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n date: \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n type: \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\",\\\\\\\\n },\\\\\\\\n});\\\\\\\\n```\\\\\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\\\\\n### Create new dataset items\\\\\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset_item(\\\\\\\\n dataset_name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # any python object or value, optional\\\\\\\\n input={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # any python object or value, optional\\\\\\\\n expected_output={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # metadata, optional\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"llama3\\\\\\\\\\\\\\\",\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/observability/sdk/python/evaluation\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Evaluations\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Evaluations with the Langfuse Python SDK - Langfuse\\\\\\\\nDocs[Observability](/docs/observability/overview \\\\\\\\\\\\\\\"Observability\\\\\\\\\\\\\\\")\\\\\\\\n[SDKs](/docs/observability/sdk/overview \\\\\\\\\\\\\\\"SDKs\\\\\\\\\\\\\\\")\\\\\\\\n[Python](/docs/observability/sdk/python/overview \\\\\\\\\\\\\\\"Python\\\\\\\\\\\\\\\")Evaluation\\\\\\\\nCopy as Markdown\\\\\\\\n# Evaluations\\\\\\\\nThe Python SDK provides ways to evaluate your application. You can add custom scores to your traces and observations, or use the SDK to execute Dataset Runs.\\\\\\\\nThis page shows the evaluation methods that are supported by the Python SDK. Please refer to the [Evaluation documentation](/docs/evaluation/overview) for more information on how to evaluate your application in Langfuse.\\\\\\\\n## Create Scores\\\\\\\\nLangfuseSpan / LangfuseGeneration object methodsContext-aware methodsLow-level method\\\\\\\\n  * `span_or_generation_obj.score()`: Scores the specific observation object.\\\\\\\\n  * `span_or_generation_obj.score_trace()`: Scores the entire trace to which the object belongs.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nwith langfuse.start_as_current_generation(name=\\\\\\\\\\\\\\\"summary_generation\\\\\\\\\\\\\\\") as gen:\\\\\\\\n # ... LLM call ...\\\\\\\\n gen.update(output=\\\\\\\\\\\\\\\"summary text...\\\\\\\\\\\\\\\")\\\\\\\\n # Score this specific generation\\\\\\\\n gen.score(name=\\\\\\\\\\\\\\\"conciseness\\\\\\\\\\\\\\\", value=0.8, data_type=\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\")\\\\\\\\n # Score the overall trace\\\\\\\\n gen.score_trace(name=\\\\\\\\\\\\\\\"user_feedback_rating\\\\\\\\\\\\\\\", value=\\\\\\\\\\\\\\\"positive\\\\\\\\\\\\\\\", data_type=\\\\\\\\\\\\\\\"CATEGORICAL\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\n  * `langfuse.score_current_span()`: Scores the currently active observation in the context.\\\\\\\\n  * `langfuse.score_current_trace()`: Scores the trace of the currently active observation.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nwith langfuse.start_as_current_span(name=\\\\\\\\\\\\\\\"complex_task\\\\\\\\\\\\\\\") as task_span:\\\\\\\\n # ... perform task ...\\\\\\\\n langfuse.score_current_span(name=\\\\\\\\\\\\\\\"task_component_quality\\\\\\\\\\\\\\\", value=True, data_type=\\\\\\\\\\\\\\\"BOOLEAN\\\\\\\\\\\\\\\")\\\\\\\\n # ...\\\\\\\\n if task_is_fully_successful:\\\\\\\\n langfuse.score_current_trace(name=\\\\\\\\\\\\\\\"overall_success\\\\\\\\\\\\\\\", value=1.0, data_type=\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n  * Creates a score for a specified `trace_id` and optionally `observation_id`.\\\\\\\\n  * Useful when IDs are known, or for scoring after the trace/observation has completed.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nlangfuse.create_score(\\\\\\\\n name=\\\\\\\\\\\\\\\"fact_check_accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value=0.95, ///# Can be float for NUMERIC/BOOLEAN, string for CATEGORICAL\\\\\\\\n trace_id=\\\\\\\\\\\\\\\"abcdef1234567890abcdef1234567890\\\\\\\\\\\\\\\",\\\\\\\\n observation_id=\\\\\\\\\\\\\\\"1234567890abcdef\\\\\\\\\\\\\\\", ///# Optional: if scoring a specific observation\\\\\\\\n session_id=\\\\\\\\\\\\\\\"session_123\\\\\\\\\\\\\\\", ///# Optional: if scoring a specific session\\\\\\\\n data_type=\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\", ///# \\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"BOOLEAN\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"CATEGORICAL\\\\\\\\\\\\\\\"\\\\\\\\n comment=\\\\\\\\\\\\\\\"Source verified for 95% of claims.\\\\\\\\\\\\\\\"\\\\\\\\n)\\\\\\\\n```\\\\\\\\n**Score Parameters:**\\\\\\\\nParameter| Type| Description  \\\\\\\\n---|---|---  \\\\\\\\n`name`| `str`| Name of the score (e.g., \\\\\\\\u201crelevance\\\\\\\\u201d, \\\\\\\\u201caccuracy\\\\\\\\u201d). **Required.**  \\\\\\\\n`value`| `Union[float, str]`| Score value. Float for `NUMERIC`/`BOOLEAN`, string for `CATEGORICAL`. **Required.**  \\\\\\\\n`trace_id`| `str`| ID of the trace to associate with (for `create_score`). **Required.**  \\\\\\\\n`observation_id`| `Optional[str]`| ID of the specific observation to score (for `create_score`).  \\\\\\\\n`session_id`| `Optional[str]`| ID of the specific session to score (for `create_score`).  \\\\\\\\n`score_id`| `Optional[str]`| Custom ID for the score (auto-generated if None).  \\\\\\\\n`data_type`| `Optional[ScoreDataType]`| `\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\"`, `\\\\\\\\\\\\\\\"BOOLEAN\\\\\\\\\\\\\\\"`, or `\\\\\\\\\\\\\\\"CATEGORICAL\\\\\\\\\\\\\\\"`. Inferred if not provided based on value type and score config on server.  \\\\\\\\n`comment`| `Optional[str]`| Optional comment or explanation for the score.  \\\\\\\\n`config_id`| `Optional[str]`| Optional ID of a pre-defined score configuration in Langfuse.  \\\\\\\\n  \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThe most powerful way to use datasets is by linking your application\\\\\\\\u2019s executions (traces) to specific dataset items when performing an evaluation run. The `DatasetItemClient.run()` method provides a context manager to streamline this process.\\\\\\\\n**How`item.run()` works:**\\\\\\\\nWhen you use `with item.run(run_name=\\\\\\\\\\\\\\\"your_eval_run_name\\\\\\\\\\\\\\\") as root_span:`:\\\\\\\\n  1. **Trace Creation** : A new Langfuse trace is initiated specifically for processing this dataset item within the context of the named run.\\\\\\\\n  2. **Trace Naming & Metadata**: \\\\\\\\n    * The trace is automatically named (e.g., \\\\\\\\u201cDataset run: your_eval_run_name\\\\\\\\u201d).\\\\\\\\n    * Essential metadata is added to this trace, including `dataset_item_id` (the ID of `item`), `run_name`, and `dataset_id`.\\\\\\\\n  3. **DatasetRunItem Linking** : The SDK makes an API call to Langfuse to create a `DatasetRunItem`. This backend object formally links: \\\\\\\\n    * The `dataset_item_id`\\\\\\\\n    * The `trace_id` of the newly created trace\\\\\\\\n    * The provided `run_name`\\\\\\\\n    * Any `run_metadata` or `run_description` you pass to `item.run()`. This linkage is what populates the \\\\\\\\u201cRuns\\\\\\\\u201d tab for your dataset in the Langfuse UI, allowing you to see all traces associated with a particular evaluation run.\\\\\\\\n  4. **Contextual Span** : The context manager yields `root_span`, which is a `LangfuseSpan` object representing the root span of this new trace.\\\\\\\\n  5. **Automatic Nesting** : Any Langfuse observations (spans or generations) created _inside_ the `with` block will automatically become children of `root_span` and thus part of the trace linked to this dataset item and run.\\\\\\\\n**Example:**\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\ndataset_name = \\\\\\\\\\\\\\\"qna-eval\\\\\\\\\\\\\\\"\\\\\\\\ncurrent_run_name = \\\\\\\\\\\\\\\"qna_model_v3_run_05_20\\\\\\\\\\\\\\\" # Identifies this specific evaluation run\\\\\\\\n \\\\\\\\n///# Assume 'my_qna_app' is your instrumented application function\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"def my_qna_app(question: str, context: str, item_id: str, run_name: str):\\\\\\\\n with langfuse.start_as_current_generation(\\\\\\\\n name=\\\\\\\\\\\\\\\"qna-llm-call\\\\\\\\\\\\\\\",\\\\\\\\n input={\\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\": question, \\\\\\\\\\\\\\\"context\\\\\\\\\\\\\\\": context},\\\\\\\\n metadata={\\\\\\\\\\\\\\\"item_id\\\\\\\\\\\\\\\": item_id, \\\\\\\\\\\\\\\"run\\\\\\\\\\\\\\\": run_name}, ///# Example metadata for the generation\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\"\\\\\\\\n ) as generation:\\\\\\\\n # Simulate LLM call\\\\\\\\n answer = f\\\\\\\\\\\\\\\"Answer to '{question}' using context.\\\\\\\\\\\\\\\" # Replace with actual LLM call\\\\\\\\n generation.update(output={\\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer})\\\\\\\\n \\\\\\\\n # Update the trace with the input and output\\\\\\\\n generation.update_trace(\\\\\\\\n input={\\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\": question, \\\\\\\\\\\\\\\"context\\\\\\\\\\\\\\\": context},\\\\\\\\n output={\\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer},\\\\\\\\n )\\\\\\\\n \\\\\\\\n return answer\\\\\\\\n \\\\\\\\ndataset = langfuse.get_dataset(name=dataset_name) ///# Fetch your pre-populated dataset\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n print(f\\\\\\\\\\\\\\\"Running evaluation for item: {item.id} (Input: {item.input})\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # Use the item.run() context manager\\\\\\\\n with item.run(\\\\\\\\n run_name=current_run_name,\\\\\\\\n run_metadata={\\\\\\\\\\\\\\\"model_provider\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"OpenAI\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"temperature_setting\\\\\\\\\\\\\\\": 0.7},\\\\\\\\n run_description=\\\\\\\\\\\\\\\"Evaluation run for Q&A model v3 on May 20th\\\\\\\\\\\\\\\"\\\\\\\\n ) as root_span: ///# root_span is the root span of the new trace for this item and run.\\\\\\\\n # All subsequent langfuse operations within this block are part of this trace.\\\\\\\\n \\\\\\\\n # Call your application logic\\\\\\\\n generated_answer = my_qna_app(\\\\\\\\n question=item.input[\\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\"],\\\\\\\\n context=item.input[\\\\\\\\\\\\\\\"context\\\\\\\\\\\\\\\"],\\\\\\\\n item_id=item.id,\\\\\\\\n run_name=current_run_name\\\\\\\\n )\\\\\\\\n \\\\\\\\n print(f\\\\\\\\\\\\\\\" Item {item.id} processed. Trace ID: {root_span.trace_id}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # Optionally, score the result against the expected output\\\\\\\\n if item.expected_output and generated_answer == item.expected_output.get(\\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\"):\\\\\\\\n root_span.score_trace(name=\\\\\\\\\\\\\\\"exact_match\\\\\\\\\\\\\\\", value=1.0)\\\\\\\\n else:\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n root_span.score_trace(name=\\\\\\\\\\\\\\\"exact_match\\\\\\\\\\\\\\\", value=0.0)\\\\\\\\n \\\\\\\\nprint(f\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\nFinished processing dataset '{dataset_name}' for run '{current_run_name}'.\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nBy using `item.run()`, you ensure each dataset item\\\\\\\\u2019s processing is neatly encapsulated in its own trace, and these traces are aggregated under the specified `run_name` in the Langfuse UI. This allows for systematic review of results, comparison across runs, and deep dives into individual processing traces.\\\\\\\\nLast updated on August 20, 2025\\\\\\\\n[Instrumentation](/docs/observability/sdk/python/instrumentation \\\\\\\\\\\\\\\"Instrumentation\\\\\\\\\\\\\\\")\\\\\\\\n[Advanced usage](/docs/observability/sdk/python/advanced-usage \\\\\\\\\\\\\\\"Advanced usage\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"All new Datasets, Experimentation and Evaluation documentation\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"## New Datasets and Evaluation documentation\\\\\\\\nBased on this week\\\\\\\\u2019s changes, we have have restructured the documentation to:\\\\\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\\\\\n### Evaluation\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Evaluation documentation](/docs/scores)\\\\\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nPOST /api/public/scores\\\\\\\\n```\\\\\\\\nAdd scores via Python or JS SDK.\\\\\\\\nExample (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.score(\\\\\\\\n trace_id=\\\\\\\\\\\\\\\"123\\\\\\\\\\\\\\\",\\\\\\\\n name=\\\\\\\\\\\\\\\"my_custom_evaluator\\\\\\\\\\\\\\\",\\\\\\\\n value=0.5,\\\\\\\\n)\\\\\\\\n```\\\\\\\\n### Datasets\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Remote Dataset Runs\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Task function that answers geography questions\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n question = item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]\\\\\\\\n response = OpenAI().chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": question}]\\\\\\\\n )\\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n if expected_output and expected_output.lower() in output.lower():\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=1.0)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=0.0)\\\\\\\\n \\\\\\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n accuracies = [\\\\\\\\n eval.value for result in item_results\\\\\\\\n for eval in result.evaluations if eval.name == \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\"\\\\\\\\n ]\\\\\\\\n \\\\\\\\n if not accuracies:\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=None)\\\\\\\\n \\\\\\\\n avg = sum(accuracies) / len(accuracies)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=avg, comment=f\\\\\\\\\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n@pytest.fixture\\\\\\\\ndef langfuse_client():\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Initialize Langfuse client for testing\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n return get_client()\\\\\\\\n \\\\\\\\ndef test_geography_accuracy_passes(langfuse_client):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Test that passes when accuracy is above threshold\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n result = langfuse_client.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Geography Test - Should Pass\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=geography_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy_evaluator]\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Access the run evaluator result directly\\\\\\\\n avg_accuracy = next(\\\\\\\\n eval.value for eval in result.run_evaluations\\\\\\\\n if eval.name == \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\"\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Assert minimum accuracy threshold\\\\\\\\n assert avg_accuracy >= 0.8, f\\\\\\\\\\\\\\\"Average accuracy {avg_accuracy:.2f} below threshold 0.8\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\ndef test_geography_accuracy_fails(langfuse_client):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Example test that demonstrates failure conditions\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n # Use a weaker model or harder questions to demonstrate test failure\\\\\\\\n def failing_task(*, item, **kwargs):\\\\\\\\n # Simulate a task that gives wrong answers\\\\\\\\n return \\\\\\\\\\\\\\\"I don't know\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n result = langfuse_client.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Geography Test - Should Fail\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=failing_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy_evaluator]\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Access the run evaluator result directly\\\\\\\\n avg_accuracy = next(\\\\\\\\n eval.value for eval in result.run_evaluations\\\\\\\\n if eval.name == \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\"\\\\\\\\n )\\\\\\\\n \\\\\\\\n # This test will fail because the task gives wrong answers\\\\\\\\n with pytest.raises(AssertionError):\\\\\\\\n assert avg_accuracy >= 0.8, f\\\\\\\\\\\\\\\"Expected test to fail with low accuracy: {avg_accuracy:.2f}\\\\\\\\\\\\\\\"\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n// test/geography-experiment.test.ts\\\\\\\\nimport { describe, it, expect, beforeAll, afterAll } from 'vitest';\\\\\\\\nimport { OpenAI } from \\\\\\\\\\\\\\\"openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { NodeSDK } from \\\\\\\\\\\\\\\"@opentelemetry/sdk-node\\\\\\\\\\\\\\\";\\\\\\\\nimport { LangfuseClient, ExperimentItem } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\nimport { observeOpenAI } from \\\\\\\\\\\\\\\"@langfuse/openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { LangfuseSpanProcessor } from \\\\\\\\\\\\\\\"@langfuse/otel\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Test data for European capitals\\\\\\\\nconst testData: ExperimentItem[] = [\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\", expected: \\\\\\\\\\\\\\\"Paris\\\\\\\\\\\\\\\" },\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\", expected: \\\\\\\\\\\\\\\"Berlin\\\\\\\\\\\\\\\" },\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of Spain?\\\\\\\\\\\\\\\", expected: \\\\\\\\\\\\\\\"Madrid\\\\\\\\\\\\\\\" },\\\\\\\\n];\\\\\\\\n \\\\\\\\nlet otelSdk: NodeSDK;\\\\\\\\nlet langfuse: LangfuseClient;\\\\\\\\n \\\\\\\\nbeforeAll(async () => {\\\\\\\\n // Initialize OpenTelemetry\\\\\\\\n otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\\\\\n otelSdk.start();\\\\\\\\n \\\\\\\\n // Initialize Langfuse client\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { Factuality, Levenshtein } from \\\\\\\\\\\\\\\"autoevals\\\\\\\\\\\\\\\";\\\\\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\\\\\n \\\\\\\\n// Use with additional parameters\\\\\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\\\\\n Factuality,\\\\\\\\n { model: \\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\" } // Additional AutoEvals parameters\\\\\\\\n);\\\\\\\\n \\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"AutoEvals Integration Test\\\\\\\\\\\\\\\",\\\\\\\\n data: testDataset,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [\\\\\\\\n factualityEvaluator,\\\\\\\\n levenshteinEvaluator,\\\\\\\\n customFactualityEvaluator\\\\\\\\n ]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Remote Dataset Runs\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Remote Dataset Runs via the SDK - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\\\\\\\\\"Dataset Runs\\\\\\\\\\\\\\\")Remote Run\\\\\\\\nCopy as Markdown\\\\\\\\n# Remote Dataset Runs\\\\\\\\nOnce you [created a dataset](/docs/evaluation/dataset-runs/datasets), you can use the dataset to test how your application performs on different inputs. Remote Dataset Runs are used to programmatically loop your applications or prompts through a dataset and optionally apply Evaluation Methods to the results.\\\\\\\\nThey are called \\\\\\\\u201cRemote Dataset Runs\\\\\\\\u201d because they can make use of \\\\\\\\u201cremote\\\\\\\\u201d or external logic and code.\\\\\\\\nOptionally, you can also trigger Remote Dataset Runs via the Langfuse UI which will call them via a webhook.\\\\\\\\n## Why use Remote Dataset Runs?\\\\\\\\n  * Full flexibility to use your own application logic\\\\\\\\n  * Use custom scoring functions to evaluate the outputs\\\\\\\\n  * Run multiple experiments on the same dataset in parallel\\\\\\\\n  * Easy to integrate with your existing evaluation infrastructure\\\\\\\\nSequence Diagram\\\\\\\\n## Experiment runner SDK\\\\\\\\nBoth the Python and JS/TS SDKs provide a high-level abstraction for running an experiment on a dataset. The dataset can be both local or hosted on Langfuse. Using the Experiment runner is the recommended way to run an experiment on a dataset with our SDK.\\\\\\\\nThe experiment runner automatically handles:\\\\\\\\n  * **Concurrent execution** of tasks with configurable limits\\\\\\\\n  * **Automatic tracing** of all executions for observability\\\\\\\\n  * **Flexible evaluation** with both item-level and run-level evaluators\\\\\\\\n  * **Error isolation** so individual failures don\\\\\\\\u2019t stop the experiment\\\\\\\\n  * **Dataset integration** for easy comparison and tracking\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThe experiment runner SDK supports both datasets hosted on Langfuse and datasets hosted locally. If you are using a dataset hosted on Langfuse for your experiment, the SDK will automatically create a dataset run for you that you can inspect and compare in the Langfuse UI. For locally hosted datasets not on Langfuse, only traces and scores (if evaluations are used) are tracked in Langfuse.\\\\\\\\n### Basic Usage\\\\\\\\nStart with the simplest possible experiment to test your task function on local data. If you already have a dataset in Langfuse, see here.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\nfrom langfuse.openai import OpenAI\\\\\\\\n \\\\\\\\n///# Initialize client\\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\n///# Define your task function\\\\\\\\ndef my_task(*, item, **kwargs):\\\\\\\\n question = item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]\\\\\\\\n response = OpenAI().chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4.1\\\\\\\\\\\\\\\", messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": question}]\\\\\\\\n )\\\\\\\\n \\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\n \\\\\\\\n///# Run experiment on local data\\\\\\\\nlocal_data = [\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\"},\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\"},\\\\\\\\n]\\\\\\\\n \\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Geography Quiz\\\\\\\\\\\\\\\",\\\\\\\\n description=\\\\\\\\\\\\\\\"Testing basic functionality\\\\\\\\\\\\\\\",\\\\\\\\n data=local_data,\\\\\\\\n task=my_task,\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# Use format method to display results\\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\nMake sure that OpenTelemetry is properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { OpenAI } from \\\\\\\\\\\\\\\"openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { NodeSDK } from \\\\\\\\\\\\\\\"@opentelemetry/sdk-node\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\nimport {\\\\\\\\n LangfuseClient,\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n ExperimentTask,\\\\\\\\n ExperimentItem,\\\\\\\\n} from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\nimport { observeOpenAI } from \\\\\\\\\\\\\\\"@langfuse/openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { LangfuseSpanProcessor } from \\\\\\\\\\\\\\\"@langfuse/otel\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Initialize OpenTelemetry\\\\\\\\nconst otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\\\\\notelSdk.start();\\\\\\\\n \\\\\\\\n// Initialize client\\\\\\\\nconst langfuse = new LangfuseClient();\\\\\\\\n \\\\\\\\n// Run experiment on local data\\\\\\\\nconst localData: ExperimentItem[] = [\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\" },\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\" },\\\\\\\\n];\\\\\\\\n \\\\\\\\n// Define your task function\\\\\\\\nconst myTask: ExperimentTask = async (item) => {\\\\\\\\n const question = item.input;\\\\\\\\n \\\\\\\\n const response = await observeOpenAI(new OpenAI()).chat.completions.create({\\\\\\\\n model: \\\\\\\\\\\\\\\"gpt-4.1\\\\\\\\\\\\\\\",\\\\\\\\n messages: [\\\\\\\\n {\\\\\\\\n role: \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\",\\\\\\\\n content: question,\\\\\\\\n },\\\\\\\\n ],\\\\\\\\n });\\\\\\\\n \\\\\\\\n return response;\\\\\\\\n};\\\\\\\\n \\\\\\\\n// Run the experiment\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Geography Quiz\\\\\\\\\\\\\\\",\\\\\\\\n description: \\\\\\\\\\\\\\\"Testing basic functionality\\\\\\\\\\\\\\\",\\\\\\\\n data: localData,\\\\\\\\n task: myTask,\\\\\\\\n});\\\\\\\\n \\\\\\\\n// Print formatted result\\\\\\\\nconsole.log(await result.format());\\\\\\\\n \\\\\\\\n// Important: shut down OTEL SDK to deliver traces\\\\\\\\nawait otelSdk.shutdown();\\\\\\\\n```\\\\\\\\n **Note for JS/TS SDK** : OpenTelemetry must be properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\\\\\nWhen running experiments on local data, only traces are created in Langfuse - no dataset runs are generated. Each task execution creates an individual trace for observability and debugging.\\\\\\\\n### Usage with Langfuse Datasets\\\\\\\\nRun experiments directly on datasets stored in Langfuse for automatic tracing and comparison.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n///# Get dataset from Langfuse\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"my-evaluation-dataset\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n///# Run experiment directly on the dataset\\\\\\\\nresult = dataset.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Production Model Test\\\\\\\\\\\\\\\",\\\\\\\\n description=\\\\\\\\\\\\\\\"Monthly evaluation of our production model\\\\\\\\\\\\\\\",\\\\\\\\n task=my_task\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# Use format method to display results\\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n// Get dataset from Langfuse\\\\\\\\nconst dataset = await langfuse.dataset.get(\\\\\\\\\\\\\\\"my-evaluation-dataset\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\n// Run experiment directly on the dataset\\\\\\\\nconst result = await dataset.runExperiment({\\\\\\\\n name: \\\\\\\\\\\\\\\"Production Model Test\\\\\\\\\\\\\\\",\\\\\\\\n description: \\\\\\\\\\\\\\\"Monthly evaluation of our production model\\\\\\\\\\\\\\\",\\\\\\\\n task: myTask\\\\\\\\n});\\\\\\\\n \\\\\\\\n// Use format method to display results\\\\\\\\nconsole.log(await result.format());\\\\\\\\n \\\\\\\\n// Important: shut down OpenTelemetry to ensure traces are sent to Langfuse\\\\\\\\nawait otelSdk.shutdown()\\\\\\\\n```\\\\\\\\nWhen using Langfuse datasets, dataset runs are automatically created in Langfuse and are available for comparison in the UI. This enables tracking experiment performance over time and comparing different approaches on the same dataset.\\\\\\\\n### Advanced Features\\\\\\\\nEnhance your experiments with evaluators and advanced configuration options.\\\\\\\\n#### Evaluators\\\\\\\\nEvaluators assess the quality of task outputs at the item level. They receive the input, metadata, output, and expected output for each item and return evaluation metrics that are reported as scores on the traces in Langfuse.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import Evaluation\\\\\\\\n \\\\\\\\n///# Define evaluation functions\\\\\\\\ndef accuracy_evaluator(*, input, output, expected_output, metadata, **kwargs):\\\\\\\\n if expected_output and expected_output.lower() in output.lower():\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=1.0, comment=\\\\\\\\\\\\\\\"Correct answer found\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=0.0, comment=\\\\\\\\\\\\\\\"Incorrect answer\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\ndef length_evaluator(*, input, output, **kwargs):\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\", value=len(output), comment=f\\\\\\\\\\\\\\\"Response has {len(output)} characters\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n///# Use multiple evaluators\\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Multi-metric Evaluation\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[accuracy_evaluator, length_evaluator]\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n// Define evaluation functions\\\\\\\\nconst accuracyEvaluator = async ({ input, output, expectedOutput }) => {\\\\\\\\n if (expectedOutput && output.toLowerCase().includes(expectedOutput.toLowerCase())) {\\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value: 1.0,\\\\\\\\n comment: \\\\\\\\\\\\\\\"Correct answer found\\\\\\\\\\\\\\\"\\\\\\\\n };\\\\\\\\n }\\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value: 0.0,\\\\\\\\n comment: \\\\\\\\\\\\\\\"Incorrect answer\\\\\\\\\\\\\\\"\\\\\\\\n };\\\\\\\\n};\\\\\\\\n \\\\\\\\nconst lengthEvaluator = async ({ input, output }) => {\\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\",\\\\\\\\n value: output.length,\\\\\\\\n comment: `Response has ${output.length} characters`\\\\\\\\n };\\\\\\\\n};\\\\\\\\n \\\\\\\\n// Use multiple evaluators\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Multi-metric Evaluation\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [accuracyEvaluator, lengthEvaluator]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Run-level Evaluators\\\\\\\\nRun-level evaluators assess the full experiment results and compute aggregate metrics. When run on Langfuse datasets, these scores are attached to the full dataset run for tracking overall experiment performance.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import Evaluation\\\\\\\\n \\\\\\\\ndef average_accuracy(*, item_results, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Calculate average accuracy across all items\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n accuracies = [\\\\\\\\n eval.value for result in item_results\\\\\\\\n for eval in result.evaluations\\\\\\\\n if eval.name == \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\"\\\\\\\\n ]\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n if not accuracies:\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=None)\\\\\\\\n \\\\\\\\n avg = sum(accuracies) / len(accuracies)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=avg, comment=f\\\\\\\\\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Comprehensive Analysis\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy]\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst averageAccuracy = async ({ itemResults }) => {\\\\\\\\n // Calculate average accuracy across all items\\\\\\\\n const accuracies = itemResults\\\\\\\\n .flatMap(result => result.evaluations)\\\\\\\\n .filter(evaluation => evaluation.name === \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\")\\\\\\\\n .map(evaluation => evaluation.value as number);\\\\\\\\n \\\\\\\\n if (accuracies.length === 0) {\\\\\\\\n return { name: \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value: null };\\\\\\\\n }\\\\\\\\n \\\\\\\\n const avg = accuracies.reduce((sum, val) => sum + val, 0) / accuracies.length;\\\\\\\\n \\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value: avg,\\\\\\\\n comment: `Average accuracy: ${(avg * 100).toFixed(1)}%`\\\\\\\\n };\\\\\\\\n};\\\\\\\\n \\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Comprehensive Analysis\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [accuracyEvaluator],\\\\\\\\n runEvaluators: [averageAccuracy]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Async Tasks and Evaluators\\\\\\\\nBoth task functions and evaluators can be asynchronous.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport asyncio\\\\\\\\nfrom langfuse.openai import AsyncOpenAI\\\\\\\\n \\\\\\\\nasync def async_llm_task(*, item, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Async task using OpenAI\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n client = AsyncOpenAI()\\\\\\\\n response = await client.chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]}]\\\\\\\\n )\\\\\\\\n \\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\n///# Works seamlessly with async functions\\\\\\\\nresult = langfuse.run_experiment(\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n name=\\\\\\\\\\\\\\\"Async Experiment\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=async_llm_task,\\\\\\\\n max_concurrency=5 # Control concurrent API calls\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport OpenAI from \\\\\\\\\\\\\\\"openai\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\nconst asyncLlmTask = async (item) => {\\\\\\\\n // Async task using OpenAI\\\\\\\\n const client = new OpenAI();\\\\\\\\n const response = await client.chat.completions.create({\\\\\\\\n model: \\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages: [{ role: \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", content: item.input }]\\\\\\\\n });\\\\\\\\n \\\\\\\\n return response.choices[0].message.content;\\\\\\\\n};\\\\\\\\n \\\\\\\\n// Works seamlessly with async functions\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Async Experiment\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: asyncLlmTask,\\\\\\\\n maxConcurrency: 5 // Control concurrent API calls\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Configuration Options\\\\\\\\nCustomize experiment behavior with various configuration options.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Configurable Experiment\\\\\\\\\\\\\\\",\\\\\\\\n run_name=\\\\\\\\\\\\\\\"Custom Run Name\\\\\\\\\\\\\\\", ///# will be dataset run name if dataset is used\\\\\\\\n description=\\\\\\\\\\\\\\\"Experiment with custom configuration\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy],\\\\\\\\n max_concurrency=10, ///# Max concurrent executions\\\\\\\\n metadata={ ///# Attached to all traces\\\\\\\\n \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"temperature\\\\\\\\\\\\\\\": 0.7,\\\\\\\\n \\\\\\\\\\\\\\\"version\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"v1.2.0\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Configurable Experiment\\\\\\\\\\\\\\\",\\\\\\\\n runName: \\\\\\\\\\\\\\\"Custom Run Name\\\\\\\\\\\\\\\", // will be dataset run name if dataset is used\\\\\\\\n description: \\\\\\\\\\\\\\\"Experiment with custom configuration\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [accuracyEvaluator],\\\\\\\\n runEvaluators: [averageAccuracy],\\\\\\\\n maxConcurrency: 10, // Max concurrent executions\\\\\\\\n metadata: { // Attached to all traces\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n model: \\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n temperature: 0.7,\\\\\\\\n version: \\\\\\\\\\\\\\\"v1.2.0\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Testing in CI Environments\\\\\\\\nIntegrate the experiment runner with testing frameworks like Pytest and Vitest to run automated evaluations in your CI pipeline. Use evaluators to create assertions that can fail tests based on evaluation results.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n///# test_geography_experiment.py\\\\\\\\nimport pytest\\\\\\\\nfrom langfuse import get_client, Evaluation\\\\\\\\nfrom langfuse.openai import OpenAI\\\\\\\\n \\\\\\\\n///# Test data for European capitals\\\\\\\\ntest_data = [\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Paris\\\\\\\\\\\\\\\"},\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Berlin\\\\\\\\\\\\\\\"},\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of Spain?\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Madrid\\\\\\\\\\\\\\\"},\\\\\\\\n]\\\\\\\\n \\\\\\\\ndef geography_task(*, item, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Task function that answers geography questions\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n question = item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]\\\\\\\\n response = OpenAI().chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": question}]\\\\\\\\n )\\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n if expected_output and expected_output.lower() in output.lower():\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=1.0)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=0.0)\\\\\\\\n \\\\\\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n accuracies = [\\\\\\\\n eval.value for result in item_results\\\\\\\\n for eval in result.evaluations if eval.name == \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\"\\\\\\\\n ]\\\\\\\\n \\\\\\\\n if not accuracies:\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=None)\\\\\\\\n \\\\\\\\n avg = sum(accuracies) / len(accuracies)\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThese examples show how to use the experiment runner\\\\\\\\u2019s evaluation results to create meaningful test assertions in your CI pipeline. Tests can fail when accuracy drops below acceptable thresholds, ensuring model quality standards are maintained automatically.\\\\\\\\n### Autoevals Integration\\\\\\\\nAccess pre-built evaluation functions through the [autoevals library](https://github.com/braintrustdata/autoevals) integration.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\nThe Python SDK supports AutoEvals evaluators through direct integration:\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse.experiment import create_evaluator_from_autoevals\\\\\\\\nfrom autoevals.llm import Factuality\\\\\\\\n \\\\\\\\nevaluator = create_evaluator_from_autoevals(Factuality())\\\\\\\\n \\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Autoevals Integration Test\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[evaluator]\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { Factuality, Levenshtein } from \\\\\\\\\\\\\\\"autoevals\\\\\\\\\\\\\\\";\\\\\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\\\\\n \\\\\\\\n// Use with additional parameters\\\\\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\\\\\n Factuality,\\\\\\\\n { model: \\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\" } // Additional AutoEvals parameters\\\\\\\\n);\\\\\\\\n \\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"AutoEvals Integration Test\\\\\\\\\\\\\\\",\\\\\\\\n data: testDataset,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [\\\\\\\\n factualityEvaluator,\\\\\\\\n levenshteinEvaluator,\\\\\\\\n customFactualityEvaluator\\\\\\\\n ]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n## Low-level SDK methods\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\\\\\n comment: \\\\\\\\\\\\\\\"This is a comment\\\\\\\\\\\\\\\", // optional, useful to add reasoning\\\\\\\\n });\\\\\\\\n}\\\\\\\\n \\\\\\\\n// Flush the langfuse client to ensure all score data is sent to the server at the end of the experiment run\\\\\\\\nawait langfuse.flush();\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\nfrom langfuse.langchain import CallbackHandler\\\\\\\\n///#from .app import my_llm_application\\\\\\\\n \\\\\\\\n///# Load the dataset\\\\\\\\ndataset = get_client().get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n///# Initialize the Langfuse handler\\\\\\\\nlangfuse_handler = CallbackHandler()\\\\\\\\n \\\\\\\\n///# Loop over the dataset items\\\\\\\\nfor item in dataset.items:\\\\\\\\n # Use the item.run() context manager for automatic trace linking\\\\\\\\n with item.run(\\\\\\\\n run_name=\\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\",\\\\\\\\n run_description=\\\\\\\\\\\\\\\"My first run\\\\\\\\\\\\\\\",\\\\\\\\n run_metadata={\\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"llama3\\\\\\\\\\\\\\\"},\\\\\\\\n ) as root_span:\\\\\\\\n # Execute your LLM-app against the dataset item input\\\\\\\\n output = my_langchain_chain(item.input, langfuse_handler)\\\\\\\\n \\\\\\\\n # Update top-level trace input and output\\\\\\\\n root_span.update_trace(input=item.input, output=output.content)\\\\\\\\n \\\\\\\\n # Optionally: Add scores computed in your experiment runner, e.g. json equality check\\\\\\\\n root_span.score_trace(\\\\\\\\n name=\\\\\\\\\\\\\\\"<example_eval>\\\\\\\\\\\\\\\",\\\\\\\\n value=my_eval_fn(item.input, output, item.expected_output),\\\\\\\\n comment=\\\\\\\\\\\\\\\"This is a comment\\\\\\\\\\\\\\\", ///# optional, useful to add reasoning\\\\\\\\n )\\\\\\\\n \\\\\\\\n///# Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run\\\\\\\\nget_client().flush()\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { LangfuseClient } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\nimport { CallbackHandler } from \\\\\\\\\\\\\\\"@langfuse/langchain\\\\\\\\\\\\\\\";\\\\\\\\n...\\\\\\\\n \\\\\\\\nconst langfuse = new LangfuseClient()\\\\\\\\nconst runName = \\\\\\\\\\\\\\\"my-dataset-run\\\\\\\\\\\\\\\";\\\\\\\\nfor (const item of dataset.items) {\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/guides/cookbook/example_external_evaluation_pipelines\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline - Langfuse\\\\\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\\\\\\\\\"Cookbooks\\\\\\\\\\\\\\\")External Evaluation Pipelines\\\\\\\\nCopy as Markdown\\\\\\\\nThis is a Jupyter notebook\\\\\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\\\\\n# Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\\\\\\nThis cookbook explains how to build an external evaluation pipeline to measure the performance of your production LLM application using Langfuse.\\\\\\\\nAs a rule of thumb, we encourage you to check first if the [evaluations in the Langfuse UI](https://langfuse.com/docs/scores/model-based-evals) cover your use case. If your needs go beyond these, you can still implement in Langfuse custom evaluation templates without code.\\\\\\\\nConsider implementing an external evaluation pipeline if you need:\\\\\\\\n  * More control over **when** traces get evaluated. You could schedule the pipeline to run at specific times or responding to event-based triggers like Webhooks.\\\\\\\\n  * Greater flexibility with your custom evaluations, when your needs go beyond what\\\\\\\\u2019s possible with the Langfuse UI\\\\\\\\n  * Version control for your custom evaluations\\\\\\\\n  * The ability to evaluate data using existing evaluation frameworks\\\\\\\\nIf your use case meets any of this situations, let\\\\\\\\u2019s go ahead and implement your first external evaluation pipeline!\\\\\\\\n* * *\\\\\\\\nBy the end of this cookbook, you\\\\\\\\u2019ll be able to:\\\\\\\\n  * Create a synthetic dataset to test your models.\\\\\\\\n  * Use the Langfuse client to gather and filter traces of previous model runs\\\\\\\\n  * Evaluate these traces offline and incrementally\\\\\\\\n  * Add scores to existing Langfuse traces\\\\\\\\nConceptually, we will implement the following architecture:\\\\\\\\n* * *\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n**Note** : While we\\\\\\\\u2019re using a Jupyter notebook for this cookbook, in production you\\\\\\\\u2019d use your preferred orchestration tool. Just make sure to extract the code into a .py file and ensure all dependencies are available at runtime.\\\\\\\\n## (Prep-work) Loading synthetic traces to Langfuse\\\\\\\\nIn this demo, we\\\\\\\\u2019ll build a mock application: a science communicator LLM that explains any topic in an engaging and approachable way.\\\\\\\\nSince we don\\\\\\\\u2019t have real user data, our first step is to create a synthetic dataset. We\\\\\\\\u2019ll generate a variety of potential questions that real users might ask. While this is a great way to kickstart your LLM development, collecting real user queries as soon as possible is invaluable.\\\\\\\\nYou can get your Langfuse API keys [here](https://cloud.langfuse.com/) and OpenAI API key [here](https://platform.openai.com/api-keys)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n%pip install langfuse openai deepeval --upgrade\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport os\\\\\\\\n \\\\\\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"pk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddea\\\\\\\\ud83c\\\\\\\\uddfa EU region\\\\\\\\n///# os.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://us.cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddfa\\\\\\\\ud83c\\\\\\\\uddf8 US region\\\\\\\\n \\\\\\\\n///# Your openai key\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"OPENAI_API_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-proj-...\\\\\\\\\\\\\\\"\\\\\\\\n```\\\\\\\\nLet\\\\\\\\u2019s go ahead and generate a list of topic suggestions that we can later query to our application.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport openai\\\\\\\\n \\\\\\\\ntopic_suggestion = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\" You're a world-class journalist, specialized\\\\\\\\nin figuring out which are the topics that excite people the most.\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nBATCH_SIZE = 10\\\\\\\\nTOTAL_TRACES = 50\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nnow = datetime.now()\\\\\\\\nfive_am_today = datetime(now.year, now.month, now.day, 5, 0)\\\\\\\\nfive_am_yesterday = five_am_today - timedelta(days=1)\\\\\\\\n \\\\\\\\ntraces_batch = langfuse.api.trace.list(page=1,\\\\\\\\n limit=BATCH_SIZE,\\\\\\\\n tags=\\\\\\\\\\\\\\\"ext_eval_pipelines\\\\\\\\\\\\\\\",\\\\\\\\n from_timestamp=five_am_yesterday,\\\\\\\\n to_timestamp=datetime.now()\\\\\\\\n ).data\\\\\\\\n \\\\\\\\nprint(f\\\\\\\\\\\\\\\"Traces in first batch: {len(traces_batch)}\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nTraces in first batch: 10\\\\\\\\n## 2\\\\\\\\\\\\\\\\. Run your evaluations\\\\\\\\nLangfuse can handle numerical, boolean and categorical (`string`) scores. Wrapping your custom evaluation logic in a function is often a good practice. Evaluation functions should take a `trace` as input and yield a valid score. Let\\\\\\\\u2019s begin with a simple example using a categorical score.\\\\\\\\n### 2.1. Categoric Evaluations\\\\\\\\nWhen analyzing the outputs of your LLM applications, you may want to evaluate traits that are best defined qualitatively, such as sentiment, tonality or text complexity (Grade level).\\\\\\\\nWe\\\\\\\\u2019re building a science educator LLM that should sound engaging and positive. To ensure it hits the right notes, we\\\\\\\\u2019ll evaluate the tone of its outputs to see if they match our intent. We\\\\\\\\u2019ll draft an evaluation prompt ourselves (no library) to identify the three main tones in each model output.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\ntemplate_tone_eval = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nYou're an expert in human emotional intelligence. You can identify with ease the\\\\\\\\n tone in human-written text. Your task is to identify the tones present in a\\\\\\\\n piece of <text/> with precission. Your output is a comma separated list of three\\\\\\\\n tones. PRINT THE LIST ALONE, NOTHING ELSE.\\\\\\\\n \\\\\\\\n<possible_tones>\\\\\\\\nneutral, confident, joyful, optimistic, friendly, urgent, analytical, respectful\\\\\\\\n</possible_tones>\\\\\\\\n \\\\\\\\n<example_1>\\\\\\\\nInput: Citizen science plays a crucial role in research by involving everyday\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\npeople in scientific projects. This collaboration allows researchers to collect\\\\\\\\nvast amounts of data that would be impossible to gather on their own. Citizen\\\\\\\\nscientists contribute valuable observations and insights that can lead to new\\\\\\\\ndiscoveries and advancements in various fields. By participating in citizen\\\\\\\\nscience projects, individuals can actively contribute to scientific research\\\\\\\\nand make a meaningful impact on our understanding of the world around us.\\\\\\\\n \\\\\\\\nOutput: respectful,optimistic,confident\\\\\\\\n</example_1>\\\\\\\\n \\\\\\\\n<example_2>\\\\\\\\nInput: Bionics is a field that combines biology and engineering to create\\\\\\\\ndevices that can enhance human abilities. By merging humans and machines,\\\\\\\\nbionics aims to improve quality of life for individuals with disabilities\\\\\\\\nor enhance performance for others. These technologies often mimic natural\\\\\\\\nprocesses in the body to create seamless integration. Overall, bionics holds\\\\\\\\ngreat potential for revolutionizing healthcare and technology in the future.\\\\\\\\n \\\\\\\\nOutput: optimistic,confident,analytical\\\\\\\\n</example_2>\\\\\\\\n \\\\\\\\n<example_3>\\\\\\\\nInput: Social media can have both positive and negative impacts on mental\\\\\\\\nhealth. On the positive side, it can help people connect, share experiences,\\\\\\\\nand find support. However, excessive use of social media can also lead to\\\\\\\\nfeelings of inadequacy, loneliness, and anxiety. It's important to find a\\\\\\\\nbalance and be mindful of how social media affects your mental well-being.\\\\\\\\nRemember, it's okay to take breaks and prioritize your mental health.\\\\\\\\n \\\\\\\\nOutput: friendly,neutral,respectful\\\\\\\\n</example_3>\\\\\\\\n \\\\\\\\n<text>\\\\\\\\n{text}\\\\\\\\n</text>\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n \\\\\\\\ntest_tone_score = openai.chat.completions.create(\\\\\\\\n messages=[\\\\\\\\n {\\\\\\\\n \\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": template_tone_eval.format(\\\\\\\\n text=traces_batch[1].output),\\\\\\\\n }\\\\\\\\n ],\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\n temperature=0\\\\\\\\n).choices[0].message.content\\\\\\\\nprint(f\\\\\\\\\\\\\\\"User query: {traces_batch[1].input['args'][0]}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Model answer: {traces_batch[1].output}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Dominant tones: {test_tone_score}\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nIdentifying human intents and tones can be tricky for language models. To handle this, we used a multi-shot prompt, which means giving the model several examples to learn from. Now let\\\\\\\\u2019s wrap our code in an evaluation function for convenience.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\ndef tone_score(trace):\\\\\\\\n return openai.chat.completions.create(\\\\\\\\n messages=[\\\\\\\\n {\\\\\\\\n \\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": template_tone_eval.format(text=trace.output),\\\\\\\\n }\\\\\\\\n ],\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\",\\\\\\\\n temperature=0\\\\\\\\n ).choices[0].message.content\\\\\\\\n \\\\\\\\ntone_score(traces_batch[1])\\\\\\\\n```\\\\\\\\nGreat! Now let\\\\\\\\u2019s go ahead and create a numeric evaluation score.\\\\\\\\n### 2.2. Numeric Evaluations\\\\\\\\nIn this cookbook, we\\\\\\\\u2019ll use the `Deepeval` framework ([docs](https://docs.confident-ai.com/docs/getting-started)) to handle our numeric evaluations. Deepeval provides scores ranging from zero to one for many common LLM metrics. Plus, you can create custom metrics by simply describing them in plain language. To ensure our app\\\\\\\\u2019s responses are joyful and engaging, we\\\\\\\\u2019ll define a custom \\\\\\\\u2018joyfulness\\\\\\\\u2019 score.\\\\\\\\nYou can use any evaluation library. These are popular ones:\\\\\\\\n  * OpenAI Evals ([GitHub](https://github.com/openai/evals))\\\\\\\\n  * Langchain Evaluators\\\\\\\\n  * [RAGAS](https://docs.ragas.io/en/latest/concepts/metrics/index.html) for RAG applications\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom deepeval.metrics import GEval\\\\\\\\nfrom deepeval.test_case import LLMTestCaseParams, LLMTestCase\\\\\\\\n \\\\\\\\ndef joyfulness_score(trace):\\\\\\\\n joyfulness_metric = GEval(\\\\\\\\n name=\\\\\\\\\\\\\\\"Correctness\\\\\\\\\\\\\\\",\\\\\\\\n criteria=\\\\\\\\\\\\\\\"Determine whether the output is engaging and fun.\\\\\\\\\\\\\\\",\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\\\\\\\\n )\\\\\\\\n test_case = LLMTestCase(\\\\\\\\n input=trace.input[\\\\\\\\\\\\\\\"args\\\\\\\\\\\\\\\"],\\\\\\\\n actual_output=trace.output)\\\\\\\\n \\\\\\\\n joyfulness_metric.measure(test_case)\\\\\\\\n \\\\\\\\n print(f\\\\\\\\\\\\\\\"Score: {joyfulness_metric.score}\\\\\\\\\\\\\\\")\\\\\\\\n print(f\\\\\\\\\\\\\\\"Reason: {joyfulness_metric.reason}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n return {\\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": joyfulness_metric.score, \\\\\\\\\\\\\\\"reason\\\\\\\\\\\\\\\": joyfulness_metric.reason}\\\\\\\\n \\\\\\\\njoyfulness_score(traces_batch[1])\\\\\\\\n```\\\\\\\\nUnder the hood, GEval uses chain of thought (CoT) prompting to formulate a set of criteria for scoring prompts. When developing your own metrics, it\\\\\\\\u2019s important to review the reasoning behind these scores. This helps ensure that the model evaluates the traces just as you intended when you wrote the evaluation prompt.\\\\\\\\nOur eval function returns a dictionary with both the score and the model\\\\\\\\u2019s reasoning. We do this as we\\\\\\\\u2019ll persist the reasoning with every langfuse score, ensuring interpretability.\\\\\\\\nNow we\\\\\\\\u2019re done with defining our evaluation functions. Let\\\\\\\\u2019s push those scores back to Langfuse!\\\\\\\\n## 3\\\\\\\\\\\\\\\\. Pushing Scores to Langfuse\\\\\\\\nNow that we have our evaluation functions ready, it\\\\\\\\u2019s time to put them to work. Use the Langfuse client to add scores to existing traces.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_score(\\\\\\\\n trace_id=traces_batch[1].id,\\\\\\\\n name=\\\\\\\\\\\\\\\"tone\\\\\\\\\\\\\\\",\\\\\\\\n value=joyfulness_score(traces_batch[1])[\\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\"],\\\\\\\\n comment=joyfulness_score(traces_batch[1])[\\\\\\\\\\\\\\\"reason\\\\\\\\\\\\\\\"]\\\\\\\\n)\\\\\\\\n```\\\\\\\\nAnd thus, you\\\\\\\\u2019ve added your first externally-evaluated score to Langfuse! Just 49 more to go \\\\\\\\ud83d\\\\\\\\ude01. But don\\\\\\\\u2019t worry \\\\\\\\u2014 our solutions are easy to scale.\\\\\\\\n## 4\\\\\\\\\\\\\\\\. Putting everything together\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nUntil now, we went through each of the necessary steps to build an external evaluation pipeline: Fetching traces, running the evaluations, and persisting the scores to Langfuse. Let\\\\\\\\u2019s sum it up into a compact script that you could run in your evaluation pipeline.\\\\\\\\nWe\\\\\\\\u2019ll fetch the data in batches of 10 traces and then iterate through each trace to score it and push the scores back to Langfuse. Note that this batch size is for demonstration purposes. In a production setup, you might want to process multiple batches in parallel to speed things up. Batching not only reduces the memory load on your system but also allows you to create checkpoints, so you can easily resume if something goes wrong.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport math\\\\\\\\n \\\\\\\\nfor page_number in range(1, math.ceil(TOTAL_TRACES/BATCH_SIZE)):\\\\\\\\n \\\\\\\\n traces_batch = langfuse.api.trace.list(\\\\\\\\n tags=\\\\\\\\\\\\\\\"ext_eval_pipelines\\\\\\\\\\\\\\\",\\\\\\\\n page=page_number,\\\\\\\\n from_timestamp=five_am_yesterday,\\\\\\\\n to_timestamp=five_am_today,\\\\\\\\n limit=BATCH_SIZE\\\\\\\\n ).data\\\\\\\\n \\\\\\\\n for trace in traces_batch:\\\\\\\\n print(f\\\\\\\\\\\\\\\"Processing {trace.name}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n if trace.output is None:\\\\\\\\n print(f\\\\\\\\\\\\\\\"Warning: \\\\\\\\\\\\\\\\n Trace {trace.name} had no generated output, \\\\\\\\\\\\\\\\\\\\\\\\n it was skipped\\\\\\\\\\\\\\\")\\\\\\\\n continue\\\\\\\\n \\\\\\\\n langfuse.create_score(\\\\\\\\n trace_id=trace.id,\\\\\\\\n name=\\\\\\\\\\\\\\\"tone\\\\\\\\\\\\\\\",\\\\\\\\n value=tone_score(trace)\\\\\\\\n )\\\\\\\\n \\\\\\\\n jscore = joyfulness_score(trace)\\\\\\\\n langfuse.create_score(\\\\\\\\n trace_id=trace.id,\\\\\\\\n name=\\\\\\\\\\\\\\\"joyfulness\\\\\\\\\\\\\\\",\\\\\\\\n value=jscore[\\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\"],\\\\\\\\n comment=jscore[\\\\\\\\\\\\\\\"reason\\\\\\\\\\\\\\\"]\\\\\\\\n )\\\\\\\\n \\\\\\\\n print(f\\\\\\\\\\\\\\\"Batch {page_number} processed \\\\\\\\ud83d\\\\\\\\ude80 \\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nIf your pipeline ran successfully, you should see your score in the Langfuse UI. \\\\\\\\nAnd that\\\\\\\\u2019s it! You\\\\\\\\u2019re now ready to integrate these lines into your preferred orchestration tool to ensure they run at the right times.\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nTo achieve our original goal of running the script every day at 5 am, simply schedule a Cron task in your chosen environment with the rule `cron(0 5 * * ? *)`.\\\\\\\\nThanks for coding along! I hope you enjoyed the tutorial and found it helpful.\\\\\\\\nLast updated on August 20, 2025\\\\\\\\n[Example - Tracing and Evaluation for the OpenAI-Agents SDK](/guides/cookbook/example_evaluating_openai_agents \\\\\\\\\\\\\\\"Example - Tracing and Evaluation for the OpenAI-Agents SDK\\\\\\\\\\\\\\\")\\\\\\\\n[Guide - Building an intent classification pipeline](/guides/cookbook/example_intent_classification_pipeline \\\\\\\\\\\\\\\"Guide - Building an intent classification pipeline\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"github_discussion\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://github.com/orgs/langfuse/discussions/7572\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"LLM-as-a-Judge Evaluators for Dataset doesn't work as expected.\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\\\\\n\\\\\\\\n---\\\\\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=hallucination) | [Report \\\\\\\\ud83d\\\\\\\\udc1b](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\\\\\n\\\\\\\\n        Reply #5 to comment #1 from dosubot on June 30, 2025:\\\\\\\\n\\\\\\\\n        I am using ui and variable mapping is as follows:\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n{{search_query}} is mapped to Object: Dataset item, Object Variable: Input, JsonPath: <empty>\\\\\\\\r\\\\\\\\n{{summary}} is mapped to Object: Dataset item, Object Variable: Metadata, JsonPath: <empty>\\\\\\\\r\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"{{expected}} is mapped to Object: Dataset item,  Object Variable: Expected Output, JsonPath: <empty>\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\nA single row format example from my dataset: \\\\\\\\r\\\\\\\\nInput column: \\\\\\\\\\\\\\\"Test input\\\\\\\\\\\\\\\", Expected Output column: \\\\\\\\\\\\\\\"Test expected output\\\\\\\\\\\\\\\", Metadata column: \\\\\\\\\\\\\\\"Test metadata\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n        Reply #6 to comment #1 from dosubot on June 30, 2025:\\\\\\\\n\\\\\\\\n        <!-- Answer -->\\\\\\\\nYour mapping setup looks correct, but the backend eligibility logic is strict: if any mapped variable can't be resolved (for example, if the field is empty or the type doesn't match expectations), the item is silently skipped and you get \\\\\\\\\\\\\\\"0 elements\\\\\\\\\\\\\\\" processed. \\\\\\\\n\\\\\\\\nOne likely issue is the Metadata field\\\\\\\\u2014if it's a plain string (like \\\\\\\\\\\\\\\"Test metadata\\\\\\\\\\\\\\\"), but the system expects a JSON object (since many Langfuse examples use objects for metadata), this can cause the mapping to fail even if JsonPath is empty. When JsonPath is empty, the backend uses the top-level value as-is, but if the field is a string and the mapping expects an object (or vice versa), the item is excluded from evaluation\\\\\\\\u00a0[see code logic](https://github.com/langfuse/langfuse/blob/98cc1bb7a1759e29de3dd818f6a500df48fb5e82/worker/src/features/evaluation/evalService.ts).\\\\\\\\n\\\\\\\\nTry this: map all variables to fields you know are plain strings (like Input or Expected Output) and see if the evaluation processes any items. If it works, the issue is with the Metadata field's type. To confirm, create a dataset item where Metadata is a JSON object (e.g., {\\\\\\\\\\\\\\\"meta\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"value\\\\\\\\\\\\\\\"}) and try mapping with and without a JsonPath. This will help isolate the problem.\\\\\\\\n\\\\\\\\nIf you still get 0 elements, export a sample of your dataset as JSON and share it\\\\\\\\u2014there may be a subtle data or mapping mismatch. \\\\\\\\n\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nMore details on dataset item structure and mapping: [Langfuse docs](https://langfuse.com/docs/datasets/get-started).\\\\\\\\n\\\\\\\\n\\\\\\\\n<!-- Dosu Comment Footer -->\\\\\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\\\\\n\\\\\\\\n---\\\\\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=hallucination) | [Report \\\\\\\\ud83d\\\\\\\\udc1b](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\\\\\n\\\\\\\\n        Reply #7 to comment #1 from dosubot on June 30, 2025:\\\\\\\\n\\\\\\\\n        I have created a new dataset with single row in json format, so columns Input, Expected Output and Metadata have following values respectively but still the same issue:\\\\\\\\r\\\\\\\\n{ \\\\\\\\\\\\\\\"search_query\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What's MLflow?\\\\\\\\\\\\\\\" }\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n{ \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"MLflow tracks ML experiments.\\\\\\\\\\\\\\\" } \\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2023-09-25-datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Datasets (beta)\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Datasets (beta) - Langfuse\\\\\\\\n[\\\\\\\\u2190 Back to changelog](/changelog#2023-09-25-datasets)\\\\\\\\nSeptember 25, 2023\\\\\\\\n# Datasets (beta)\\\\\\\\n[Marc Klingen](https://twitter.com/marcklingen)\\\\\\\\nCollect sets of inputs and expected outputs in Langfuse to evaluate your LLM app. Use evaluations to benchmark different experiments.\\\\\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\\\\\n_Run experiment on dataset_\\\\\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n \\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n # output also returned as it is used to evaluate the run\\\\\\\\n generation, output = my_llm_application.run(item.input)\\\\\\\\n \\\\\\\\n # link the execution trace to the dataset item and give it a run_name\\\\\\\\n item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # optionally, evaluate the output to compare different runs more easily\\\\\\\\n generation.score(\\\\\\\\n name=\\\\\\\\\\\\\\\"<example_eval>\\\\\\\\\\\\\\\",\\\\\\\\n # any float value\\\\\\\\n value=my_eval_fn(\\\\\\\\n item.input,\\\\\\\\n output,\\\\\\\\n item.expected_output\\\\\\\\n )\\\\\\\\n )\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst dataset = await langfuse.getDataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\nfor (const item of dataset.items) {\\\\\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n // output also returned as it is used to evaluate the run\\\\\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\\\\\n \\\\\\\\n // link the execution trace to the dataset item and give it a run_name\\\\\\\\n await item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\n // optionally, evaluate the output to compare different runs more easily\\\\\\\\n generation.score({\\\\\\\\n name: \\\\\\\\\\\\\\\"<score_name>\\\\\\\\\\\\\\\",\\\\\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\\\\\n });\\\\\\\\n}\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Troubleshooting and FAQ\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\\\\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\\\\\\\\\(\\\\\\\\\\\\\\\\)` method\\\\\\\\\\\\\\\")\\\\\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\\\\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\\\\\\\\\")\\\\\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\\\\\\\\\"Langfuse Support: \\\\\\\\\\\\\\\\[Experiment\\\\\\\\\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\\\\\\\\\")\\\\\\\\n[Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\\\\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing\\\\\\\\\\\\\\\")\\\\\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\\\\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\\\\\\\\\")\\\\\\\\n[Results for some data items not present when comparing experiments](https://github.com/orgs/langfuse/discussions/5928 \\\\\\\\\\\\\\\"Langfuse Support: Results for some data items not present when comparing experiments\\\\\\\\\\\\\\\")\\\\\\\\n[Deleting Metrics for Langfuse](https://github.com/orgs/langfuse/discussions/5849 \\\\\\\\\\\\\\\"Langfuse Support: Deleting Metrics for Langfuse\\\\\\\\\\\\\\\")\\\\\\\\n[Discrepancies between dataset items found in the UI vs retrieved from the SDK/API](https://github.com/orgs/langfuse/discussions/5822 \\\\\\\\\\\\\\\"Langfuse Support: Discrepancies between dataset items found in the UI vs retrieved from the SDK/API\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Request batch adding of multiple traces to datasets](https://github.com/orgs/langfuse/discussions/8526 \\\\\\\\\\\\\\\"Langfuse Ideas: Request batch adding of multiple traces to datasets\\\\\\\\\\\\\\\")\\\\\\\\n[How to categorize topics and create a pie chart?](https://github.com/orgs/langfuse/discussions/8512 \\\\\\\\\\\\\\\"Langfuse Ideas: How to categorize topics and create a pie chart?\\\\\\\\\\\\\\\")\\\\\\\\n[Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window](https://github.com/orgs/langfuse/discussions/8485 \\\\\\\\\\\\\\\"Langfuse Ideas: Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window\\\\\\\\\\\\\\\")\\\\\\\\n[feat(LLM-as-a-judge): support stratified sampling by trace property](https://github.com/orgs/langfuse/discussions/8480 \\\\\\\\\\\\\\\"Langfuse Ideas: feat\\\\\\\\\\\\\\\\(LLM-as-a-judge\\\\\\\\\\\\\\\\): support stratified sampling by trace property\\\\\\\\\\\\\\\")\\\\\\\\n[Bulk Evaluation Does not Displays the status of the dataset (Like for E.g. processing, evaluated, executed)](https://github.com/orgs/langfuse/discussions/8410 \\\\\\\\\\\\\\\"Langfuse Ideas: Bulk Evaluation Does not Displays the status of the dataset \\\\\\\\\\\\\\\\(Like for E.g. processing, evaluated, executed\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[Annotation Queue Creation API](https://github.com/orgs/langfuse/discussions/8372 \\\\\\\\\\\\\\\"Langfuse Ideas: Annotation Queue Creation API\\\\\\\\\\\\\\\")\\\\\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\\\\\\\\\"Langfuse Ideas: \\\\\\\\\\\\\\\\[Langfuse Cloud\\\\\\\\\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\\\\\\\\\")\\\\\\\\n[feat(evals): allow canceling a running evaluator with pending evaluation jobs](https://github.com/orgs/langfuse/discussions/8310 \\\\\\\\\\\\\\\"Langfuse Ideas: feat\\\\\\\\\\\\\\\\(evals\\\\\\\\\\\\\\\\): allow canceling a running evaluator with pending evaluation jobs\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Support Description on Dataset Items](https://github.com/orgs/langfuse/discussions/6011 \\\\\\\\\\\\\\\"Langfuse Ideas: Support Description on Dataset Items\\\\\\\\\\\\\\\")\\\\\\\\n[Alta Integration](https://github.com/orgs/langfuse/discussions/5957 \\\\\\\\\\\\\\\"Langfuse Ideas: Alta Integration\\\\\\\\\\\\\\\")\\\\\\\\n[Add run name to columns when looking at a specific dataset item](https://github.com/orgs/langfuse/discussions/5929 \\\\\\\\\\\\\\\"Langfuse Ideas: Add run name to columns when looking at a specific dataset item\\\\\\\\\\\\\\\")\\\\\\\\n[Delete multiple dataset runs](https://github.com/orgs/langfuse/discussions/5893 \\\\\\\\\\\\\\\"Langfuse Ideas: Delete multiple dataset runs\\\\\\\\\\\\\\\")\\\\\\\\n[Navigation between items in a dataset run is confusing - context of the selected dataset run is lost](https://github.com/orgs/langfuse/discussions/5892 \\\\\\\\\\\\\\\"Langfuse Ideas: Navigation between items in a dataset run is confusing - context of the selected dataset run is lost\\\\\\\\\\\\\\\")\\\\\\\\n[Enhanced score distribution visualization in experiment analysis](https://github.com/orgs/langfuse/discussions/5819 \\\\\\\\\\\\\\\"Langfuse Ideas: Enhanced score distribution visualization in experiment analysis\\\\\\\\\\\\\\\")\\\\\\\\n[Multi-step Prompt Experiments and Playground](https://github.com/orgs/langfuse/discussions/5812 \\\\\\\\\\\\\\\"Langfuse Ideas: Multi-step Prompt Experiments and Playground\\\\\\\\\\\\\\\")\\\\\\\\n[Option to add trace to new dataset](https://github.com/orgs/langfuse/discussions/5756 \\\\\\\\\\\\\\\"Langfuse Ideas: Option to add trace to new dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Simplified UI for Scoring](https://github.com/orgs/langfuse/discussions/5721 \\\\\\\\\\\\\\\"Langfuse Ideas: Simplified UI for Scoring\\\\\\\\\\\\\\\")\\\\\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\\\\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Datasets: Add selection of traces to a dataset](https://github.com/orgs/langfuse/discussions/4353 \\\\\\\\\\\\\\\"Langfuse Ideas: Datasets: Add selection of traces to a dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Multi-user annotation capability in Annotation Queues](https://github.com/orgs/langfuse/discussions/4348 \\\\\\\\\\\\\\\"Langfuse Ideas: Multi-user annotation capability in Annotation Queues\\\\\\\\\\\\\\\")\\\\\\\\n[Multi-turn / session experiments in datasets](https://github.com/orgs/langfuse/discussions/4208 \\\\\\\\\\\\\\\"Langfuse Ideas: Multi-turn / session experiments in datasets\\\\\\\\\\\\\\\")\\\\\\\\n[Enable to use variable of prompt on evaluator.](https://github.com/orgs/langfuse/discussions/4121 \\\\\\\\\\\\\\\"Langfuse Ideas: Enable to use variable of prompt on evaluator.\\\\\\\\\\\\\\\")\\\\\\\\n[Sessions Table: Scores Column](https://github.com/orgs/langfuse/discussions/4120 \\\\\\\\\\\\\\\"Langfuse Ideas: Sessions Table: Scores Column\\\\\\\\\\\\\\\")\\\\\\\\n[Add new filters for the LLM as a Judge Evaluation (other scores and cost)](https://github.com/orgs/langfuse/discussions/4106 \\\\\\\\\\\\\\\"Langfuse Ideas: Add new filters for the LLM as a Judge Evaluation \\\\\\\\\\\\\\\\(other scores and cost\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[Export dataset run table](https://github.com/orgs/langfuse/discussions/4077 \\\\\\\\\\\\\\\"Langfuse Ideas: Export dataset run table\\\\\\\\\\\\\\\")\\\\\\\\n[feat: support adding trace tags in annotation queue view](https://github.com/orgs/langfuse/discussions/4037 \\\\\\\\\\\\\\\"Langfuse Ideas: feat: support adding trace tags in annotation queue view\\\\\\\\\\\\\\\")\\\\\\\\n[Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times](https://github.com/orgs/langfuse/discussions/4035 \\\\\\\\\\\\\\\"Langfuse Ideas: Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times\\\\\\\\\\\\\\\")\\\\\\\\n[Diff support for dataset runs view](https://github.com/orgs/langfuse/discussions/4025 \\\\\\\\\\\\\\\"Langfuse Ideas: Diff support for dataset runs view\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Create Support for gemini models in playground](https://github.com/orgs/langfuse/discussions/4019 \\\\\\\\\\\\\\\"Langfuse Ideas: Create Support for gemini models in playground\\\\\\\\\\\\\\\")\\\\\\\\n[Change AWS access pattern for Bedrock LLM usage, assume role](https://github.com/orgs/langfuse/discussions/3988 \\\\\\\\\\\\\\\"Langfuse Ideas: Change AWS access pattern for Bedrock LLM usage, assume role\\\\\\\\\\\\\\\")\\\\\\\\n[Add ability to export and import evaluators between projects](https://github.com/orgs/langfuse/discussions/3970 \\\\\\\\\\\\\\\"Langfuse Ideas: Add ability to export and import evaluators between projects\\\\\\\\\\\\\\\")\\\\\\\\n[feat: Folder structure for dataset organisation](https://github.com/orgs/langfuse/discussions/3935 \\\\\\\\\\\\\\\"Langfuse Ideas: feat: Folder structure for dataset organisation\\\\\\\\\\\\\\\")\\\\\\\\n[Model-based evaluations triggered by observations](https://github.com/orgs/langfuse/discussions/3918 \\\\\\\\\\\\\\\"Langfuse Ideas: Model-based evaluations triggered by observations\\\\\\\\\\\\\\\")\\\\\\\\n[Scores: Conditional Annotation](https://github.com/orgs/langfuse/discussions/3842 \\\\\\\\\\\\\\\"Langfuse Ideas: Scores: Conditional Annotation\\\\\\\\\\\\\\\")\\\\\\\\n[Annotation Queues: define optional/mandatory score configs by queue](https://github.com/orgs/langfuse/discussions/3841 \\\\\\\\\\\\\\\"Langfuse Ideas: Annotation Queues: define optional/mandatory score configs by queue\\\\\\\\\\\\\\\")\\\\\\\\n[Scores: support for recording multiple choice selection as score value](https://github.com/orgs/langfuse/discussions/3840 \\\\\\\\\\\\\\\"Langfuse Ideas: Scores: support for recording multiple choice selection as score value\\\\\\\\\\\\\\\")\\\\\\\\n[Filter by status in dataset items table](https://github.com/orgs/langfuse/discussions/3818 \\\\\\\\\\\\\\\"Langfuse Ideas: Filter by status in dataset items table\\\\\\\\\\\\\\\")\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/overview\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Evaluation Overview\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Evaluation of LLM Applications - Langfuse\\\\\\\\nDocsEvaluationOverview\\\\\\\\nCopy as Markdown\\\\\\\\n# Evaluation Overview\\\\\\\\nEvaluation is a critical aspect of developing and deploying LLM applications. Usually, teams use a multitude of different evaluation methods to score the performance of their AI application depending on the use case and the stage of the development process.\\\\\\\\n\\\\\\\\ud83c\\\\\\\\udfa5\\\\\\\\n[ **Watch this walkthrough**](/watch-demo?tab=evaluation) of Langfuse Evaluation and how to use it to improve your LLM application.\\\\\\\\n## Why use LLM Evaluation?\\\\\\\\nLLM evaluation is crucial for improving the accuracy and robustness of language models, ultimately enhancing the user experience and trust in your AI application. Here are the key benefits:\\\\\\\\n  * **Quality Assurance** : Detect hallucinations, factual inaccuracies, and inconsistent outputs to ensure your AI app delivers reliable results\\\\\\\\n  * **Performance Monitoring** : Measure response quality, relevance, and user satisfaction across different scenarios and edge cases\\\\\\\\n  * **Continuous Improvement** : Identify areas for enhancement and track improvements over time through structured evaluation metrics\\\\\\\\n  * **User Trust** : Build confidence in your AI application by demonstrating consistent, high-quality outputs through systematic evaluation\\\\\\\\n  * **Risk Mitigation** : Catch potential issues before they reach production users, reducing the likelihood of poor user experiences or reputational damage\\\\\\\\n## Online & Offline Evaluation\\\\\\\\n**Offline Evaluation involves**\\\\\\\\n  * Evaluating the application in a controlled setting\\\\\\\\n  * Typically using curated test Datasets instead of live user queries\\\\\\\\n  * Heavily used during development (can be part of CI/CD pipelines) to measure improvements / regressions\\\\\\\\n  * Repeatable and you can get clear accuracy metrics since you have ground truth.\\\\\\\\n**Online Evaluation involves**\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Python SDK: Pass ScoreConfig instead of config_id when creating scores](https://github.com/orgs/langfuse/discussions/8623 \\\\\\\\\\\\\\\"Langfuse Ideas: Python SDK: Pass ScoreConfig instead of config_id when creating scores\\\\\\\\\\\\\\\")\\\\\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\\\\\\\\\"Langfuse Ideas: \\\\\\\\\\\\\\\\[Langfuse Cloud\\\\\\\\\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\\\\\\\\\")\\\\\\\\n[Score Configs: Allow editing the categories of a categorical score](https://github.com/orgs/langfuse/discussions/8259 \\\\\\\\\\\\\\\"Langfuse Ideas: Score Configs: Allow editing the categories of a categorical score\\\\\\\\\\\\\\\")\\\\\\\\n[UI-LLM as a Jury](https://github.com/orgs/langfuse/discussions/8195 \\\\\\\\\\\\\\\"Langfuse Ideas: UI-LLM as a Jury\\\\\\\\\\\\\\\")\\\\\\\\n[Enable Immediate Score Management for User Feedback](https://github.com/orgs/langfuse/discussions/7686 \\\\\\\\\\\\\\\"Langfuse Ideas: Enable Immediate Score Management for User Feedback\\\\\\\\\\\\\\\")\\\\\\\\n[Filter by scores in session view](https://github.com/orgs/langfuse/discussions/7528 \\\\\\\\\\\\\\\"Langfuse Ideas: Filter by scores in session view\\\\\\\\\\\\\\\")\\\\\\\\n[Support break lines on evaluation run tooltip hint](https://github.com/orgs/langfuse/discussions/7452 \\\\\\\\\\\\\\\"Langfuse Ideas: Support break lines on evaluation run tooltip hint\\\\\\\\\\\\\\\")\\\\\\\\n[Support new lines when storing / displaying score comments](https://github.com/orgs/langfuse/discussions/6473 \\\\\\\\\\\\\\\"Langfuse Ideas: Support new lines when storing / displaying score comments\\\\\\\\\\\\\\\")\\\\\\\\n[Evaluator: Filter for Scores](https://github.com/orgs/langfuse/discussions/6236 \\\\\\\\\\\\\\\"Langfuse Ideas: Evaluator: Filter for Scores\\\\\\\\\\\\\\\")\\\\\\\\n[Code-based custom evaluators](https://github.com/orgs/langfuse/discussions/6087 \\\\\\\\\\\\\\\"Langfuse Ideas: Code-based custom evaluators\\\\\\\\\\\\\\\")\\\\\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\\\\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Session-level scores](https://github.com/orgs/langfuse/discussions/2728 \\\\\\\\\\\\\\\"Langfuse Ideas: Session-level scores\\\\\\\\\\\\\\\")\\\\\\\\n[Scoring dataset runs, e.g. precision, recall, f-value](https://github.com/orgs/langfuse/discussions/2511 \\\\\\\\\\\\\\\"Langfuse Ideas: Scoring dataset runs, e.g. precision, recall, f-value\\\\\\\\\\\\\\\")\\\\\\\\n[Adding userId / author to score (custom metadata)](https://github.com/orgs/langfuse/discussions/2469 \\\\\\\\\\\\\\\"Langfuse Ideas: Adding userId / author to score \\\\\\\\\\\\\\\\(custom metadata\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[Add string data type in score config](https://github.com/orgs/langfuse/discussions/2402 \\\\\\\\\\\\\\\"Langfuse Ideas: Add string data type in score config\\\\\\\\\\\\\\\")\\\\\\\\n[API to delete scores](https://github.com/orgs/langfuse/discussions/1133 \\\\\\\\\\\\\\\"Langfuse Ideas: API to delete scores\\\\\\\\\\\\\\\")\\\\\\\\nGitHubSupportGitHubIdeas\\\\\\\\nUpvotes[GitHubNew](https://github.com/orgs/langfuse/discussions/new/choose)\\\\\\\\n  * 9votes\\\\\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486)\\\\\\\\nmsanand\\\\\\\\u20223/25/2024\\\\\\\\u2022\\\\\\\\n2Resolved\\\\\\\\n  * 4votes\\\\\\\\n[Filter Categorical Score Values](https://github.com/orgs/langfuse/discussions/3797)\\\\\\\\nalabrashJr\\\\\\\\u202210/17/2024\\\\\\\\u2022\\\\\\\\n3Resolved\\\\\\\\n  * 3votes\\\\\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590)\\\\\\\\nanuras\\\\\\\\u20228/18/2025\\\\\\\\u2022\\\\\\\\n1Resolved\\\\\\\\n  * 3votes\\\\\\\\n[Support for Metric Calculation (Precision@K, Recall@K) and Adding Custom Metrics Use Case Overview](https://github.com/orgs/langfuse/discussions/5215)\\\\\\\\nsrimantacse\\\\\\\\u20221/27/2025\\\\\\\\u2022\\\\\\\\n2Resolved\\\\\\\\n  * 2votes\\\\\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556)\\\\\\\\nflabbergastedbd\\\\\\\\u20228/15/2025\\\\\\\\u2022\\\\\\\\n1\\\\\\\\n  * 1votes\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}]}\\\"}],\\\"isError\\\":false}\"},\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"ai\",\"attributes\":{}}}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "input": "{\"query\":\"N+1 evaluations Langfuse how to implement n+1 evaluation pattern batch eval per prediction vs per dataset Python JS\"}",
        "output": "{\"_meta\":{\"id\":\"inkeep-chatcmpl-40fc3a6d-a341-4906-ba8f-eecf1570e0e3\",\"model\":\"inkeep-rag\",\"choices\":[{\"finish_reason\":\"stop\",\"log_probs\":null,\"index\":0,\"message\":{\"content\":\"{\\\"content\\\": [{\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\", \\\"title\\\": \\\"Troubleshooting and FAQ\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[API - /scores not respecting value when operator '='](https://github.com/orgs/langfuse/discussions/8770 \\\\\\\"Langfuse Support: API - /scores not respecting value when operator '='\\\\\\\")\\\\n[Does LangFuse support evaluations on an existing dataset (.csv)](https://github.com/orgs/langfuse/discussions/8665 \\\\\\\"Langfuse Support: Does LangFuse support evaluations on an existing dataset \\\\\\\\(.csv\\\\\\\\)\\\\\\\")\\\\n[deleting evaluators](https://github.com/orgs/langfuse/discussions/8640 \\\\\\\"Langfuse Support: deleting evaluators\\\\\\\")\\\\n[Not able to import my LLM-as-a-Judge evals](https://github.com/orgs/langfuse/discussions/8636 \\\\\\\"Langfuse Support: Not able to import my LLM-as-a-Judge evals\\\\\\\")\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590 \\\\\\\"Langfuse Support: How to get experiment run scores programmatically?\\\\\\\")\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556 \\\\\\\"Langfuse Support: Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations\\\\\\\")\\\\n[Dataset runs restore from backups](https://github.com/orgs/langfuse/discussions/8534 \\\\\\\"Langfuse Support: Dataset runs restore from backups\\\\\\\")\\\\n[Getting scores efficiently via API for analytics purposes](https://github.com/orgs/langfuse/discussions/8520 \\\\\\\"Langfuse Support: Getting scores efficiently via API for analytics purposes\\\\\\\")\\\\n[run experiment on dataset](https://github.com/orgs/langfuse/discussions/8433 \\\\\\\"Langfuse Support: run experiment on dataset\\\\\\\")\\\\n[Experiments on Datasets with Human Annotated Labels?](https://github.com/orgs/langfuse/discussions/8414 \\\\\\\"Langfuse Support: Experiments on Datasets with Human Annotated Labels?\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[How to Recalculate Total Score on Dashboard After Updating User-Defined Model?](https://github.com/orgs/langfuse/discussions/8375 \\\\\\\"Langfuse Support: How to Recalculate Total Score on Dashboard After Updating User-Defined Model?\\\\\\\")\\\\n[How to filter by Categorical Scores in custom dashboard?](https://github.com/orgs/langfuse/discussions/8356 \\\\\\\"Langfuse Support: How to filter by Categorical Scores in custom dashboard?\\\\\\\")\\\\n[Running scheduled evals utilising LangFuse Datasets & Evaluators](https://github.com/orgs/langfuse/discussions/8355 \\\\\\\"Langfuse Support: Running scheduled evals utilising LangFuse Datasets & Evaluators\\\\\\\")\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\(\\\\\\\\)` method\\\\\\\")\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\")\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\"Langfuse Support: \\\\\\\\[Experiment\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\")\\\\n[Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing\\\\\\\")\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[2 traces generated instead of 1](https://github.com/orgs/langfuse/discussions/2244 \\\\\\\"Langfuse Support: 2 traces generated instead of 1\\\\\\\")\\\\n[Evaluations Not Available in Self-Hosted Version?](https://github.com/orgs/langfuse/discussions/2130 \\\\\\\"Langfuse Support: Evaluations Not Available in Self-Hosted Version?\\\\\\\")\\\\n[Deleting Duplicate Items in a Dataset](https://github.com/orgs/langfuse/discussions/2099 \\\\\\\"Langfuse Support: Deleting Duplicate Items in a Dataset\\\\\\\")\\\\n[Availability of evals when self-hosting](https://github.com/orgs/langfuse/discussions/2042 \\\\\\\"Langfuse Support: Availability of evals when self-hosting\\\\\\\")\\\\n[How to utilize a dataset w/ typescript and langchain integration](https://github.com/orgs/langfuse/discussions/1969 \\\\\\\"Langfuse Support: How to utilize a dataset w/ typescript and langchain integration\\\\\\\")\\\\n[Scoring a trace after the LLM chain returns](https://github.com/orgs/langfuse/discussions/1610 \\\\\\\"Langfuse Support: Scoring a trace after the LLM chain returns\\\\\\\")\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486 \\\\\\\"Langfuse Support: Update/delete score using python sdk\\\\\\\")\\\\n[Linking dataset run items with existing callback handler](https://github.com/orgs/langfuse/discussions/1445 \\\\\\\"Langfuse Support: Linking dataset run items with existing callback handler\\\\\\\")\\\\n[Datasets list / by id](https://github.com/orgs/langfuse/discussions/1420 \\\\\\\"Langfuse Support: Datasets list / by id\\\\\\\")\\\\n[Run items not appearing when linking to a trace and not a span or a generation](https://github.com/orgs/langfuse/discussions/1357 \\\\\\\"Langfuse Support: Run items not appearing when linking to a trace and not a span or a generation\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Option to run Experiments without traces](https://github.com/orgs/langfuse/discussions/8133 \\\\\\\"Langfuse Ideas: Option to run Experiments without traces\\\\\\\")\\\\n[Auto-generate dataset items](https://github.com/orgs/langfuse/discussions/8126 \\\\\\\"Langfuse Ideas: Auto-generate dataset items\\\\\\\")\\\\n[feature: support for creation of custom) model adapters](https://github.com/orgs/langfuse/discussions/8123 \\\\\\\"Langfuse Ideas: feature: support for creation of custom\\\\\\\\) model adapters\\\\\\\")\\\\n[Prompt experiment result download.](https://github.com/orgs/langfuse/discussions/8120 \\\\\\\"Langfuse Ideas: Prompt experiment result download.\\\\\\\")\\\\n[Multiple predictions per-item in a single run](https://github.com/orgs/langfuse/discussions/8040 \\\\\\\"Langfuse Ideas: Multiple predictions per-item in a single run\\\\\\\")\\\\n[Add evaluator(s) to python SDK (FernLangfuse)](https://github.com/orgs/langfuse/discussions/8018 \\\\\\\"Langfuse Ideas: Add evaluator\\\\\\\\(s\\\\\\\\) to python SDK \\\\\\\\(FernLangfuse\\\\\\\\)\\\\\\\")\\\\n[feat(dataset-runs): return all dataset run item scores in a given run](https://github.com/orgs/langfuse/discussions/8011 \\\\\\\"Langfuse Ideas: feat\\\\\\\\(dataset-runs\\\\\\\\): return all dataset run item scores in a given run\\\\\\\")\\\\n[Delete an evaluator from the evaluator library](https://github.com/orgs/langfuse/discussions/7960 \\\\\\\"Langfuse Ideas: Delete an evaluator from the evaluator library\\\\\\\")\\\\n[Archive or Delete Evaluators from the Evaluator Library](https://github.com/orgs/langfuse/discussions/7868 \\\\\\\"Langfuse Ideas: Archive or Delete Evaluators from the Evaluator Library\\\\\\\")\\\\n[Edit dataset run name, description](https://github.com/orgs/langfuse/discussions/7814 \\\\\\\"Langfuse Ideas: Edit dataset run name, description\\\\\\\")\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\", \\\"title\\\": \\\"Datasets\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Datasets - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")Dataset RunsDatasets\\\\nCopy as Markdown\\\\n# Datasets\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\n## Why use datasets?\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\n  * Create test cases for your application with real production traces\\\\n  * Collaboratively create and collect dataset items with your team\\\\n  * Have a single source of truth for your test data\\\\n## Get Started\\\\n### Creating a dataset\\\\nDatasets have a name which is unique within a project.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset(\\\\n name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # optional description\\\\n description=\\\\\\\"My first dataset\\\\\\\",\\\\n # optional metadata\\\\n metadata={\\\\n \\\\\\\"author\\\\\\\": \\\\\\\"Alice\\\\\\\",\\\\n \\\\\\\"date\\\\\\\": \\\\\\\"2022-01-01\\\\\\\",\\\\n \\\\\\\"type\\\\\\\": \\\\\\\"benchmark\\\\\\\"\\\\n }\\\\n)\\\\n```\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\n    \\\\n    \\\\n```\\\\nimport { LangfuseClient } from \\\\\\\"@langfuse/client\\\\\\\"\\\\n \\\\nconst langfuse = new LangfuseClient()\\\\n \\\\nawait langfuse.api.datasets.create({\\\\n name: \\\\\\\"<dataset_name>\\\\\\\",\\\\n // optional description\\\\n description: \\\\\\\"My first dataset\\\\\\\",\\\\n // optional metadata\\\\n metadata: {\\\\n author: \\\\\\\"Alice\\\\\\\",\\\\n date: \\\\\\\"2022-01-01\\\\\\\",\\\\n type: \\\\\\\"benchmark\\\\\\\",\\\\n },\\\\n});\\\\n```\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\n### Create new dataset items\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset_item(\\\\n dataset_name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # any python object or value, optional\\\\n input={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # any python object or value, optional\\\\n expected_output={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # metadata, optional\\\\n metadata={\\\\n \\\\\\\"model\\\\\\\": \\\\\\\"llama3\\\\\\\",\\\\n }\\\\n)\\\\n```\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/observability/sdk/python/evaluation\\\", \\\"title\\\": \\\"Evaluations\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Evaluations with the Langfuse Python SDK - Langfuse\\\\nDocs[Observability](/docs/observability/overview \\\\\\\"Observability\\\\\\\")\\\\n[SDKs](/docs/observability/sdk/overview \\\\\\\"SDKs\\\\\\\")\\\\n[Python](/docs/observability/sdk/python/overview \\\\\\\"Python\\\\\\\")Evaluation\\\\nCopy as Markdown\\\\n# Evaluations\\\\nThe Python SDK provides ways to evaluate your application. You can add custom scores to your traces and observations, or use the SDK to execute Dataset Runs.\\\\nThis page shows the evaluation methods that are supported by the Python SDK. Please refer to the [Evaluation documentation](/docs/evaluation/overview) for more information on how to evaluate your application in Langfuse.\\\\n## Create Scores\\\\nLangfuseSpan / LangfuseGeneration object methodsContext-aware methodsLow-level method\\\\n  * `span_or_generation_obj.score()`: Scores the specific observation object.\\\\n  * `span_or_generation_obj.score_trace()`: Scores the entire trace to which the object belongs.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nwith langfuse.start_as_current_generation(name=\\\\\\\"summary_generation\\\\\\\") as gen:\\\\n # ... LLM call ...\\\\n gen.update(output=\\\\\\\"summary text...\\\\\\\")\\\\n # Score this specific generation\\\\n gen.score(name=\\\\\\\"conciseness\\\\\\\", value=0.8, data_type=\\\\\\\"NUMERIC\\\\\\\")\\\\n # Score the overall trace\\\\n gen.score_trace(name=\\\\\\\"user_feedback_rating\\\\\\\", value=\\\\\\\"positive\\\\\\\", data_type=\\\\\\\"CATEGORICAL\\\\\\\")\\\\n```\\\\n  * `langfuse.score_current_span()`: Scores the currently active observation in the context.\\\\n  * `langfuse.score_current_trace()`: Scores the trace of the currently active observation.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nwith langfuse.start_as_current_span(name=\\\\\\\"complex_task\\\\\\\") as task_span:\\\\n # ... perform task ...\\\\n langfuse.score_current_span(name=\\\\\\\"task_component_quality\\\\\\\", value=True, data_type=\\\\\\\"BOOLEAN\\\\\\\")\\\\n # ...\\\\n if task_is_fully_successful:\\\\n langfuse.score_current_trace(name=\\\\\\\"overall_success\\\\\\\", value=1.0, data_type=\\\\\\\"NUMERIC\\\\\\\")\\\\n```\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n  * Creates a score for a specified `trace_id` and optionally `observation_id`.\\\\n  * Useful when IDs are known, or for scoring after the trace/observation has completed.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nlangfuse.create_score(\\\\n name=\\\\\\\"fact_check_accuracy\\\\\\\",\\\\n value=0.95, ///# Can be float for NUMERIC/BOOLEAN, string for CATEGORICAL\\\\n trace_id=\\\\\\\"abcdef1234567890abcdef1234567890\\\\\\\",\\\\n observation_id=\\\\\\\"1234567890abcdef\\\\\\\", ///# Optional: if scoring a specific observation\\\\n session_id=\\\\\\\"session_123\\\\\\\", ///# Optional: if scoring a specific session\\\\n data_type=\\\\\\\"NUMERIC\\\\\\\", ///# \\\\\\\"NUMERIC\\\\\\\", \\\\\\\"BOOLEAN\\\\\\\", \\\\\\\"CATEGORICAL\\\\\\\"\\\\n comment=\\\\\\\"Source verified for 95% of claims.\\\\\\\"\\\\n)\\\\n```\\\\n**Score Parameters:**\\\\nParameter| Type| Description  \\\\n---|---|---  \\\\n`name`| `str`| Name of the score (e.g., \\\\u201crelevance\\\\u201d, \\\\u201caccuracy\\\\u201d). **Required.**  \\\\n`value`| `Union[float, str]`| Score value. Float for `NUMERIC`/`BOOLEAN`, string for `CATEGORICAL`. **Required.**  \\\\n`trace_id`| `str`| ID of the trace to associate with (for `create_score`). **Required.**  \\\\n`observation_id`| `Optional[str]`| ID of the specific observation to score (for `create_score`).  \\\\n`session_id`| `Optional[str]`| ID of the specific session to score (for `create_score`).  \\\\n`score_id`| `Optional[str]`| Custom ID for the score (auto-generated if None).  \\\\n`data_type`| `Optional[ScoreDataType]`| `\\\\\\\"NUMERIC\\\\\\\"`, `\\\\\\\"BOOLEAN\\\\\\\"`, or `\\\\\\\"CATEGORICAL\\\\\\\"`. Inferred if not provided based on value type and score config on server.  \\\\n`comment`| `Optional[str]`| Optional comment or explanation for the score.  \\\\n`config_id`| `Optional[str]`| Optional ID of a pre-defined score configuration in Langfuse.  \\\\n  \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThe most powerful way to use datasets is by linking your application\\\\u2019s executions (traces) to specific dataset items when performing an evaluation run. The `DatasetItemClient.run()` method provides a context manager to streamline this process.\\\\n**How`item.run()` works:**\\\\nWhen you use `with item.run(run_name=\\\\\\\"your_eval_run_name\\\\\\\") as root_span:`:\\\\n  1. **Trace Creation** : A new Langfuse trace is initiated specifically for processing this dataset item within the context of the named run.\\\\n  2. **Trace Naming & Metadata**: \\\\n    * The trace is automatically named (e.g., \\\\u201cDataset run: your_eval_run_name\\\\u201d).\\\\n    * Essential metadata is added to this trace, including `dataset_item_id` (the ID of `item`), `run_name`, and `dataset_id`.\\\\n  3. **DatasetRunItem Linking** : The SDK makes an API call to Langfuse to create a `DatasetRunItem`. This backend object formally links: \\\\n    * The `dataset_item_id`\\\\n    * The `trace_id` of the newly created trace\\\\n    * The provided `run_name`\\\\n    * Any `run_metadata` or `run_description` you pass to `item.run()`. This linkage is what populates the \\\\u201cRuns\\\\u201d tab for your dataset in the Langfuse UI, allowing you to see all traces associated with a particular evaluation run.\\\\n  4. **Contextual Span** : The context manager yields `root_span`, which is a `LangfuseSpan` object representing the root span of this new trace.\\\\n  5. **Automatic Nesting** : Any Langfuse observations (spans or generations) created _inside_ the `with` block will automatically become children of `root_span` and thus part of the trace linked to this dataset item and run.\\\\n**Example:**\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\ndataset_name = \\\\\\\"qna-eval\\\\\\\"\\\\ncurrent_run_name = \\\\\\\"qna_model_v3_run_05_20\\\\\\\" # Identifies this specific evaluation run\\\\n \\\\n///# Assume 'my_qna_app' is your instrumented application function\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"def my_qna_app(question: str, context: str, item_id: str, run_name: str):\\\\n with langfuse.start_as_current_generation(\\\\n name=\\\\\\\"qna-llm-call\\\\\\\",\\\\n input={\\\\\\\"question\\\\\\\": question, \\\\\\\"context\\\\\\\": context},\\\\n metadata={\\\\\\\"item_id\\\\\\\": item_id, \\\\\\\"run\\\\\\\": run_name}, ///# Example metadata for the generation\\\\n model=\\\\\\\"gpt-4o\\\\\\\"\\\\n ) as generation:\\\\n # Simulate LLM call\\\\n answer = f\\\\\\\"Answer to '{question}' using context.\\\\\\\" # Replace with actual LLM call\\\\n generation.update(output={\\\\\\\"answer\\\\\\\": answer})\\\\n \\\\n # Update the trace with the input and output\\\\n generation.update_trace(\\\\n input={\\\\\\\"question\\\\\\\": question, \\\\\\\"context\\\\\\\": context},\\\\n output={\\\\\\\"answer\\\\\\\": answer},\\\\n )\\\\n \\\\n return answer\\\\n \\\\ndataset = langfuse.get_dataset(name=dataset_name) ///# Fetch your pre-populated dataset\\\\n \\\\nfor item in dataset.items:\\\\n print(f\\\\\\\"Running evaluation for item: {item.id} (Input: {item.input})\\\\\\\")\\\\n \\\\n # Use the item.run() context manager\\\\n with item.run(\\\\n run_name=current_run_name,\\\\n run_metadata={\\\\\\\"model_provider\\\\\\\": \\\\\\\"OpenAI\\\\\\\", \\\\\\\"temperature_setting\\\\\\\": 0.7},\\\\n run_description=\\\\\\\"Evaluation run for Q&A model v3 on May 20th\\\\\\\"\\\\n ) as root_span: ///# root_span is the root span of the new trace for this item and run.\\\\n # All subsequent langfuse operations within this block are part of this trace.\\\\n \\\\n # Call your application logic\\\\n generated_answer = my_qna_app(\\\\n question=item.input[\\\\\\\"question\\\\\\\"],\\\\n context=item.input[\\\\\\\"context\\\\\\\"],\\\\n item_id=item.id,\\\\n run_name=current_run_name\\\\n )\\\\n \\\\n print(f\\\\\\\" Item {item.id} processed. Trace ID: {root_span.trace_id}\\\\\\\")\\\\n \\\\n # Optionally, score the result against the expected output\\\\n if item.expected_output and generated_answer == item.expected_output.get(\\\\\\\"answer\\\\\\\"):\\\\n root_span.score_trace(name=\\\\\\\"exact_match\\\\\\\", value=1.0)\\\\n else:\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n root_span.score_trace(name=\\\\\\\"exact_match\\\\\\\", value=0.0)\\\\n \\\\nprint(f\\\\\\\"\\\\\\\\nFinished processing dataset '{dataset_name}' for run '{current_run_name}'.\\\\\\\")\\\\n```\\\\nBy using `item.run()`, you ensure each dataset item\\\\u2019s processing is neatly encapsulated in its own trace, and these traces are aggregated under the specified `run_name` in the Langfuse UI. This allows for systematic review of results, comparison across runs, and deep dives into individual processing traces.\\\\nLast updated on August 20, 2025\\\\n[Instrumentation](/docs/observability/sdk/python/instrumentation \\\\\\\"Instrumentation\\\\\\\")\\\\n[Advanced usage](/docs/observability/sdk/python/advanced-usage \\\\\\\"Advanced usage\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\", \\\"title\\\": \\\"All new Datasets, Experimentation and Evaluation documentation\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"## New Datasets and Evaluation documentation\\\\nBased on this week\\\\u2019s changes, we have have restructured the documentation to:\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\n### Evaluation\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Evaluation documentation](/docs/scores)\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\n    \\\\n    \\\\n```\\\\nPOST /api/public/scores\\\\n```\\\\nAdd scores via Python or JS SDK.\\\\nExample (Python)\\\\n    \\\\n    \\\\n```\\\\nlangfuse.score(\\\\n trace_id=\\\\\\\"123\\\\\\\",\\\\n name=\\\\\\\"my_custom_evaluator\\\\\\\",\\\\n value=0.5,\\\\n)\\\\n```\\\\n### Datasets\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\", \\\"title\\\": \\\"Remote Dataset Runs\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Task function that answers geography questions\\\\\\\"\\\\\\\"\\\\\\\"\\\\n question = item[\\\\\\\"input\\\\\\\"]\\\\n response = OpenAI().chat.completions.create(\\\\n model=\\\\\\\"gpt-4\\\\\\\",\\\\n messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": question}]\\\\n )\\\\n return response.choices[0].message.content\\\\n \\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\"\\\\\\\"\\\\\\\"\\\\n if expected_output and expected_output.lower() in output.lower():\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=1.0)\\\\n \\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=0.0)\\\\n \\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\"\\\\\\\"\\\\\\\"\\\\n accuracies = [\\\\n eval.value for result in item_results\\\\n for eval in result.evaluations if eval.name == \\\\\\\"accuracy\\\\\\\"\\\\n ]\\\\n \\\\n if not accuracies:\\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=None)\\\\n \\\\n avg = sum(accuracies) / len(accuracies)\\\\n \\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=avg, comment=f\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\")\\\\n \\\\n@pytest.fixture\\\\ndef langfuse_client():\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Initialize Langfuse client for testing\\\\\\\"\\\\\\\"\\\\\\\"\\\\n return get_client()\\\\n \\\\ndef test_geography_accuracy_passes(langfuse_client):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Test that passes when accuracy is above threshold\\\\\\\"\\\\\\\"\\\\\\\"\\\\n result = langfuse_client.run_experiment(\\\\n name=\\\\\\\"Geography Test - Should Pass\\\\\\\",\\\\n data=test_data,\\\\n task=geography_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy_evaluator]\\\\n )\\\\n \\\\n # Access the run evaluator result directly\\\\n avg_accuracy = next(\\\\n eval.value for eval in result.run_evaluations\\\\n if eval.name == \\\\\\\"avg_accuracy\\\\\\\"\\\\n )\\\\n \\\\n # Assert minimum accuracy threshold\\\\n assert avg_accuracy >= 0.8, f\\\\\\\"Average accuracy {avg_accuracy:.2f} below threshold 0.8\\\\\\\"\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\ndef test_geography_accuracy_fails(langfuse_client):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Example test that demonstrates failure conditions\\\\\\\"\\\\\\\"\\\\\\\"\\\\n # Use a weaker model or harder questions to demonstrate test failure\\\\n def failing_task(*, item, **kwargs):\\\\n # Simulate a task that gives wrong answers\\\\n return \\\\\\\"I don't know\\\\\\\"\\\\n \\\\n result = langfuse_client.run_experiment(\\\\n name=\\\\\\\"Geography Test - Should Fail\\\\\\\",\\\\n data=test_data,\\\\n task=failing_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy_evaluator]\\\\n )\\\\n \\\\n # Access the run evaluator result directly\\\\n avg_accuracy = next(\\\\n eval.value for eval in result.run_evaluations\\\\n if eval.name == \\\\\\\"avg_accuracy\\\\\\\"\\\\n )\\\\n \\\\n # This test will fail because the task gives wrong answers\\\\n with pytest.raises(AssertionError):\\\\n assert avg_accuracy >= 0.8, f\\\\\\\"Expected test to fail with low accuracy: {avg_accuracy:.2f}\\\\\\\"\\\\n```\\\\n    \\\\n    \\\\n```\\\\n// test/geography-experiment.test.ts\\\\nimport { describe, it, expect, beforeAll, afterAll } from 'vitest';\\\\nimport { OpenAI } from \\\\\\\"openai\\\\\\\";\\\\nimport { NodeSDK } from \\\\\\\"@opentelemetry/sdk-node\\\\\\\";\\\\nimport { LangfuseClient, ExperimentItem } from \\\\\\\"@langfuse/client\\\\\\\";\\\\nimport { observeOpenAI } from \\\\\\\"@langfuse/openai\\\\\\\";\\\\nimport { LangfuseSpanProcessor } from \\\\\\\"@langfuse/otel\\\\\\\";\\\\n \\\\n// Test data for European capitals\\\\nconst testData: ExperimentItem[] = [\\\\n { input: \\\\\\\"What is the capital of France?\\\\\\\", expected: \\\\\\\"Paris\\\\\\\" },\\\\n { input: \\\\\\\"What is the capital of Germany?\\\\\\\", expected: \\\\\\\"Berlin\\\\\\\" },\\\\n { input: \\\\\\\"What is the capital of Spain?\\\\\\\", expected: \\\\\\\"Madrid\\\\\\\" },\\\\n];\\\\n \\\\nlet otelSdk: NodeSDK;\\\\nlet langfuse: LangfuseClient;\\\\n \\\\nbeforeAll(async () => {\\\\n // Initialize OpenTelemetry\\\\n otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\n otelSdk.start();\\\\n \\\\n // Initialize Langfuse client\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\n    \\\\n    \\\\n```\\\\nimport { Factuality, Levenshtein } from \\\\\\\"autoevals\\\\\\\";\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\"@langfuse/client\\\\\\\";\\\\n \\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\n \\\\n// Use with additional parameters\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\n Factuality,\\\\n { model: \\\\\\\"gpt-4o\\\\\\\" } // Additional AutoEvals parameters\\\\n);\\\\n \\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"AutoEvals Integration Test\\\\\\\",\\\\n data: testDataset,\\\\n task: myTask,\\\\n evaluators: [\\\\n factualityEvaluator,\\\\n levenshteinEvaluator,\\\\n customFactualityEvaluator\\\\n ]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\", \\\"title\\\": \\\"Remote Dataset Runs\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Remote Dataset Runs via the SDK - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\"Dataset Runs\\\\\\\")Remote Run\\\\nCopy as Markdown\\\\n# Remote Dataset Runs\\\\nOnce you [created a dataset](/docs/evaluation/dataset-runs/datasets), you can use the dataset to test how your application performs on different inputs. Remote Dataset Runs are used to programmatically loop your applications or prompts through a dataset and optionally apply Evaluation Methods to the results.\\\\nThey are called \\\\u201cRemote Dataset Runs\\\\u201d because they can make use of \\\\u201cremote\\\\u201d or external logic and code.\\\\nOptionally, you can also trigger Remote Dataset Runs via the Langfuse UI which will call them via a webhook.\\\\n## Why use Remote Dataset Runs?\\\\n  * Full flexibility to use your own application logic\\\\n  * Use custom scoring functions to evaluate the outputs\\\\n  * Run multiple experiments on the same dataset in parallel\\\\n  * Easy to integrate with your existing evaluation infrastructure\\\\nSequence Diagram\\\\n## Experiment runner SDK\\\\nBoth the Python and JS/TS SDKs provide a high-level abstraction for running an experiment on a dataset. The dataset can be both local or hosted on Langfuse. Using the Experiment runner is the recommended way to run an experiment on a dataset with our SDK.\\\\nThe experiment runner automatically handles:\\\\n  * **Concurrent execution** of tasks with configurable limits\\\\n  * **Automatic tracing** of all executions for observability\\\\n  * **Flexible evaluation** with both item-level and run-level evaluators\\\\n  * **Error isolation** so individual failures don\\\\u2019t stop the experiment\\\\n  * **Dataset integration** for easy comparison and tracking\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThe experiment runner SDK supports both datasets hosted on Langfuse and datasets hosted locally. If you are using a dataset hosted on Langfuse for your experiment, the SDK will automatically create a dataset run for you that you can inspect and compare in the Langfuse UI. For locally hosted datasets not on Langfuse, only traces and scores (if evaluations are used) are tracked in Langfuse.\\\\n### Basic Usage\\\\nStart with the simplest possible experiment to test your task function on local data. If you already have a dataset in Langfuse, see here.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\nfrom langfuse.openai import OpenAI\\\\n \\\\n///# Initialize client\\\\nlangfuse = get_client()\\\\n \\\\n///# Define your task function\\\\ndef my_task(*, item, **kwargs):\\\\n question = item[\\\\\\\"input\\\\\\\"]\\\\n response = OpenAI().chat.completions.create(\\\\n model=\\\\\\\"gpt-4.1\\\\\\\", messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": question}]\\\\n )\\\\n \\\\n return response.choices[0].message.content\\\\n \\\\n \\\\n///# Run experiment on local data\\\\nlocal_data = [\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of France?\\\\\\\"},\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of Germany?\\\\\\\"},\\\\n]\\\\n \\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Geography Quiz\\\\\\\",\\\\n description=\\\\\\\"Testing basic functionality\\\\\\\",\\\\n data=local_data,\\\\n task=my_task,\\\\n)\\\\n \\\\n///# Use format method to display results\\\\nprint(result.format())\\\\n```\\\\nMake sure that OpenTelemetry is properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\n    \\\\n    \\\\n```\\\\nimport { OpenAI } from \\\\\\\"openai\\\\\\\";\\\\nimport { NodeSDK } from \\\\\\\"@opentelemetry/sdk-node\\\\\\\";\\\\n \\\\nimport {\\\\n LangfuseClient,\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n ExperimentTask,\\\\n ExperimentItem,\\\\n} from \\\\\\\"@langfuse/client\\\\\\\";\\\\nimport { observeOpenAI } from \\\\\\\"@langfuse/openai\\\\\\\";\\\\nimport { LangfuseSpanProcessor } from \\\\\\\"@langfuse/otel\\\\\\\";\\\\n \\\\n// Initialize OpenTelemetry\\\\nconst otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\notelSdk.start();\\\\n \\\\n// Initialize client\\\\nconst langfuse = new LangfuseClient();\\\\n \\\\n// Run experiment on local data\\\\nconst localData: ExperimentItem[] = [\\\\n { input: \\\\\\\"What is the capital of France?\\\\\\\" },\\\\n { input: \\\\\\\"What is the capital of Germany?\\\\\\\" },\\\\n];\\\\n \\\\n// Define your task function\\\\nconst myTask: ExperimentTask = async (item) => {\\\\n const question = item.input;\\\\n \\\\n const response = await observeOpenAI(new OpenAI()).chat.completions.create({\\\\n model: \\\\\\\"gpt-4.1\\\\\\\",\\\\n messages: [\\\\n {\\\\n role: \\\\\\\"user\\\\\\\",\\\\n content: question,\\\\n },\\\\n ],\\\\n });\\\\n \\\\n return response;\\\\n};\\\\n \\\\n// Run the experiment\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Geography Quiz\\\\\\\",\\\\n description: \\\\\\\"Testing basic functionality\\\\\\\",\\\\n data: localData,\\\\n task: myTask,\\\\n});\\\\n \\\\n// Print formatted result\\\\nconsole.log(await result.format());\\\\n \\\\n// Important: shut down OTEL SDK to deliver traces\\\\nawait otelSdk.shutdown();\\\\n```\\\\n **Note for JS/TS SDK** : OpenTelemetry must be properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\nWhen running experiments on local data, only traces are created in Langfuse - no dataset runs are generated. Each task execution creates an individual trace for observability and debugging.\\\\n### Usage with Langfuse Datasets\\\\nRun experiments directly on datasets stored in Langfuse for automatic tracing and comparison.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\n///# Get dataset from Langfuse\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\ndataset = langfuse.get_dataset(\\\\\\\"my-evaluation-dataset\\\\\\\")\\\\n \\\\n///# Run experiment directly on the dataset\\\\nresult = dataset.run_experiment(\\\\n name=\\\\\\\"Production Model Test\\\\\\\",\\\\n description=\\\\\\\"Monthly evaluation of our production model\\\\\\\",\\\\n task=my_task\\\\n)\\\\n \\\\n///# Use format method to display results\\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\n// Get dataset from Langfuse\\\\nconst dataset = await langfuse.dataset.get(\\\\\\\"my-evaluation-dataset\\\\\\\");\\\\n \\\\n// Run experiment directly on the dataset\\\\nconst result = await dataset.runExperiment({\\\\n name: \\\\\\\"Production Model Test\\\\\\\",\\\\n description: \\\\\\\"Monthly evaluation of our production model\\\\\\\",\\\\n task: myTask\\\\n});\\\\n \\\\n// Use format method to display results\\\\nconsole.log(await result.format());\\\\n \\\\n// Important: shut down OpenTelemetry to ensure traces are sent to Langfuse\\\\nawait otelSdk.shutdown()\\\\n```\\\\nWhen using Langfuse datasets, dataset runs are automatically created in Langfuse and are available for comparison in the UI. This enables tracking experiment performance over time and comparing different approaches on the same dataset.\\\\n### Advanced Features\\\\nEnhance your experiments with evaluators and advanced configuration options.\\\\n#### Evaluators\\\\nEvaluators assess the quality of task outputs at the item level. They receive the input, metadata, output, and expected output for each item and return evaluation metrics that are reported as scores on the traces in Langfuse.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import Evaluation\\\\n \\\\n///# Define evaluation functions\\\\ndef accuracy_evaluator(*, input, output, expected_output, metadata, **kwargs):\\\\n if expected_output and expected_output.lower() in output.lower():\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=1.0, comment=\\\\\\\"Correct answer found\\\\\\\")\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=0.0, comment=\\\\\\\"Incorrect answer\\\\\\\")\\\\n \\\\ndef length_evaluator(*, input, output, **kwargs):\\\\n return Evaluation(name=\\\\\\\"response_length\\\\\\\", value=len(output), comment=f\\\\\\\"Response has {len(output)} characters\\\\\\\")\\\\n \\\\n///# Use multiple evaluators\\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Multi-metric Evaluation\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[accuracy_evaluator, length_evaluator]\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\n// Define evaluation functions\\\\nconst accuracyEvaluator = async ({ input, output, expectedOutput }) => {\\\\n if (expectedOutput && output.toLowerCase().includes(expectedOutput.toLowerCase())) {\\\\n return {\\\\n name: \\\\\\\"accuracy\\\\\\\",\\\\n value: 1.0,\\\\n comment: \\\\\\\"Correct answer found\\\\\\\"\\\\n };\\\\n }\\\\n return {\\\\n name: \\\\\\\"accuracy\\\\\\\",\\\\n value: 0.0,\\\\n comment: \\\\\\\"Incorrect answer\\\\\\\"\\\\n };\\\\n};\\\\n \\\\nconst lengthEvaluator = async ({ input, output }) => {\\\\n return {\\\\n name: \\\\\\\"response_length\\\\\\\",\\\\n value: output.length,\\\\n comment: `Response has ${output.length} characters`\\\\n };\\\\n};\\\\n \\\\n// Use multiple evaluators\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Multi-metric Evaluation\\\\\\\",\\\\n data: testData,\\\\n task: myTask,\\\\n evaluators: [accuracyEvaluator, lengthEvaluator]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Run-level Evaluators\\\\nRun-level evaluators assess the full experiment results and compute aggregate metrics. When run on Langfuse datasets, these scores are attached to the full dataset run for tracking overall experiment performance.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import Evaluation\\\\n \\\\ndef average_accuracy(*, item_results, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Calculate average accuracy across all items\\\\\\\"\\\\\\\"\\\\\\\"\\\\n accuracies = [\\\\n eval.value for result in item_results\\\\n for eval in result.evaluations\\\\n if eval.name == \\\\\\\"accuracy\\\\\\\"\\\\n ]\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n if not accuracies:\\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=None)\\\\n \\\\n avg = sum(accuracies) / len(accuracies)\\\\n \\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=avg, comment=f\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\")\\\\n \\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Comprehensive Analysis\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy]\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst averageAccuracy = async ({ itemResults }) => {\\\\n // Calculate average accuracy across all items\\\\n const accuracies = itemResults\\\\n .flatMap(result => result.evaluations)\\\\n .filter(evaluation => evaluation.name === \\\\\\\"accuracy\\\\\\\")\\\\n .map(evaluation => evaluation.value as number);\\\\n \\\\n if (accuracies.length === 0) {\\\\n return { name: \\\\\\\"avg_accuracy\\\\\\\", value: null };\\\\n }\\\\n \\\\n const avg = accuracies.reduce((sum, val) => sum + val, 0) / accuracies.length;\\\\n \\\\n return {\\\\n name: \\\\\\\"avg_accuracy\\\\\\\",\\\\n value: avg,\\\\n comment: `Average accuracy: ${(avg * 100).toFixed(1)}%`\\\\n };\\\\n};\\\\n \\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Comprehensive Analysis\\\\\\\",\\\\n data: testData,\\\\n task: myTask,\\\\n evaluators: [accuracyEvaluator],\\\\n runEvaluators: [averageAccuracy]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Async Tasks and Evaluators\\\\nBoth task functions and evaluators can be asynchronous.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nimport asyncio\\\\nfrom langfuse.openai import AsyncOpenAI\\\\n \\\\nasync def async_llm_task(*, item, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Async task using OpenAI\\\\\\\"\\\\\\\"\\\\\\\"\\\\n client = AsyncOpenAI()\\\\n response = await client.chat.completions.create(\\\\n model=\\\\\\\"gpt-4\\\\\\\",\\\\n messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": item[\\\\\\\"input\\\\\\\"]}]\\\\n )\\\\n \\\\n return response.choices[0].message.content\\\\n \\\\n///# Works seamlessly with async functions\\\\nresult = langfuse.run_experiment(\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n name=\\\\\\\"Async Experiment\\\\\\\",\\\\n data=test_data,\\\\n task=async_llm_task,\\\\n max_concurrency=5 # Control concurrent API calls\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\nimport OpenAI from \\\\\\\"openai\\\\\\\";\\\\n \\\\nconst asyncLlmTask = async (item) => {\\\\n // Async task using OpenAI\\\\n const client = new OpenAI();\\\\n const response = await client.chat.completions.create({\\\\n model: \\\\\\\"gpt-4\\\\\\\",\\\\n messages: [{ role: \\\\\\\"user\\\\\\\", content: item.input }]\\\\n });\\\\n \\\\n return response.choices[0].message.content;\\\\n};\\\\n \\\\n// Works seamlessly with async functions\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Async Experiment\\\\\\\",\\\\n data: testData,\\\\n task: asyncLlmTask,\\\\n maxConcurrency: 5 // Control concurrent API calls\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Configuration Options\\\\nCustomize experiment behavior with various configuration options.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Configurable Experiment\\\\\\\",\\\\n run_name=\\\\\\\"Custom Run Name\\\\\\\", ///# will be dataset run name if dataset is used\\\\n description=\\\\\\\"Experiment with custom configuration\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy],\\\\n max_concurrency=10, ///# Max concurrent executions\\\\n metadata={ ///# Attached to all traces\\\\n \\\\\\\"model\\\\\\\": \\\\\\\"gpt-4\\\\\\\",\\\\n \\\\\\\"temperature\\\\\\\": 0.7,\\\\n \\\\\\\"version\\\\\\\": \\\\\\\"v1.2.0\\\\\\\"\\\\n }\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Configurable Experiment\\\\\\\",\\\\n runName: \\\\\\\"Custom Run Name\\\\\\\", // will be dataset run name if dataset is used\\\\n description: \\\\\\\"Experiment with custom configuration\\\\\\\",\\\\n data: testData,\\\\n task: myTask,\\\\n evaluators: [accuracyEvaluator],\\\\n runEvaluators: [averageAccuracy],\\\\n maxConcurrency: 10, // Max concurrent executions\\\\n metadata: { // Attached to all traces\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n model: \\\\\\\"gpt-4\\\\\\\",\\\\n temperature: 0.7,\\\\n version: \\\\\\\"v1.2.0\\\\\\\"\\\\n }\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Testing in CI Environments\\\\nIntegrate the experiment runner with testing frameworks like Pytest and Vitest to run automated evaluations in your CI pipeline. Use evaluators to create assertions that can fail tests based on evaluation results.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\n///# test_geography_experiment.py\\\\nimport pytest\\\\nfrom langfuse import get_client, Evaluation\\\\nfrom langfuse.openai import OpenAI\\\\n \\\\n///# Test data for European capitals\\\\ntest_data = [\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of France?\\\\\\\", \\\\\\\"expected\\\\\\\": \\\\\\\"Paris\\\\\\\"},\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of Germany?\\\\\\\", \\\\\\\"expected\\\\\\\": \\\\\\\"Berlin\\\\\\\"},\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of Spain?\\\\\\\", \\\\\\\"expected\\\\\\\": \\\\\\\"Madrid\\\\\\\"},\\\\n]\\\\n \\\\ndef geography_task(*, item, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Task function that answers geography questions\\\\\\\"\\\\\\\"\\\\\\\"\\\\n question = item[\\\\\\\"input\\\\\\\"]\\\\n response = OpenAI().chat.completions.create(\\\\n model=\\\\\\\"gpt-4\\\\\\\",\\\\n messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": question}]\\\\n )\\\\n return response.choices[0].message.content\\\\n \\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\"\\\\\\\"\\\\\\\"\\\\n if expected_output and expected_output.lower() in output.lower():\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=1.0)\\\\n \\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=0.0)\\\\n \\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\"\\\\\\\"\\\\\\\"\\\\n accuracies = [\\\\n eval.value for result in item_results\\\\n for eval in result.evaluations if eval.name == \\\\\\\"accuracy\\\\\\\"\\\\n ]\\\\n \\\\n if not accuracies:\\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=None)\\\\n \\\\n avg = sum(accuracies) / len(accuracies)\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThese examples show how to use the experiment runner\\\\u2019s evaluation results to create meaningful test assertions in your CI pipeline. Tests can fail when accuracy drops below acceptable thresholds, ensuring model quality standards are maintained automatically.\\\\n### Autoevals Integration\\\\nAccess pre-built evaluation functions through the [autoevals library](https://github.com/braintrustdata/autoevals) integration.\\\\nPython SDKJS/TS SDK\\\\nThe Python SDK supports AutoEvals evaluators through direct integration:\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse.experiment import create_evaluator_from_autoevals\\\\nfrom autoevals.llm import Factuality\\\\n \\\\nevaluator = create_evaluator_from_autoevals(Factuality())\\\\n \\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Autoevals Integration Test\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[evaluator]\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\n    \\\\n    \\\\n```\\\\nimport { Factuality, Levenshtein } from \\\\\\\"autoevals\\\\\\\";\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\"@langfuse/client\\\\\\\";\\\\n \\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\n \\\\n// Use with additional parameters\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\n Factuality,\\\\n { model: \\\\\\\"gpt-4o\\\\\\\" } // Additional AutoEvals parameters\\\\n);\\\\n \\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"AutoEvals Integration Test\\\\\\\",\\\\n data: testDataset,\\\\n task: myTask,\\\\n evaluators: [\\\\n factualityEvaluator,\\\\n levenshteinEvaluator,\\\\n customFactualityEvaluator\\\\n ]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n## Low-level SDK methods\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\n comment: \\\\\\\"This is a comment\\\\\\\", // optional, useful to add reasoning\\\\n });\\\\n}\\\\n \\\\n// Flush the langfuse client to ensure all score data is sent to the server at the end of the experiment run\\\\nawait langfuse.flush();\\\\n```\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\nfrom langfuse.langchain import CallbackHandler\\\\n///#from .app import my_llm_application\\\\n \\\\n///# Load the dataset\\\\ndataset = get_client().get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\n///# Initialize the Langfuse handler\\\\nlangfuse_handler = CallbackHandler()\\\\n \\\\n///# Loop over the dataset items\\\\nfor item in dataset.items:\\\\n # Use the item.run() context manager for automatic trace linking\\\\n with item.run(\\\\n run_name=\\\\\\\"<run_name>\\\\\\\",\\\\n run_description=\\\\\\\"My first run\\\\\\\",\\\\n run_metadata={\\\\\\\"model\\\\\\\": \\\\\\\"llama3\\\\\\\"},\\\\n ) as root_span:\\\\n # Execute your LLM-app against the dataset item input\\\\n output = my_langchain_chain(item.input, langfuse_handler)\\\\n \\\\n # Update top-level trace input and output\\\\n root_span.update_trace(input=item.input, output=output.content)\\\\n \\\\n # Optionally: Add scores computed in your experiment runner, e.g. json equality check\\\\n root_span.score_trace(\\\\n name=\\\\\\\"<example_eval>\\\\\\\",\\\\n value=my_eval_fn(item.input, output, item.expected_output),\\\\n comment=\\\\\\\"This is a comment\\\\\\\", ///# optional, useful to add reasoning\\\\n )\\\\n \\\\n///# Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run\\\\nget_client().flush()\\\\n```\\\\n    \\\\n    \\\\n```\\\\nimport { LangfuseClient } from \\\\\\\"@langfuse/client\\\\\\\";\\\\nimport { CallbackHandler } from \\\\\\\"@langfuse/langchain\\\\\\\";\\\\n...\\\\n \\\\nconst langfuse = new LangfuseClient()\\\\nconst runName = \\\\\\\"my-dataset-run\\\\\\\";\\\\nfor (const item of dataset.items) {\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/guides/cookbook/example_external_evaluation_pipelines\\\", \\\"title\\\": \\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline - Langfuse\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\"Cookbooks\\\\\\\")External Evaluation Pipelines\\\\nCopy as Markdown\\\\nThis is a Jupyter notebook\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\n# Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\\nThis cookbook explains how to build an external evaluation pipeline to measure the performance of your production LLM application using Langfuse.\\\\nAs a rule of thumb, we encourage you to check first if the [evaluations in the Langfuse UI](https://langfuse.com/docs/scores/model-based-evals) cover your use case. If your needs go beyond these, you can still implement in Langfuse custom evaluation templates without code.\\\\nConsider implementing an external evaluation pipeline if you need:\\\\n  * More control over **when** traces get evaluated. You could schedule the pipeline to run at specific times or responding to event-based triggers like Webhooks.\\\\n  * Greater flexibility with your custom evaluations, when your needs go beyond what\\\\u2019s possible with the Langfuse UI\\\\n  * Version control for your custom evaluations\\\\n  * The ability to evaluate data using existing evaluation frameworks\\\\nIf your use case meets any of this situations, let\\\\u2019s go ahead and implement your first external evaluation pipeline!\\\\n* * *\\\\nBy the end of this cookbook, you\\\\u2019ll be able to:\\\\n  * Create a synthetic dataset to test your models.\\\\n  * Use the Langfuse client to gather and filter traces of previous model runs\\\\n  * Evaluate these traces offline and incrementally\\\\n  * Add scores to existing Langfuse traces\\\\nConceptually, we will implement the following architecture:\\\\n* * *\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n**Note** : While we\\\\u2019re using a Jupyter notebook for this cookbook, in production you\\\\u2019d use your preferred orchestration tool. Just make sure to extract the code into a .py file and ensure all dependencies are available at runtime.\\\\n## (Prep-work) Loading synthetic traces to Langfuse\\\\nIn this demo, we\\\\u2019ll build a mock application: a science communicator LLM that explains any topic in an engaging and approachable way.\\\\nSince we don\\\\u2019t have real user data, our first step is to create a synthetic dataset. We\\\\u2019ll generate a variety of potential questions that real users might ask. While this is a great way to kickstart your LLM development, collecting real user queries as soon as possible is invaluable.\\\\nYou can get your Langfuse API keys [here](https://cloud.langfuse.com/) and OpenAI API key [here](https://platform.openai.com/api-keys)\\\\n    \\\\n    \\\\n```\\\\n%pip install langfuse openai deepeval --upgrade\\\\n```\\\\n    \\\\n    \\\\n```\\\\nimport os\\\\n \\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\nos.environ[\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\"] = \\\\\\\"pk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\"] = \\\\\\\"sk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddea\\\\ud83c\\\\uddfa EU region\\\\n///# os.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://us.cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddfa\\\\ud83c\\\\uddf8 US region\\\\n \\\\n///# Your openai key\\\\nos.environ[\\\\\\\"OPENAI_API_KEY\\\\\\\"] = \\\\\\\"sk-proj-...\\\\\\\"\\\\n```\\\\nLet\\\\u2019s go ahead and generate a list of topic suggestions that we can later query to our application.\\\\n    \\\\n    \\\\n```\\\\nimport openai\\\\n \\\\ntopic_suggestion = \\\\\\\"\\\\\\\"\\\\\\\" You're a world-class journalist, specialized\\\\nin figuring out which are the topics that excite people the most.\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nBATCH_SIZE = 10\\\\nTOTAL_TRACES = 50\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nnow = datetime.now()\\\\nfive_am_today = datetime(now.year, now.month, now.day, 5, 0)\\\\nfive_am_yesterday = five_am_today - timedelta(days=1)\\\\n \\\\ntraces_batch = langfuse.api.trace.list(page=1,\\\\n limit=BATCH_SIZE,\\\\n tags=\\\\\\\"ext_eval_pipelines\\\\\\\",\\\\n from_timestamp=five_am_yesterday,\\\\n to_timestamp=datetime.now()\\\\n ).data\\\\n \\\\nprint(f\\\\\\\"Traces in first batch: {len(traces_batch)}\\\\\\\")\\\\n```\\\\nTraces in first batch: 10\\\\n## 2\\\\\\\\. Run your evaluations\\\\nLangfuse can handle numerical, boolean and categorical (`string`) scores. Wrapping your custom evaluation logic in a function is often a good practice. Evaluation functions should take a `trace` as input and yield a valid score. Let\\\\u2019s begin with a simple example using a categorical score.\\\\n### 2.1. Categoric Evaluations\\\\nWhen analyzing the outputs of your LLM applications, you may want to evaluate traits that are best defined qualitatively, such as sentiment, tonality or text complexity (Grade level).\\\\nWe\\\\u2019re building a science educator LLM that should sound engaging and positive. To ensure it hits the right notes, we\\\\u2019ll evaluate the tone of its outputs to see if they match our intent. We\\\\u2019ll draft an evaluation prompt ourselves (no library) to identify the three main tones in each model output.\\\\n    \\\\n    \\\\n```\\\\ntemplate_tone_eval = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nYou're an expert in human emotional intelligence. You can identify with ease the\\\\n tone in human-written text. Your task is to identify the tones present in a\\\\n piece of <text/> with precission. Your output is a comma separated list of three\\\\n tones. PRINT THE LIST ALONE, NOTHING ELSE.\\\\n \\\\n<possible_tones>\\\\nneutral, confident, joyful, optimistic, friendly, urgent, analytical, respectful\\\\n</possible_tones>\\\\n \\\\n<example_1>\\\\nInput: Citizen science plays a crucial role in research by involving everyday\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\npeople in scientific projects. This collaboration allows researchers to collect\\\\nvast amounts of data that would be impossible to gather on their own. Citizen\\\\nscientists contribute valuable observations and insights that can lead to new\\\\ndiscoveries and advancements in various fields. By participating in citizen\\\\nscience projects, individuals can actively contribute to scientific research\\\\nand make a meaningful impact on our understanding of the world around us.\\\\n \\\\nOutput: respectful,optimistic,confident\\\\n</example_1>\\\\n \\\\n<example_2>\\\\nInput: Bionics is a field that combines biology and engineering to create\\\\ndevices that can enhance human abilities. By merging humans and machines,\\\\nbionics aims to improve quality of life for individuals with disabilities\\\\nor enhance performance for others. These technologies often mimic natural\\\\nprocesses in the body to create seamless integration. Overall, bionics holds\\\\ngreat potential for revolutionizing healthcare and technology in the future.\\\\n \\\\nOutput: optimistic,confident,analytical\\\\n</example_2>\\\\n \\\\n<example_3>\\\\nInput: Social media can have both positive and negative impacts on mental\\\\nhealth. On the positive side, it can help people connect, share experiences,\\\\nand find support. However, excessive use of social media can also lead to\\\\nfeelings of inadequacy, loneliness, and anxiety. It's important to find a\\\\nbalance and be mindful of how social media affects your mental well-being.\\\\nRemember, it's okay to take breaks and prioritize your mental health.\\\\n \\\\nOutput: friendly,neutral,respectful\\\\n</example_3>\\\\n \\\\n<text>\\\\n{text}\\\\n</text>\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n \\\\n \\\\ntest_tone_score = openai.chat.completions.create(\\\\n messages=[\\\\n {\\\\n \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\",\\\\n \\\\\\\"content\\\\\\\": template_tone_eval.format(\\\\n text=traces_batch[1].output),\\\\n }\\\\n ],\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n model=\\\\\\\"gpt-4o\\\\\\\",\\\\n \\\\n temperature=0\\\\n).choices[0].message.content\\\\nprint(f\\\\\\\"User query: {traces_batch[1].input['args'][0]}\\\\\\\")\\\\nprint(f\\\\\\\"Model answer: {traces_batch[1].output}\\\\\\\")\\\\nprint(f\\\\\\\"Dominant tones: {test_tone_score}\\\\\\\")\\\\n```\\\\nIdentifying human intents and tones can be tricky for language models. To handle this, we used a multi-shot prompt, which means giving the model several examples to learn from. Now let\\\\u2019s wrap our code in an evaluation function for convenience.\\\\n    \\\\n    \\\\n```\\\\ndef tone_score(trace):\\\\n return openai.chat.completions.create(\\\\n messages=[\\\\n {\\\\n \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\",\\\\n \\\\\\\"content\\\\\\\": template_tone_eval.format(text=trace.output),\\\\n }\\\\n ],\\\\n model=\\\\\\\"gpt-4o\\\\\\\",\\\\n temperature=0\\\\n ).choices[0].message.content\\\\n \\\\ntone_score(traces_batch[1])\\\\n```\\\\nGreat! Now let\\\\u2019s go ahead and create a numeric evaluation score.\\\\n### 2.2. Numeric Evaluations\\\\nIn this cookbook, we\\\\u2019ll use the `Deepeval` framework ([docs](https://docs.confident-ai.com/docs/getting-started)) to handle our numeric evaluations. Deepeval provides scores ranging from zero to one for many common LLM metrics. Plus, you can create custom metrics by simply describing them in plain language. To ensure our app\\\\u2019s responses are joyful and engaging, we\\\\u2019ll define a custom \\\\u2018joyfulness\\\\u2019 score.\\\\nYou can use any evaluation library. These are popular ones:\\\\n  * OpenAI Evals ([GitHub](https://github.com/openai/evals))\\\\n  * Langchain Evaluators\\\\n  * [RAGAS](https://docs.ragas.io/en/latest/concepts/metrics/index.html) for RAG applications\\\\n    \\\\n    \\\\n```\\\\nfrom deepeval.metrics import GEval\\\\nfrom deepeval.test_case import LLMTestCaseParams, LLMTestCase\\\\n \\\\ndef joyfulness_score(trace):\\\\n joyfulness_metric = GEval(\\\\n name=\\\\\\\"Correctness\\\\\\\",\\\\n criteria=\\\\\\\"Determine whether the output is engaging and fun.\\\\\\\",\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\\\\n )\\\\n test_case = LLMTestCase(\\\\n input=trace.input[\\\\\\\"args\\\\\\\"],\\\\n actual_output=trace.output)\\\\n \\\\n joyfulness_metric.measure(test_case)\\\\n \\\\n print(f\\\\\\\"Score: {joyfulness_metric.score}\\\\\\\")\\\\n print(f\\\\\\\"Reason: {joyfulness_metric.reason}\\\\\\\")\\\\n \\\\n return {\\\\\\\"score\\\\\\\": joyfulness_metric.score, \\\\\\\"reason\\\\\\\": joyfulness_metric.reason}\\\\n \\\\njoyfulness_score(traces_batch[1])\\\\n```\\\\nUnder the hood, GEval uses chain of thought (CoT) prompting to formulate a set of criteria for scoring prompts. When developing your own metrics, it\\\\u2019s important to review the reasoning behind these scores. This helps ensure that the model evaluates the traces just as you intended when you wrote the evaluation prompt.\\\\nOur eval function returns a dictionary with both the score and the model\\\\u2019s reasoning. We do this as we\\\\u2019ll persist the reasoning with every langfuse score, ensuring interpretability.\\\\nNow we\\\\u2019re done with defining our evaluation functions. Let\\\\u2019s push those scores back to Langfuse!\\\\n## 3\\\\\\\\. Pushing Scores to Langfuse\\\\nNow that we have our evaluation functions ready, it\\\\u2019s time to put them to work. Use the Langfuse client to add scores to existing traces.\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_score(\\\\n trace_id=traces_batch[1].id,\\\\n name=\\\\\\\"tone\\\\\\\",\\\\n value=joyfulness_score(traces_batch[1])[\\\\\\\"score\\\\\\\"],\\\\n comment=joyfulness_score(traces_batch[1])[\\\\\\\"reason\\\\\\\"]\\\\n)\\\\n```\\\\nAnd thus, you\\\\u2019ve added your first externally-evaluated score to Langfuse! Just 49 more to go \\\\ud83d\\\\ude01. But don\\\\u2019t worry \\\\u2014 our solutions are easy to scale.\\\\n## 4\\\\\\\\. Putting everything together\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nUntil now, we went through each of the necessary steps to build an external evaluation pipeline: Fetching traces, running the evaluations, and persisting the scores to Langfuse. Let\\\\u2019s sum it up into a compact script that you could run in your evaluation pipeline.\\\\nWe\\\\u2019ll fetch the data in batches of 10 traces and then iterate through each trace to score it and push the scores back to Langfuse. Note that this batch size is for demonstration purposes. In a production setup, you might want to process multiple batches in parallel to speed things up. Batching not only reduces the memory load on your system but also allows you to create checkpoints, so you can easily resume if something goes wrong.\\\\n    \\\\n    \\\\n```\\\\nimport math\\\\n \\\\nfor page_number in range(1, math.ceil(TOTAL_TRACES/BATCH_SIZE)):\\\\n \\\\n traces_batch = langfuse.api.trace.list(\\\\n tags=\\\\\\\"ext_eval_pipelines\\\\\\\",\\\\n page=page_number,\\\\n from_timestamp=five_am_yesterday,\\\\n to_timestamp=five_am_today,\\\\n limit=BATCH_SIZE\\\\n ).data\\\\n \\\\n for trace in traces_batch:\\\\n print(f\\\\\\\"Processing {trace.name}\\\\\\\")\\\\n \\\\n if trace.output is None:\\\\n print(f\\\\\\\"Warning: \\\\\\\\n Trace {trace.name} had no generated output, \\\\\\\\\\\\n it was skipped\\\\\\\")\\\\n continue\\\\n \\\\n langfuse.create_score(\\\\n trace_id=trace.id,\\\\n name=\\\\\\\"tone\\\\\\\",\\\\n value=tone_score(trace)\\\\n )\\\\n \\\\n jscore = joyfulness_score(trace)\\\\n langfuse.create_score(\\\\n trace_id=trace.id,\\\\n name=\\\\\\\"joyfulness\\\\\\\",\\\\n value=jscore[\\\\\\\"score\\\\\\\"],\\\\n comment=jscore[\\\\\\\"reason\\\\\\\"]\\\\n )\\\\n \\\\n print(f\\\\\\\"Batch {page_number} processed \\\\ud83d\\\\ude80 \\\\\\\\n\\\\\\\")\\\\n```\\\\nIf your pipeline ran successfully, you should see your score in the Langfuse UI. \\\\nAnd that\\\\u2019s it! You\\\\u2019re now ready to integrate these lines into your preferred orchestration tool to ensure they run at the right times.\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nTo achieve our original goal of running the script every day at 5 am, simply schedule a Cron task in your chosen environment with the rule `cron(0 5 * * ? *)`.\\\\nThanks for coding along! I hope you enjoyed the tutorial and found it helpful.\\\\nLast updated on August 20, 2025\\\\n[Example - Tracing and Evaluation for the OpenAI-Agents SDK](/guides/cookbook/example_evaluating_openai_agents \\\\\\\"Example - Tracing and Evaluation for the OpenAI-Agents SDK\\\\\\\")\\\\n[Guide - Building an intent classification pipeline](/guides/cookbook/example_intent_classification_pipeline \\\\\\\"Guide - Building an intent classification pipeline\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"github_discussion\\\", \\\"url\\\": \\\"https://github.com/orgs/langfuse/discussions/7572\\\", \\\"title\\\": \\\"LLM-as-a-Judge Evaluators for Dataset doesn't work as expected.\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\n\\\\n---\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=hallucination) | [Report \\\\ud83d\\\\udc1b](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\n\\\\n        Reply #5 to comment #1 from dosubot on June 30, 2025:\\\\n\\\\n        I am using ui and variable mapping is as follows:\\\\r\\\\n\\\\r\\\\n{{search_query}} is mapped to Object: Dataset item, Object Variable: Input, JsonPath: <empty>\\\\r\\\\n{{summary}} is mapped to Object: Dataset item, Object Variable: Metadata, JsonPath: <empty>\\\\r\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{{expected}} is mapped to Object: Dataset item,  Object Variable: Expected Output, JsonPath: <empty>\\\\r\\\\n\\\\r\\\\nA single row format example from my dataset: \\\\r\\\\nInput column: \\\\\\\"Test input\\\\\\\", Expected Output column: \\\\\\\"Test expected output\\\\\\\", Metadata column: \\\\\\\"Test metadata\\\\\\\"\\\\n\\\\n        Reply #6 to comment #1 from dosubot on June 30, 2025:\\\\n\\\\n        <!-- Answer -->\\\\nYour mapping setup looks correct, but the backend eligibility logic is strict: if any mapped variable can't be resolved (for example, if the field is empty or the type doesn't match expectations), the item is silently skipped and you get \\\\\\\"0 elements\\\\\\\" processed. \\\\n\\\\nOne likely issue is the Metadata field\\\\u2014if it's a plain string (like \\\\\\\"Test metadata\\\\\\\"), but the system expects a JSON object (since many Langfuse examples use objects for metadata), this can cause the mapping to fail even if JsonPath is empty. When JsonPath is empty, the backend uses the top-level value as-is, but if the field is a string and the mapping expects an object (or vice versa), the item is excluded from evaluation\\\\u00a0[see code logic](https://github.com/langfuse/langfuse/blob/98cc1bb7a1759e29de3dd818f6a500df48fb5e82/worker/src/features/evaluation/evalService.ts).\\\\n\\\\nTry this: map all variables to fields you know are plain strings (like Input or Expected Output) and see if the evaluation processes any items. If it works, the issue is with the Metadata field's type. To confirm, create a dataset item where Metadata is a JSON object (e.g., {\\\\\\\"meta\\\\\\\": \\\\\\\"value\\\\\\\"}) and try mapping with and without a JsonPath. This will help isolate the problem.\\\\n\\\\nIf you still get 0 elements, export a sample of your dataset as JSON and share it\\\\u2014there may be a subtle data or mapping mismatch. \\\\n\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nMore details on dataset item structure and mapping: [Langfuse docs](https://langfuse.com/docs/datasets/get-started).\\\\n\\\\n\\\\n<!-- Dosu Comment Footer -->\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\n\\\\n---\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=hallucination) | [Report \\\\ud83d\\\\udc1b](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\n\\\\n        Reply #7 to comment #1 from dosubot on June 30, 2025:\\\\n\\\\n        I have created a new dataset with single row in json format, so columns Input, Expected Output and Metadata have following values respectively but still the same issue:\\\\r\\\\n{ \\\\\\\"search_query\\\\\\\": \\\\\\\"What's MLflow?\\\\\\\" }\\\\r\\\\n\\\\r\\\\n{ \\\\\\\"expected\\\\\\\": \\\\\\\"MLflow tracks ML experiments.\\\\\\\" } \\\\r\\\\n\\\\r\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2023-09-25-datasets\\\", \\\"title\\\": \\\"Datasets (beta)\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Datasets (beta) - Langfuse\\\\n[\\\\u2190 Back to changelog](/changelog#2023-09-25-datasets)\\\\nSeptember 25, 2023\\\\n# Datasets (beta)\\\\n[Marc Klingen](https://twitter.com/marcklingen)\\\\nCollect sets of inputs and expected outputs in Langfuse to evaluate your LLM app. Use evaluations to benchmark different experiments.\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\n_Run experiment on dataset_\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\n    \\\\n    \\\\n```\\\\n \\\\ndataset = langfuse.get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\nfor item in dataset.items:\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\n # output also returned as it is used to evaluate the run\\\\n generation, output = my_llm_application.run(item.input)\\\\n \\\\n # link the execution trace to the dataset item and give it a run_name\\\\n item.link(generation, \\\\\\\"<run_name>\\\\\\\")\\\\n \\\\n # optionally, evaluate the output to compare different runs more easily\\\\n generation.score(\\\\n name=\\\\\\\"<example_eval>\\\\\\\",\\\\n # any float value\\\\n value=my_eval_fn(\\\\n item.input,\\\\n output,\\\\n item.expected_output\\\\n )\\\\n )\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst dataset = await langfuse.getDataset(\\\\\\\"<dataset_name>\\\\\\\");\\\\n \\\\nfor (const item of dataset.items) {\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\n // output also returned as it is used to evaluate the run\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\n \\\\n // link the execution trace to the dataset item and give it a run_name\\\\n await item.link(generation, \\\\\\\"<run_name>\\\\\\\");\\\\n \\\\n // optionally, evaluate the output to compare different runs more easily\\\\n generation.score({\\\\n name: \\\\\\\"<score_name>\\\\\\\",\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\n });\\\\n}\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\", \\\"title\\\": \\\"Troubleshooting and FAQ\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\(\\\\\\\\)` method\\\\\\\")\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\")\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\"Langfuse Support: \\\\\\\\[Experiment\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\")\\\\n[Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing\\\\\\\")\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\")\\\\n[Results for some data items not present when comparing experiments](https://github.com/orgs/langfuse/discussions/5928 \\\\\\\"Langfuse Support: Results for some data items not present when comparing experiments\\\\\\\")\\\\n[Deleting Metrics for Langfuse](https://github.com/orgs/langfuse/discussions/5849 \\\\\\\"Langfuse Support: Deleting Metrics for Langfuse\\\\\\\")\\\\n[Discrepancies between dataset items found in the UI vs retrieved from the SDK/API](https://github.com/orgs/langfuse/discussions/5822 \\\\\\\"Langfuse Support: Discrepancies between dataset items found in the UI vs retrieved from the SDK/API\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Request batch adding of multiple traces to datasets](https://github.com/orgs/langfuse/discussions/8526 \\\\\\\"Langfuse Ideas: Request batch adding of multiple traces to datasets\\\\\\\")\\\\n[How to categorize topics and create a pie chart?](https://github.com/orgs/langfuse/discussions/8512 \\\\\\\"Langfuse Ideas: How to categorize topics and create a pie chart?\\\\\\\")\\\\n[Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window](https://github.com/orgs/langfuse/discussions/8485 \\\\\\\"Langfuse Ideas: Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window\\\\\\\")\\\\n[feat(LLM-as-a-judge): support stratified sampling by trace property](https://github.com/orgs/langfuse/discussions/8480 \\\\\\\"Langfuse Ideas: feat\\\\\\\\(LLM-as-a-judge\\\\\\\\): support stratified sampling by trace property\\\\\\\")\\\\n[Bulk Evaluation Does not Displays the status of the dataset (Like for E.g. processing, evaluated, executed)](https://github.com/orgs/langfuse/discussions/8410 \\\\\\\"Langfuse Ideas: Bulk Evaluation Does not Displays the status of the dataset \\\\\\\\(Like for E.g. processing, evaluated, executed\\\\\\\\)\\\\\\\")\\\\n[Annotation Queue Creation API](https://github.com/orgs/langfuse/discussions/8372 \\\\\\\"Langfuse Ideas: Annotation Queue Creation API\\\\\\\")\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\"Langfuse Ideas: \\\\\\\\[Langfuse Cloud\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\")\\\\n[feat(evals): allow canceling a running evaluator with pending evaluation jobs](https://github.com/orgs/langfuse/discussions/8310 \\\\\\\"Langfuse Ideas: feat\\\\\\\\(evals\\\\\\\\): allow canceling a running evaluator with pending evaluation jobs\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Support Description on Dataset Items](https://github.com/orgs/langfuse/discussions/6011 \\\\\\\"Langfuse Ideas: Support Description on Dataset Items\\\\\\\")\\\\n[Alta Integration](https://github.com/orgs/langfuse/discussions/5957 \\\\\\\"Langfuse Ideas: Alta Integration\\\\\\\")\\\\n[Add run name to columns when looking at a specific dataset item](https://github.com/orgs/langfuse/discussions/5929 \\\\\\\"Langfuse Ideas: Add run name to columns when looking at a specific dataset item\\\\\\\")\\\\n[Delete multiple dataset runs](https://github.com/orgs/langfuse/discussions/5893 \\\\\\\"Langfuse Ideas: Delete multiple dataset runs\\\\\\\")\\\\n[Navigation between items in a dataset run is confusing - context of the selected dataset run is lost](https://github.com/orgs/langfuse/discussions/5892 \\\\\\\"Langfuse Ideas: Navigation between items in a dataset run is confusing - context of the selected dataset run is lost\\\\\\\")\\\\n[Enhanced score distribution visualization in experiment analysis](https://github.com/orgs/langfuse/discussions/5819 \\\\\\\"Langfuse Ideas: Enhanced score distribution visualization in experiment analysis\\\\\\\")\\\\n[Multi-step Prompt Experiments and Playground](https://github.com/orgs/langfuse/discussions/5812 \\\\\\\"Langfuse Ideas: Multi-step Prompt Experiments and Playground\\\\\\\")\\\\n[Option to add trace to new dataset](https://github.com/orgs/langfuse/discussions/5756 \\\\\\\"Langfuse Ideas: Option to add trace to new dataset\\\\\\\")\\\\n[Simplified UI for Scoring](https://github.com/orgs/langfuse/discussions/5721 \\\\\\\"Langfuse Ideas: Simplified UI for Scoring\\\\\\\")\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Datasets: Add selection of traces to a dataset](https://github.com/orgs/langfuse/discussions/4353 \\\\\\\"Langfuse Ideas: Datasets: Add selection of traces to a dataset\\\\\\\")\\\\n[Multi-user annotation capability in Annotation Queues](https://github.com/orgs/langfuse/discussions/4348 \\\\\\\"Langfuse Ideas: Multi-user annotation capability in Annotation Queues\\\\\\\")\\\\n[Multi-turn / session experiments in datasets](https://github.com/orgs/langfuse/discussions/4208 \\\\\\\"Langfuse Ideas: Multi-turn / session experiments in datasets\\\\\\\")\\\\n[Enable to use variable of prompt on evaluator.](https://github.com/orgs/langfuse/discussions/4121 \\\\\\\"Langfuse Ideas: Enable to use variable of prompt on evaluator.\\\\\\\")\\\\n[Sessions Table: Scores Column](https://github.com/orgs/langfuse/discussions/4120 \\\\\\\"Langfuse Ideas: Sessions Table: Scores Column\\\\\\\")\\\\n[Add new filters for the LLM as a Judge Evaluation (other scores and cost)](https://github.com/orgs/langfuse/discussions/4106 \\\\\\\"Langfuse Ideas: Add new filters for the LLM as a Judge Evaluation \\\\\\\\(other scores and cost\\\\\\\\)\\\\\\\")\\\\n[Export dataset run table](https://github.com/orgs/langfuse/discussions/4077 \\\\\\\"Langfuse Ideas: Export dataset run table\\\\\\\")\\\\n[feat: support adding trace tags in annotation queue view](https://github.com/orgs/langfuse/discussions/4037 \\\\\\\"Langfuse Ideas: feat: support adding trace tags in annotation queue view\\\\\\\")\\\\n[Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times](https://github.com/orgs/langfuse/discussions/4035 \\\\\\\"Langfuse Ideas: Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times\\\\\\\")\\\\n[Diff support for dataset runs view](https://github.com/orgs/langfuse/discussions/4025 \\\\\\\"Langfuse Ideas: Diff support for dataset runs view\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Create Support for gemini models in playground](https://github.com/orgs/langfuse/discussions/4019 \\\\\\\"Langfuse Ideas: Create Support for gemini models in playground\\\\\\\")\\\\n[Change AWS access pattern for Bedrock LLM usage, assume role](https://github.com/orgs/langfuse/discussions/3988 \\\\\\\"Langfuse Ideas: Change AWS access pattern for Bedrock LLM usage, assume role\\\\\\\")\\\\n[Add ability to export and import evaluators between projects](https://github.com/orgs/langfuse/discussions/3970 \\\\\\\"Langfuse Ideas: Add ability to export and import evaluators between projects\\\\\\\")\\\\n[feat: Folder structure for dataset organisation](https://github.com/orgs/langfuse/discussions/3935 \\\\\\\"Langfuse Ideas: feat: Folder structure for dataset organisation\\\\\\\")\\\\n[Model-based evaluations triggered by observations](https://github.com/orgs/langfuse/discussions/3918 \\\\\\\"Langfuse Ideas: Model-based evaluations triggered by observations\\\\\\\")\\\\n[Scores: Conditional Annotation](https://github.com/orgs/langfuse/discussions/3842 \\\\\\\"Langfuse Ideas: Scores: Conditional Annotation\\\\\\\")\\\\n[Annotation Queues: define optional/mandatory score configs by queue](https://github.com/orgs/langfuse/discussions/3841 \\\\\\\"Langfuse Ideas: Annotation Queues: define optional/mandatory score configs by queue\\\\\\\")\\\\n[Scores: support for recording multiple choice selection as score value](https://github.com/orgs/langfuse/discussions/3840 \\\\\\\"Langfuse Ideas: Scores: support for recording multiple choice selection as score value\\\\\\\")\\\\n[Filter by status in dataset items table](https://github.com/orgs/langfuse/discussions/3818 \\\\\\\"Langfuse Ideas: Filter by status in dataset items table\\\\\\\")\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/overview\\\", \\\"title\\\": \\\"Evaluation Overview\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Evaluation of LLM Applications - Langfuse\\\\nDocsEvaluationOverview\\\\nCopy as Markdown\\\\n# Evaluation Overview\\\\nEvaluation is a critical aspect of developing and deploying LLM applications. Usually, teams use a multitude of different evaluation methods to score the performance of their AI application depending on the use case and the stage of the development process.\\\\n\\\\ud83c\\\\udfa5\\\\n[ **Watch this walkthrough**](/watch-demo?tab=evaluation) of Langfuse Evaluation and how to use it to improve your LLM application.\\\\n## Why use LLM Evaluation?\\\\nLLM evaluation is crucial for improving the accuracy and robustness of language models, ultimately enhancing the user experience and trust in your AI application. Here are the key benefits:\\\\n  * **Quality Assurance** : Detect hallucinations, factual inaccuracies, and inconsistent outputs to ensure your AI app delivers reliable results\\\\n  * **Performance Monitoring** : Measure response quality, relevance, and user satisfaction across different scenarios and edge cases\\\\n  * **Continuous Improvement** : Identify areas for enhancement and track improvements over time through structured evaluation metrics\\\\n  * **User Trust** : Build confidence in your AI application by demonstrating consistent, high-quality outputs through systematic evaluation\\\\n  * **Risk Mitigation** : Catch potential issues before they reach production users, reducing the likelihood of poor user experiences or reputational damage\\\\n## Online & Offline Evaluation\\\\n**Offline Evaluation involves**\\\\n  * Evaluating the application in a controlled setting\\\\n  * Typically using curated test Datasets instead of live user queries\\\\n  * Heavily used during development (can be part of CI/CD pipelines) to measure improvements / regressions\\\\n  * Repeatable and you can get clear accuracy metrics since you have ground truth.\\\\n**Online Evaluation involves**\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Python SDK: Pass ScoreConfig instead of config_id when creating scores](https://github.com/orgs/langfuse/discussions/8623 \\\\\\\"Langfuse Ideas: Python SDK: Pass ScoreConfig instead of config_id when creating scores\\\\\\\")\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\"Langfuse Ideas: \\\\\\\\[Langfuse Cloud\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\")\\\\n[Score Configs: Allow editing the categories of a categorical score](https://github.com/orgs/langfuse/discussions/8259 \\\\\\\"Langfuse Ideas: Score Configs: Allow editing the categories of a categorical score\\\\\\\")\\\\n[UI-LLM as a Jury](https://github.com/orgs/langfuse/discussions/8195 \\\\\\\"Langfuse Ideas: UI-LLM as a Jury\\\\\\\")\\\\n[Enable Immediate Score Management for User Feedback](https://github.com/orgs/langfuse/discussions/7686 \\\\\\\"Langfuse Ideas: Enable Immediate Score Management for User Feedback\\\\\\\")\\\\n[Filter by scores in session view](https://github.com/orgs/langfuse/discussions/7528 \\\\\\\"Langfuse Ideas: Filter by scores in session view\\\\\\\")\\\\n[Support break lines on evaluation run tooltip hint](https://github.com/orgs/langfuse/discussions/7452 \\\\\\\"Langfuse Ideas: Support break lines on evaluation run tooltip hint\\\\\\\")\\\\n[Support new lines when storing / displaying score comments](https://github.com/orgs/langfuse/discussions/6473 \\\\\\\"Langfuse Ideas: Support new lines when storing / displaying score comments\\\\\\\")\\\\n[Evaluator: Filter for Scores](https://github.com/orgs/langfuse/discussions/6236 \\\\\\\"Langfuse Ideas: Evaluator: Filter for Scores\\\\\\\")\\\\n[Code-based custom evaluators](https://github.com/orgs/langfuse/discussions/6087 \\\\\\\"Langfuse Ideas: Code-based custom evaluators\\\\\\\")\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Session-level scores](https://github.com/orgs/langfuse/discussions/2728 \\\\\\\"Langfuse Ideas: Session-level scores\\\\\\\")\\\\n[Scoring dataset runs, e.g. precision, recall, f-value](https://github.com/orgs/langfuse/discussions/2511 \\\\\\\"Langfuse Ideas: Scoring dataset runs, e.g. precision, recall, f-value\\\\\\\")\\\\n[Adding userId / author to score (custom metadata)](https://github.com/orgs/langfuse/discussions/2469 \\\\\\\"Langfuse Ideas: Adding userId / author to score \\\\\\\\(custom metadata\\\\\\\\)\\\\\\\")\\\\n[Add string data type in score config](https://github.com/orgs/langfuse/discussions/2402 \\\\\\\"Langfuse Ideas: Add string data type in score config\\\\\\\")\\\\n[API to delete scores](https://github.com/orgs/langfuse/discussions/1133 \\\\\\\"Langfuse Ideas: API to delete scores\\\\\\\")\\\\nGitHubSupportGitHubIdeas\\\\nUpvotes[GitHubNew](https://github.com/orgs/langfuse/discussions/new/choose)\\\\n  * 9votes\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486)\\\\nmsanand\\\\u20223/25/2024\\\\u2022\\\\n2Resolved\\\\n  * 4votes\\\\n[Filter Categorical Score Values](https://github.com/orgs/langfuse/discussions/3797)\\\\nalabrashJr\\\\u202210/17/2024\\\\u2022\\\\n3Resolved\\\\n  * 3votes\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590)\\\\nanuras\\\\u20228/18/2025\\\\u2022\\\\n1Resolved\\\\n  * 3votes\\\\n[Support for Metric Calculation (Precision@K, Recall@K) and Adding Custom Metrics Use Case Overview](https://github.com/orgs/langfuse/discussions/5215)\\\\nsrimantacse\\\\u20221/27/2025\\\\u2022\\\\n2Resolved\\\\n  * 2votes\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556)\\\\nflabbergastedbd\\\\u20228/15/2025\\\\u2022\\\\n1\\\\n  * 1votes\\\"}], \\\"type\\\": \\\"content\\\"}}]}\",\"role\":\"assistant\",\"tool_calls\":null,\"function_call\":null}}],\"usage\":{\"prompt_tokens\":23,\"completion_tokens\":20274,\"total_tokens\":20297},\"system_fingerprint\":\"inkeep-fp-9a382b06-43e9-4c7a-b61b-7b35190c514a\",\"object\":\"chat.completion\",\"service_tier\":null,\"created\":1758264684},\"content\":[{\"type\":\"text\",\"text\":\"{\\\"content\\\": [{\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\", \\\"title\\\": \\\"Troubleshooting and FAQ\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[API - /scores not respecting value when operator '='](https://github.com/orgs/langfuse/discussions/8770 \\\\\\\"Langfuse Support: API - /scores not respecting value when operator '='\\\\\\\")\\\\n[Does LangFuse support evaluations on an existing dataset (.csv)](https://github.com/orgs/langfuse/discussions/8665 \\\\\\\"Langfuse Support: Does LangFuse support evaluations on an existing dataset \\\\\\\\(.csv\\\\\\\\)\\\\\\\")\\\\n[deleting evaluators](https://github.com/orgs/langfuse/discussions/8640 \\\\\\\"Langfuse Support: deleting evaluators\\\\\\\")\\\\n[Not able to import my LLM-as-a-Judge evals](https://github.com/orgs/langfuse/discussions/8636 \\\\\\\"Langfuse Support: Not able to import my LLM-as-a-Judge evals\\\\\\\")\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590 \\\\\\\"Langfuse Support: How to get experiment run scores programmatically?\\\\\\\")\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556 \\\\\\\"Langfuse Support: Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations\\\\\\\")\\\\n[Dataset runs restore from backups](https://github.com/orgs/langfuse/discussions/8534 \\\\\\\"Langfuse Support: Dataset runs restore from backups\\\\\\\")\\\\n[Getting scores efficiently via API for analytics purposes](https://github.com/orgs/langfuse/discussions/8520 \\\\\\\"Langfuse Support: Getting scores efficiently via API for analytics purposes\\\\\\\")\\\\n[run experiment on dataset](https://github.com/orgs/langfuse/discussions/8433 \\\\\\\"Langfuse Support: run experiment on dataset\\\\\\\")\\\\n[Experiments on Datasets with Human Annotated Labels?](https://github.com/orgs/langfuse/discussions/8414 \\\\\\\"Langfuse Support: Experiments on Datasets with Human Annotated Labels?\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[How to Recalculate Total Score on Dashboard After Updating User-Defined Model?](https://github.com/orgs/langfuse/discussions/8375 \\\\\\\"Langfuse Support: How to Recalculate Total Score on Dashboard After Updating User-Defined Model?\\\\\\\")\\\\n[How to filter by Categorical Scores in custom dashboard?](https://github.com/orgs/langfuse/discussions/8356 \\\\\\\"Langfuse Support: How to filter by Categorical Scores in custom dashboard?\\\\\\\")\\\\n[Running scheduled evals utilising LangFuse Datasets & Evaluators](https://github.com/orgs/langfuse/discussions/8355 \\\\\\\"Langfuse Support: Running scheduled evals utilising LangFuse Datasets & Evaluators\\\\\\\")\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\(\\\\\\\\)` method\\\\\\\")\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\")\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\"Langfuse Support: \\\\\\\\[Experiment\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\")\\\\n[Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing\\\\\\\")\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[2 traces generated instead of 1](https://github.com/orgs/langfuse/discussions/2244 \\\\\\\"Langfuse Support: 2 traces generated instead of 1\\\\\\\")\\\\n[Evaluations Not Available in Self-Hosted Version?](https://github.com/orgs/langfuse/discussions/2130 \\\\\\\"Langfuse Support: Evaluations Not Available in Self-Hosted Version?\\\\\\\")\\\\n[Deleting Duplicate Items in a Dataset](https://github.com/orgs/langfuse/discussions/2099 \\\\\\\"Langfuse Support: Deleting Duplicate Items in a Dataset\\\\\\\")\\\\n[Availability of evals when self-hosting](https://github.com/orgs/langfuse/discussions/2042 \\\\\\\"Langfuse Support: Availability of evals when self-hosting\\\\\\\")\\\\n[How to utilize a dataset w/ typescript and langchain integration](https://github.com/orgs/langfuse/discussions/1969 \\\\\\\"Langfuse Support: How to utilize a dataset w/ typescript and langchain integration\\\\\\\")\\\\n[Scoring a trace after the LLM chain returns](https://github.com/orgs/langfuse/discussions/1610 \\\\\\\"Langfuse Support: Scoring a trace after the LLM chain returns\\\\\\\")\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486 \\\\\\\"Langfuse Support: Update/delete score using python sdk\\\\\\\")\\\\n[Linking dataset run items with existing callback handler](https://github.com/orgs/langfuse/discussions/1445 \\\\\\\"Langfuse Support: Linking dataset run items with existing callback handler\\\\\\\")\\\\n[Datasets list / by id](https://github.com/orgs/langfuse/discussions/1420 \\\\\\\"Langfuse Support: Datasets list / by id\\\\\\\")\\\\n[Run items not appearing when linking to a trace and not a span or a generation](https://github.com/orgs/langfuse/discussions/1357 \\\\\\\"Langfuse Support: Run items not appearing when linking to a trace and not a span or a generation\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Option to run Experiments without traces](https://github.com/orgs/langfuse/discussions/8133 \\\\\\\"Langfuse Ideas: Option to run Experiments without traces\\\\\\\")\\\\n[Auto-generate dataset items](https://github.com/orgs/langfuse/discussions/8126 \\\\\\\"Langfuse Ideas: Auto-generate dataset items\\\\\\\")\\\\n[feature: support for creation of custom) model adapters](https://github.com/orgs/langfuse/discussions/8123 \\\\\\\"Langfuse Ideas: feature: support for creation of custom\\\\\\\\) model adapters\\\\\\\")\\\\n[Prompt experiment result download.](https://github.com/orgs/langfuse/discussions/8120 \\\\\\\"Langfuse Ideas: Prompt experiment result download.\\\\\\\")\\\\n[Multiple predictions per-item in a single run](https://github.com/orgs/langfuse/discussions/8040 \\\\\\\"Langfuse Ideas: Multiple predictions per-item in a single run\\\\\\\")\\\\n[Add evaluator(s) to python SDK (FernLangfuse)](https://github.com/orgs/langfuse/discussions/8018 \\\\\\\"Langfuse Ideas: Add evaluator\\\\\\\\(s\\\\\\\\) to python SDK \\\\\\\\(FernLangfuse\\\\\\\\)\\\\\\\")\\\\n[feat(dataset-runs): return all dataset run item scores in a given run](https://github.com/orgs/langfuse/discussions/8011 \\\\\\\"Langfuse Ideas: feat\\\\\\\\(dataset-runs\\\\\\\\): return all dataset run item scores in a given run\\\\\\\")\\\\n[Delete an evaluator from the evaluator library](https://github.com/orgs/langfuse/discussions/7960 \\\\\\\"Langfuse Ideas: Delete an evaluator from the evaluator library\\\\\\\")\\\\n[Archive or Delete Evaluators from the Evaluator Library](https://github.com/orgs/langfuse/discussions/7868 \\\\\\\"Langfuse Ideas: Archive or Delete Evaluators from the Evaluator Library\\\\\\\")\\\\n[Edit dataset run name, description](https://github.com/orgs/langfuse/discussions/7814 \\\\\\\"Langfuse Ideas: Edit dataset run name, description\\\\\\\")\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\", \\\"title\\\": \\\"Datasets\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Datasets - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")Dataset RunsDatasets\\\\nCopy as Markdown\\\\n# Datasets\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\n## Why use datasets?\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\n  * Create test cases for your application with real production traces\\\\n  * Collaboratively create and collect dataset items with your team\\\\n  * Have a single source of truth for your test data\\\\n## Get Started\\\\n### Creating a dataset\\\\nDatasets have a name which is unique within a project.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset(\\\\n name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # optional description\\\\n description=\\\\\\\"My first dataset\\\\\\\",\\\\n # optional metadata\\\\n metadata={\\\\n \\\\\\\"author\\\\\\\": \\\\\\\"Alice\\\\\\\",\\\\n \\\\\\\"date\\\\\\\": \\\\\\\"2022-01-01\\\\\\\",\\\\n \\\\\\\"type\\\\\\\": \\\\\\\"benchmark\\\\\\\"\\\\n }\\\\n)\\\\n```\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\n    \\\\n    \\\\n```\\\\nimport { LangfuseClient } from \\\\\\\"@langfuse/client\\\\\\\"\\\\n \\\\nconst langfuse = new LangfuseClient()\\\\n \\\\nawait langfuse.api.datasets.create({\\\\n name: \\\\\\\"<dataset_name>\\\\\\\",\\\\n // optional description\\\\n description: \\\\\\\"My first dataset\\\\\\\",\\\\n // optional metadata\\\\n metadata: {\\\\n author: \\\\\\\"Alice\\\\\\\",\\\\n date: \\\\\\\"2022-01-01\\\\\\\",\\\\n type: \\\\\\\"benchmark\\\\\\\",\\\\n },\\\\n});\\\\n```\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\n### Create new dataset items\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset_item(\\\\n dataset_name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # any python object or value, optional\\\\n input={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # any python object or value, optional\\\\n expected_output={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # metadata, optional\\\\n metadata={\\\\n \\\\\\\"model\\\\\\\": \\\\\\\"llama3\\\\\\\",\\\\n }\\\\n)\\\\n```\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/observability/sdk/python/evaluation\\\", \\\"title\\\": \\\"Evaluations\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Evaluations with the Langfuse Python SDK - Langfuse\\\\nDocs[Observability](/docs/observability/overview \\\\\\\"Observability\\\\\\\")\\\\n[SDKs](/docs/observability/sdk/overview \\\\\\\"SDKs\\\\\\\")\\\\n[Python](/docs/observability/sdk/python/overview \\\\\\\"Python\\\\\\\")Evaluation\\\\nCopy as Markdown\\\\n# Evaluations\\\\nThe Python SDK provides ways to evaluate your application. You can add custom scores to your traces and observations, or use the SDK to execute Dataset Runs.\\\\nThis page shows the evaluation methods that are supported by the Python SDK. Please refer to the [Evaluation documentation](/docs/evaluation/overview) for more information on how to evaluate your application in Langfuse.\\\\n## Create Scores\\\\nLangfuseSpan / LangfuseGeneration object methodsContext-aware methodsLow-level method\\\\n  * `span_or_generation_obj.score()`: Scores the specific observation object.\\\\n  * `span_or_generation_obj.score_trace()`: Scores the entire trace to which the object belongs.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nwith langfuse.start_as_current_generation(name=\\\\\\\"summary_generation\\\\\\\") as gen:\\\\n # ... LLM call ...\\\\n gen.update(output=\\\\\\\"summary text...\\\\\\\")\\\\n # Score this specific generation\\\\n gen.score(name=\\\\\\\"conciseness\\\\\\\", value=0.8, data_type=\\\\\\\"NUMERIC\\\\\\\")\\\\n # Score the overall trace\\\\n gen.score_trace(name=\\\\\\\"user_feedback_rating\\\\\\\", value=\\\\\\\"positive\\\\\\\", data_type=\\\\\\\"CATEGORICAL\\\\\\\")\\\\n```\\\\n  * `langfuse.score_current_span()`: Scores the currently active observation in the context.\\\\n  * `langfuse.score_current_trace()`: Scores the trace of the currently active observation.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nwith langfuse.start_as_current_span(name=\\\\\\\"complex_task\\\\\\\") as task_span:\\\\n # ... perform task ...\\\\n langfuse.score_current_span(name=\\\\\\\"task_component_quality\\\\\\\", value=True, data_type=\\\\\\\"BOOLEAN\\\\\\\")\\\\n # ...\\\\n if task_is_fully_successful:\\\\n langfuse.score_current_trace(name=\\\\\\\"overall_success\\\\\\\", value=1.0, data_type=\\\\\\\"NUMERIC\\\\\\\")\\\\n```\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n  * Creates a score for a specified `trace_id` and optionally `observation_id`.\\\\n  * Useful when IDs are known, or for scoring after the trace/observation has completed.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nlangfuse.create_score(\\\\n name=\\\\\\\"fact_check_accuracy\\\\\\\",\\\\n value=0.95, ///# Can be float for NUMERIC/BOOLEAN, string for CATEGORICAL\\\\n trace_id=\\\\\\\"abcdef1234567890abcdef1234567890\\\\\\\",\\\\n observation_id=\\\\\\\"1234567890abcdef\\\\\\\", ///# Optional: if scoring a specific observation\\\\n session_id=\\\\\\\"session_123\\\\\\\", ///# Optional: if scoring a specific session\\\\n data_type=\\\\\\\"NUMERIC\\\\\\\", ///# \\\\\\\"NUMERIC\\\\\\\", \\\\\\\"BOOLEAN\\\\\\\", \\\\\\\"CATEGORICAL\\\\\\\"\\\\n comment=\\\\\\\"Source verified for 95% of claims.\\\\\\\"\\\\n)\\\\n```\\\\n**Score Parameters:**\\\\nParameter| Type| Description  \\\\n---|---|---  \\\\n`name`| `str`| Name of the score (e.g., \\\\u201crelevance\\\\u201d, \\\\u201caccuracy\\\\u201d). **Required.**  \\\\n`value`| `Union[float, str]`| Score value. Float for `NUMERIC`/`BOOLEAN`, string for `CATEGORICAL`. **Required.**  \\\\n`trace_id`| `str`| ID of the trace to associate with (for `create_score`). **Required.**  \\\\n`observation_id`| `Optional[str]`| ID of the specific observation to score (for `create_score`).  \\\\n`session_id`| `Optional[str]`| ID of the specific session to score (for `create_score`).  \\\\n`score_id`| `Optional[str]`| Custom ID for the score (auto-generated if None).  \\\\n`data_type`| `Optional[ScoreDataType]`| `\\\\\\\"NUMERIC\\\\\\\"`, `\\\\\\\"BOOLEAN\\\\\\\"`, or `\\\\\\\"CATEGORICAL\\\\\\\"`. Inferred if not provided based on value type and score config on server.  \\\\n`comment`| `Optional[str]`| Optional comment or explanation for the score.  \\\\n`config_id`| `Optional[str]`| Optional ID of a pre-defined score configuration in Langfuse.  \\\\n  \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThe most powerful way to use datasets is by linking your application\\\\u2019s executions (traces) to specific dataset items when performing an evaluation run. The `DatasetItemClient.run()` method provides a context manager to streamline this process.\\\\n**How`item.run()` works:**\\\\nWhen you use `with item.run(run_name=\\\\\\\"your_eval_run_name\\\\\\\") as root_span:`:\\\\n  1. **Trace Creation** : A new Langfuse trace is initiated specifically for processing this dataset item within the context of the named run.\\\\n  2. **Trace Naming & Metadata**: \\\\n    * The trace is automatically named (e.g., \\\\u201cDataset run: your_eval_run_name\\\\u201d).\\\\n    * Essential metadata is added to this trace, including `dataset_item_id` (the ID of `item`), `run_name`, and `dataset_id`.\\\\n  3. **DatasetRunItem Linking** : The SDK makes an API call to Langfuse to create a `DatasetRunItem`. This backend object formally links: \\\\n    * The `dataset_item_id`\\\\n    * The `trace_id` of the newly created trace\\\\n    * The provided `run_name`\\\\n    * Any `run_metadata` or `run_description` you pass to `item.run()`. This linkage is what populates the \\\\u201cRuns\\\\u201d tab for your dataset in the Langfuse UI, allowing you to see all traces associated with a particular evaluation run.\\\\n  4. **Contextual Span** : The context manager yields `root_span`, which is a `LangfuseSpan` object representing the root span of this new trace.\\\\n  5. **Automatic Nesting** : Any Langfuse observations (spans or generations) created _inside_ the `with` block will automatically become children of `root_span` and thus part of the trace linked to this dataset item and run.\\\\n**Example:**\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\ndataset_name = \\\\\\\"qna-eval\\\\\\\"\\\\ncurrent_run_name = \\\\\\\"qna_model_v3_run_05_20\\\\\\\" # Identifies this specific evaluation run\\\\n \\\\n///# Assume 'my_qna_app' is your instrumented application function\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"def my_qna_app(question: str, context: str, item_id: str, run_name: str):\\\\n with langfuse.start_as_current_generation(\\\\n name=\\\\\\\"qna-llm-call\\\\\\\",\\\\n input={\\\\\\\"question\\\\\\\": question, \\\\\\\"context\\\\\\\": context},\\\\n metadata={\\\\\\\"item_id\\\\\\\": item_id, \\\\\\\"run\\\\\\\": run_name}, ///# Example metadata for the generation\\\\n model=\\\\\\\"gpt-4o\\\\\\\"\\\\n ) as generation:\\\\n # Simulate LLM call\\\\n answer = f\\\\\\\"Answer to '{question}' using context.\\\\\\\" # Replace with actual LLM call\\\\n generation.update(output={\\\\\\\"answer\\\\\\\": answer})\\\\n \\\\n # Update the trace with the input and output\\\\n generation.update_trace(\\\\n input={\\\\\\\"question\\\\\\\": question, \\\\\\\"context\\\\\\\": context},\\\\n output={\\\\\\\"answer\\\\\\\": answer},\\\\n )\\\\n \\\\n return answer\\\\n \\\\ndataset = langfuse.get_dataset(name=dataset_name) ///# Fetch your pre-populated dataset\\\\n \\\\nfor item in dataset.items:\\\\n print(f\\\\\\\"Running evaluation for item: {item.id} (Input: {item.input})\\\\\\\")\\\\n \\\\n # Use the item.run() context manager\\\\n with item.run(\\\\n run_name=current_run_name,\\\\n run_metadata={\\\\\\\"model_provider\\\\\\\": \\\\\\\"OpenAI\\\\\\\", \\\\\\\"temperature_setting\\\\\\\": 0.7},\\\\n run_description=\\\\\\\"Evaluation run for Q&A model v3 on May 20th\\\\\\\"\\\\n ) as root_span: ///# root_span is the root span of the new trace for this item and run.\\\\n # All subsequent langfuse operations within this block are part of this trace.\\\\n \\\\n # Call your application logic\\\\n generated_answer = my_qna_app(\\\\n question=item.input[\\\\\\\"question\\\\\\\"],\\\\n context=item.input[\\\\\\\"context\\\\\\\"],\\\\n item_id=item.id,\\\\n run_name=current_run_name\\\\n )\\\\n \\\\n print(f\\\\\\\" Item {item.id} processed. Trace ID: {root_span.trace_id}\\\\\\\")\\\\n \\\\n # Optionally, score the result against the expected output\\\\n if item.expected_output and generated_answer == item.expected_output.get(\\\\\\\"answer\\\\\\\"):\\\\n root_span.score_trace(name=\\\\\\\"exact_match\\\\\\\", value=1.0)\\\\n else:\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n root_span.score_trace(name=\\\\\\\"exact_match\\\\\\\", value=0.0)\\\\n \\\\nprint(f\\\\\\\"\\\\\\\\nFinished processing dataset '{dataset_name}' for run '{current_run_name}'.\\\\\\\")\\\\n```\\\\nBy using `item.run()`, you ensure each dataset item\\\\u2019s processing is neatly encapsulated in its own trace, and these traces are aggregated under the specified `run_name` in the Langfuse UI. This allows for systematic review of results, comparison across runs, and deep dives into individual processing traces.\\\\nLast updated on August 20, 2025\\\\n[Instrumentation](/docs/observability/sdk/python/instrumentation \\\\\\\"Instrumentation\\\\\\\")\\\\n[Advanced usage](/docs/observability/sdk/python/advanced-usage \\\\\\\"Advanced usage\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\", \\\"title\\\": \\\"All new Datasets, Experimentation and Evaluation documentation\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"## New Datasets and Evaluation documentation\\\\nBased on this week\\\\u2019s changes, we have have restructured the documentation to:\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\n### Evaluation\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Evaluation documentation](/docs/scores)\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\n    \\\\n    \\\\n```\\\\nPOST /api/public/scores\\\\n```\\\\nAdd scores via Python or JS SDK.\\\\nExample (Python)\\\\n    \\\\n    \\\\n```\\\\nlangfuse.score(\\\\n trace_id=\\\\\\\"123\\\\\\\",\\\\n name=\\\\\\\"my_custom_evaluator\\\\\\\",\\\\n value=0.5,\\\\n)\\\\n```\\\\n### Datasets\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\", \\\"title\\\": \\\"Remote Dataset Runs\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Task function that answers geography questions\\\\\\\"\\\\\\\"\\\\\\\"\\\\n question = item[\\\\\\\"input\\\\\\\"]\\\\n response = OpenAI().chat.completions.create(\\\\n model=\\\\\\\"gpt-4\\\\\\\",\\\\n messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": question}]\\\\n )\\\\n return response.choices[0].message.content\\\\n \\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\"\\\\\\\"\\\\\\\"\\\\n if expected_output and expected_output.lower() in output.lower():\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=1.0)\\\\n \\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=0.0)\\\\n \\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\"\\\\\\\"\\\\\\\"\\\\n accuracies = [\\\\n eval.value for result in item_results\\\\n for eval in result.evaluations if eval.name == \\\\\\\"accuracy\\\\\\\"\\\\n ]\\\\n \\\\n if not accuracies:\\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=None)\\\\n \\\\n avg = sum(accuracies) / len(accuracies)\\\\n \\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=avg, comment=f\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\")\\\\n \\\\n@pytest.fixture\\\\ndef langfuse_client():\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Initialize Langfuse client for testing\\\\\\\"\\\\\\\"\\\\\\\"\\\\n return get_client()\\\\n \\\\ndef test_geography_accuracy_passes(langfuse_client):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Test that passes when accuracy is above threshold\\\\\\\"\\\\\\\"\\\\\\\"\\\\n result = langfuse_client.run_experiment(\\\\n name=\\\\\\\"Geography Test - Should Pass\\\\\\\",\\\\n data=test_data,\\\\n task=geography_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy_evaluator]\\\\n )\\\\n \\\\n # Access the run evaluator result directly\\\\n avg_accuracy = next(\\\\n eval.value for eval in result.run_evaluations\\\\n if eval.name == \\\\\\\"avg_accuracy\\\\\\\"\\\\n )\\\\n \\\\n # Assert minimum accuracy threshold\\\\n assert avg_accuracy >= 0.8, f\\\\\\\"Average accuracy {avg_accuracy:.2f} below threshold 0.8\\\\\\\"\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\ndef test_geography_accuracy_fails(langfuse_client):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Example test that demonstrates failure conditions\\\\\\\"\\\\\\\"\\\\\\\"\\\\n # Use a weaker model or harder questions to demonstrate test failure\\\\n def failing_task(*, item, **kwargs):\\\\n # Simulate a task that gives wrong answers\\\\n return \\\\\\\"I don't know\\\\\\\"\\\\n \\\\n result = langfuse_client.run_experiment(\\\\n name=\\\\\\\"Geography Test - Should Fail\\\\\\\",\\\\n data=test_data,\\\\n task=failing_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy_evaluator]\\\\n )\\\\n \\\\n # Access the run evaluator result directly\\\\n avg_accuracy = next(\\\\n eval.value for eval in result.run_evaluations\\\\n if eval.name == \\\\\\\"avg_accuracy\\\\\\\"\\\\n )\\\\n \\\\n # This test will fail because the task gives wrong answers\\\\n with pytest.raises(AssertionError):\\\\n assert avg_accuracy >= 0.8, f\\\\\\\"Expected test to fail with low accuracy: {avg_accuracy:.2f}\\\\\\\"\\\\n```\\\\n    \\\\n    \\\\n```\\\\n// test/geography-experiment.test.ts\\\\nimport { describe, it, expect, beforeAll, afterAll } from 'vitest';\\\\nimport { OpenAI } from \\\\\\\"openai\\\\\\\";\\\\nimport { NodeSDK } from \\\\\\\"@opentelemetry/sdk-node\\\\\\\";\\\\nimport { LangfuseClient, ExperimentItem } from \\\\\\\"@langfuse/client\\\\\\\";\\\\nimport { observeOpenAI } from \\\\\\\"@langfuse/openai\\\\\\\";\\\\nimport { LangfuseSpanProcessor } from \\\\\\\"@langfuse/otel\\\\\\\";\\\\n \\\\n// Test data for European capitals\\\\nconst testData: ExperimentItem[] = [\\\\n { input: \\\\\\\"What is the capital of France?\\\\\\\", expected: \\\\\\\"Paris\\\\\\\" },\\\\n { input: \\\\\\\"What is the capital of Germany?\\\\\\\", expected: \\\\\\\"Berlin\\\\\\\" },\\\\n { input: \\\\\\\"What is the capital of Spain?\\\\\\\", expected: \\\\\\\"Madrid\\\\\\\" },\\\\n];\\\\n \\\\nlet otelSdk: NodeSDK;\\\\nlet langfuse: LangfuseClient;\\\\n \\\\nbeforeAll(async () => {\\\\n // Initialize OpenTelemetry\\\\n otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\n otelSdk.start();\\\\n \\\\n // Initialize Langfuse client\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\n    \\\\n    \\\\n```\\\\nimport { Factuality, Levenshtein } from \\\\\\\"autoevals\\\\\\\";\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\"@langfuse/client\\\\\\\";\\\\n \\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\n \\\\n// Use with additional parameters\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\n Factuality,\\\\n { model: \\\\\\\"gpt-4o\\\\\\\" } // Additional AutoEvals parameters\\\\n);\\\\n \\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"AutoEvals Integration Test\\\\\\\",\\\\n data: testDataset,\\\\n task: myTask,\\\\n evaluators: [\\\\n factualityEvaluator,\\\\n levenshteinEvaluator,\\\\n customFactualityEvaluator\\\\n ]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\", \\\"title\\\": \\\"Remote Dataset Runs\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Remote Dataset Runs via the SDK - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\"Dataset Runs\\\\\\\")Remote Run\\\\nCopy as Markdown\\\\n# Remote Dataset Runs\\\\nOnce you [created a dataset](/docs/evaluation/dataset-runs/datasets), you can use the dataset to test how your application performs on different inputs. Remote Dataset Runs are used to programmatically loop your applications or prompts through a dataset and optionally apply Evaluation Methods to the results.\\\\nThey are called \\\\u201cRemote Dataset Runs\\\\u201d because they can make use of \\\\u201cremote\\\\u201d or external logic and code.\\\\nOptionally, you can also trigger Remote Dataset Runs via the Langfuse UI which will call them via a webhook.\\\\n## Why use Remote Dataset Runs?\\\\n  * Full flexibility to use your own application logic\\\\n  * Use custom scoring functions to evaluate the outputs\\\\n  * Run multiple experiments on the same dataset in parallel\\\\n  * Easy to integrate with your existing evaluation infrastructure\\\\nSequence Diagram\\\\n## Experiment runner SDK\\\\nBoth the Python and JS/TS SDKs provide a high-level abstraction for running an experiment on a dataset. The dataset can be both local or hosted on Langfuse. Using the Experiment runner is the recommended way to run an experiment on a dataset with our SDK.\\\\nThe experiment runner automatically handles:\\\\n  * **Concurrent execution** of tasks with configurable limits\\\\n  * **Automatic tracing** of all executions for observability\\\\n  * **Flexible evaluation** with both item-level and run-level evaluators\\\\n  * **Error isolation** so individual failures don\\\\u2019t stop the experiment\\\\n  * **Dataset integration** for easy comparison and tracking\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThe experiment runner SDK supports both datasets hosted on Langfuse and datasets hosted locally. If you are using a dataset hosted on Langfuse for your experiment, the SDK will automatically create a dataset run for you that you can inspect and compare in the Langfuse UI. For locally hosted datasets not on Langfuse, only traces and scores (if evaluations are used) are tracked in Langfuse.\\\\n### Basic Usage\\\\nStart with the simplest possible experiment to test your task function on local data. If you already have a dataset in Langfuse, see here.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\nfrom langfuse.openai import OpenAI\\\\n \\\\n///# Initialize client\\\\nlangfuse = get_client()\\\\n \\\\n///# Define your task function\\\\ndef my_task(*, item, **kwargs):\\\\n question = item[\\\\\\\"input\\\\\\\"]\\\\n response = OpenAI().chat.completions.create(\\\\n model=\\\\\\\"gpt-4.1\\\\\\\", messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": question}]\\\\n )\\\\n \\\\n return response.choices[0].message.content\\\\n \\\\n \\\\n///# Run experiment on local data\\\\nlocal_data = [\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of France?\\\\\\\"},\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of Germany?\\\\\\\"},\\\\n]\\\\n \\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Geography Quiz\\\\\\\",\\\\n description=\\\\\\\"Testing basic functionality\\\\\\\",\\\\n data=local_data,\\\\n task=my_task,\\\\n)\\\\n \\\\n///# Use format method to display results\\\\nprint(result.format())\\\\n```\\\\nMake sure that OpenTelemetry is properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\n    \\\\n    \\\\n```\\\\nimport { OpenAI } from \\\\\\\"openai\\\\\\\";\\\\nimport { NodeSDK } from \\\\\\\"@opentelemetry/sdk-node\\\\\\\";\\\\n \\\\nimport {\\\\n LangfuseClient,\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n ExperimentTask,\\\\n ExperimentItem,\\\\n} from \\\\\\\"@langfuse/client\\\\\\\";\\\\nimport { observeOpenAI } from \\\\\\\"@langfuse/openai\\\\\\\";\\\\nimport { LangfuseSpanProcessor } from \\\\\\\"@langfuse/otel\\\\\\\";\\\\n \\\\n// Initialize OpenTelemetry\\\\nconst otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\notelSdk.start();\\\\n \\\\n// Initialize client\\\\nconst langfuse = new LangfuseClient();\\\\n \\\\n// Run experiment on local data\\\\nconst localData: ExperimentItem[] = [\\\\n { input: \\\\\\\"What is the capital of France?\\\\\\\" },\\\\n { input: \\\\\\\"What is the capital of Germany?\\\\\\\" },\\\\n];\\\\n \\\\n// Define your task function\\\\nconst myTask: ExperimentTask = async (item) => {\\\\n const question = item.input;\\\\n \\\\n const response = await observeOpenAI(new OpenAI()).chat.completions.create({\\\\n model: \\\\\\\"gpt-4.1\\\\\\\",\\\\n messages: [\\\\n {\\\\n role: \\\\\\\"user\\\\\\\",\\\\n content: question,\\\\n },\\\\n ],\\\\n });\\\\n \\\\n return response;\\\\n};\\\\n \\\\n// Run the experiment\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Geography Quiz\\\\\\\",\\\\n description: \\\\\\\"Testing basic functionality\\\\\\\",\\\\n data: localData,\\\\n task: myTask,\\\\n});\\\\n \\\\n// Print formatted result\\\\nconsole.log(await result.format());\\\\n \\\\n// Important: shut down OTEL SDK to deliver traces\\\\nawait otelSdk.shutdown();\\\\n```\\\\n **Note for JS/TS SDK** : OpenTelemetry must be properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\nWhen running experiments on local data, only traces are created in Langfuse - no dataset runs are generated. Each task execution creates an individual trace for observability and debugging.\\\\n### Usage with Langfuse Datasets\\\\nRun experiments directly on datasets stored in Langfuse for automatic tracing and comparison.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\n///# Get dataset from Langfuse\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\ndataset = langfuse.get_dataset(\\\\\\\"my-evaluation-dataset\\\\\\\")\\\\n \\\\n///# Run experiment directly on the dataset\\\\nresult = dataset.run_experiment(\\\\n name=\\\\\\\"Production Model Test\\\\\\\",\\\\n description=\\\\\\\"Monthly evaluation of our production model\\\\\\\",\\\\n task=my_task\\\\n)\\\\n \\\\n///# Use format method to display results\\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\n// Get dataset from Langfuse\\\\nconst dataset = await langfuse.dataset.get(\\\\\\\"my-evaluation-dataset\\\\\\\");\\\\n \\\\n// Run experiment directly on the dataset\\\\nconst result = await dataset.runExperiment({\\\\n name: \\\\\\\"Production Model Test\\\\\\\",\\\\n description: \\\\\\\"Monthly evaluation of our production model\\\\\\\",\\\\n task: myTask\\\\n});\\\\n \\\\n// Use format method to display results\\\\nconsole.log(await result.format());\\\\n \\\\n// Important: shut down OpenTelemetry to ensure traces are sent to Langfuse\\\\nawait otelSdk.shutdown()\\\\n```\\\\nWhen using Langfuse datasets, dataset runs are automatically created in Langfuse and are available for comparison in the UI. This enables tracking experiment performance over time and comparing different approaches on the same dataset.\\\\n### Advanced Features\\\\nEnhance your experiments with evaluators and advanced configuration options.\\\\n#### Evaluators\\\\nEvaluators assess the quality of task outputs at the item level. They receive the input, metadata, output, and expected output for each item and return evaluation metrics that are reported as scores on the traces in Langfuse.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import Evaluation\\\\n \\\\n///# Define evaluation functions\\\\ndef accuracy_evaluator(*, input, output, expected_output, metadata, **kwargs):\\\\n if expected_output and expected_output.lower() in output.lower():\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=1.0, comment=\\\\\\\"Correct answer found\\\\\\\")\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=0.0, comment=\\\\\\\"Incorrect answer\\\\\\\")\\\\n \\\\ndef length_evaluator(*, input, output, **kwargs):\\\\n return Evaluation(name=\\\\\\\"response_length\\\\\\\", value=len(output), comment=f\\\\\\\"Response has {len(output)} characters\\\\\\\")\\\\n \\\\n///# Use multiple evaluators\\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Multi-metric Evaluation\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[accuracy_evaluator, length_evaluator]\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\n// Define evaluation functions\\\\nconst accuracyEvaluator = async ({ input, output, expectedOutput }) => {\\\\n if (expectedOutput && output.toLowerCase().includes(expectedOutput.toLowerCase())) {\\\\n return {\\\\n name: \\\\\\\"accuracy\\\\\\\",\\\\n value: 1.0,\\\\n comment: \\\\\\\"Correct answer found\\\\\\\"\\\\n };\\\\n }\\\\n return {\\\\n name: \\\\\\\"accuracy\\\\\\\",\\\\n value: 0.0,\\\\n comment: \\\\\\\"Incorrect answer\\\\\\\"\\\\n };\\\\n};\\\\n \\\\nconst lengthEvaluator = async ({ input, output }) => {\\\\n return {\\\\n name: \\\\\\\"response_length\\\\\\\",\\\\n value: output.length,\\\\n comment: `Response has ${output.length} characters`\\\\n };\\\\n};\\\\n \\\\n// Use multiple evaluators\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Multi-metric Evaluation\\\\\\\",\\\\n data: testData,\\\\n task: myTask,\\\\n evaluators: [accuracyEvaluator, lengthEvaluator]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Run-level Evaluators\\\\nRun-level evaluators assess the full experiment results and compute aggregate metrics. When run on Langfuse datasets, these scores are attached to the full dataset run for tracking overall experiment performance.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import Evaluation\\\\n \\\\ndef average_accuracy(*, item_results, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Calculate average accuracy across all items\\\\\\\"\\\\\\\"\\\\\\\"\\\\n accuracies = [\\\\n eval.value for result in item_results\\\\n for eval in result.evaluations\\\\n if eval.name == \\\\\\\"accuracy\\\\\\\"\\\\n ]\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n if not accuracies:\\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=None)\\\\n \\\\n avg = sum(accuracies) / len(accuracies)\\\\n \\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=avg, comment=f\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\")\\\\n \\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Comprehensive Analysis\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy]\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst averageAccuracy = async ({ itemResults }) => {\\\\n // Calculate average accuracy across all items\\\\n const accuracies = itemResults\\\\n .flatMap(result => result.evaluations)\\\\n .filter(evaluation => evaluation.name === \\\\\\\"accuracy\\\\\\\")\\\\n .map(evaluation => evaluation.value as number);\\\\n \\\\n if (accuracies.length === 0) {\\\\n return { name: \\\\\\\"avg_accuracy\\\\\\\", value: null };\\\\n }\\\\n \\\\n const avg = accuracies.reduce((sum, val) => sum + val, 0) / accuracies.length;\\\\n \\\\n return {\\\\n name: \\\\\\\"avg_accuracy\\\\\\\",\\\\n value: avg,\\\\n comment: `Average accuracy: ${(avg * 100).toFixed(1)}%`\\\\n };\\\\n};\\\\n \\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Comprehensive Analysis\\\\\\\",\\\\n data: testData,\\\\n task: myTask,\\\\n evaluators: [accuracyEvaluator],\\\\n runEvaluators: [averageAccuracy]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Async Tasks and Evaluators\\\\nBoth task functions and evaluators can be asynchronous.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nimport asyncio\\\\nfrom langfuse.openai import AsyncOpenAI\\\\n \\\\nasync def async_llm_task(*, item, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Async task using OpenAI\\\\\\\"\\\\\\\"\\\\\\\"\\\\n client = AsyncOpenAI()\\\\n response = await client.chat.completions.create(\\\\n model=\\\\\\\"gpt-4\\\\\\\",\\\\n messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": item[\\\\\\\"input\\\\\\\"]}]\\\\n )\\\\n \\\\n return response.choices[0].message.content\\\\n \\\\n///# Works seamlessly with async functions\\\\nresult = langfuse.run_experiment(\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n name=\\\\\\\"Async Experiment\\\\\\\",\\\\n data=test_data,\\\\n task=async_llm_task,\\\\n max_concurrency=5 # Control concurrent API calls\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\nimport OpenAI from \\\\\\\"openai\\\\\\\";\\\\n \\\\nconst asyncLlmTask = async (item) => {\\\\n // Async task using OpenAI\\\\n const client = new OpenAI();\\\\n const response = await client.chat.completions.create({\\\\n model: \\\\\\\"gpt-4\\\\\\\",\\\\n messages: [{ role: \\\\\\\"user\\\\\\\", content: item.input }]\\\\n });\\\\n \\\\n return response.choices[0].message.content;\\\\n};\\\\n \\\\n// Works seamlessly with async functions\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Async Experiment\\\\\\\",\\\\n data: testData,\\\\n task: asyncLlmTask,\\\\n maxConcurrency: 5 // Control concurrent API calls\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Configuration Options\\\\nCustomize experiment behavior with various configuration options.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Configurable Experiment\\\\\\\",\\\\n run_name=\\\\\\\"Custom Run Name\\\\\\\", ///# will be dataset run name if dataset is used\\\\n description=\\\\\\\"Experiment with custom configuration\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy],\\\\n max_concurrency=10, ///# Max concurrent executions\\\\n metadata={ ///# Attached to all traces\\\\n \\\\\\\"model\\\\\\\": \\\\\\\"gpt-4\\\\\\\",\\\\n \\\\\\\"temperature\\\\\\\": 0.7,\\\\n \\\\\\\"version\\\\\\\": \\\\\\\"v1.2.0\\\\\\\"\\\\n }\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Configurable Experiment\\\\\\\",\\\\n runName: \\\\\\\"Custom Run Name\\\\\\\", // will be dataset run name if dataset is used\\\\n description: \\\\\\\"Experiment with custom configuration\\\\\\\",\\\\n data: testData,\\\\n task: myTask,\\\\n evaluators: [accuracyEvaluator],\\\\n runEvaluators: [averageAccuracy],\\\\n maxConcurrency: 10, // Max concurrent executions\\\\n metadata: { // Attached to all traces\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n model: \\\\\\\"gpt-4\\\\\\\",\\\\n temperature: 0.7,\\\\n version: \\\\\\\"v1.2.0\\\\\\\"\\\\n }\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Testing in CI Environments\\\\nIntegrate the experiment runner with testing frameworks like Pytest and Vitest to run automated evaluations in your CI pipeline. Use evaluators to create assertions that can fail tests based on evaluation results.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\n///# test_geography_experiment.py\\\\nimport pytest\\\\nfrom langfuse import get_client, Evaluation\\\\nfrom langfuse.openai import OpenAI\\\\n \\\\n///# Test data for European capitals\\\\ntest_data = [\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of France?\\\\\\\", \\\\\\\"expected\\\\\\\": \\\\\\\"Paris\\\\\\\"},\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of Germany?\\\\\\\", \\\\\\\"expected\\\\\\\": \\\\\\\"Berlin\\\\\\\"},\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of Spain?\\\\\\\", \\\\\\\"expected\\\\\\\": \\\\\\\"Madrid\\\\\\\"},\\\\n]\\\\n \\\\ndef geography_task(*, item, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Task function that answers geography questions\\\\\\\"\\\\\\\"\\\\\\\"\\\\n question = item[\\\\\\\"input\\\\\\\"]\\\\n response = OpenAI().chat.completions.create(\\\\n model=\\\\\\\"gpt-4\\\\\\\",\\\\n messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": question}]\\\\n )\\\\n return response.choices[0].message.content\\\\n \\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\"\\\\\\\"\\\\\\\"\\\\n if expected_output and expected_output.lower() in output.lower():\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=1.0)\\\\n \\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=0.0)\\\\n \\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\"\\\\\\\"\\\\\\\"\\\\n accuracies = [\\\\n eval.value for result in item_results\\\\n for eval in result.evaluations if eval.name == \\\\\\\"accuracy\\\\\\\"\\\\n ]\\\\n \\\\n if not accuracies:\\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=None)\\\\n \\\\n avg = sum(accuracies) / len(accuracies)\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThese examples show how to use the experiment runner\\\\u2019s evaluation results to create meaningful test assertions in your CI pipeline. Tests can fail when accuracy drops below acceptable thresholds, ensuring model quality standards are maintained automatically.\\\\n### Autoevals Integration\\\\nAccess pre-built evaluation functions through the [autoevals library](https://github.com/braintrustdata/autoevals) integration.\\\\nPython SDKJS/TS SDK\\\\nThe Python SDK supports AutoEvals evaluators through direct integration:\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse.experiment import create_evaluator_from_autoevals\\\\nfrom autoevals.llm import Factuality\\\\n \\\\nevaluator = create_evaluator_from_autoevals(Factuality())\\\\n \\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Autoevals Integration Test\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[evaluator]\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\n    \\\\n    \\\\n```\\\\nimport { Factuality, Levenshtein } from \\\\\\\"autoevals\\\\\\\";\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\"@langfuse/client\\\\\\\";\\\\n \\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\n \\\\n// Use with additional parameters\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\n Factuality,\\\\n { model: \\\\\\\"gpt-4o\\\\\\\" } // Additional AutoEvals parameters\\\\n);\\\\n \\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"AutoEvals Integration Test\\\\\\\",\\\\n data: testDataset,\\\\n task: myTask,\\\\n evaluators: [\\\\n factualityEvaluator,\\\\n levenshteinEvaluator,\\\\n customFactualityEvaluator\\\\n ]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n## Low-level SDK methods\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\n comment: \\\\\\\"This is a comment\\\\\\\", // optional, useful to add reasoning\\\\n });\\\\n}\\\\n \\\\n// Flush the langfuse client to ensure all score data is sent to the server at the end of the experiment run\\\\nawait langfuse.flush();\\\\n```\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\nfrom langfuse.langchain import CallbackHandler\\\\n///#from .app import my_llm_application\\\\n \\\\n///# Load the dataset\\\\ndataset = get_client().get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\n///# Initialize the Langfuse handler\\\\nlangfuse_handler = CallbackHandler()\\\\n \\\\n///# Loop over the dataset items\\\\nfor item in dataset.items:\\\\n # Use the item.run() context manager for automatic trace linking\\\\n with item.run(\\\\n run_name=\\\\\\\"<run_name>\\\\\\\",\\\\n run_description=\\\\\\\"My first run\\\\\\\",\\\\n run_metadata={\\\\\\\"model\\\\\\\": \\\\\\\"llama3\\\\\\\"},\\\\n ) as root_span:\\\\n # Execute your LLM-app against the dataset item input\\\\n output = my_langchain_chain(item.input, langfuse_handler)\\\\n \\\\n # Update top-level trace input and output\\\\n root_span.update_trace(input=item.input, output=output.content)\\\\n \\\\n # Optionally: Add scores computed in your experiment runner, e.g. json equality check\\\\n root_span.score_trace(\\\\n name=\\\\\\\"<example_eval>\\\\\\\",\\\\n value=my_eval_fn(item.input, output, item.expected_output),\\\\n comment=\\\\\\\"This is a comment\\\\\\\", ///# optional, useful to add reasoning\\\\n )\\\\n \\\\n///# Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run\\\\nget_client().flush()\\\\n```\\\\n    \\\\n    \\\\n```\\\\nimport { LangfuseClient } from \\\\\\\"@langfuse/client\\\\\\\";\\\\nimport { CallbackHandler } from \\\\\\\"@langfuse/langchain\\\\\\\";\\\\n...\\\\n \\\\nconst langfuse = new LangfuseClient()\\\\nconst runName = \\\\\\\"my-dataset-run\\\\\\\";\\\\nfor (const item of dataset.items) {\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/guides/cookbook/example_external_evaluation_pipelines\\\", \\\"title\\\": \\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline - Langfuse\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\"Cookbooks\\\\\\\")External Evaluation Pipelines\\\\nCopy as Markdown\\\\nThis is a Jupyter notebook\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\n# Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\\nThis cookbook explains how to build an external evaluation pipeline to measure the performance of your production LLM application using Langfuse.\\\\nAs a rule of thumb, we encourage you to check first if the [evaluations in the Langfuse UI](https://langfuse.com/docs/scores/model-based-evals) cover your use case. If your needs go beyond these, you can still implement in Langfuse custom evaluation templates without code.\\\\nConsider implementing an external evaluation pipeline if you need:\\\\n  * More control over **when** traces get evaluated. You could schedule the pipeline to run at specific times or responding to event-based triggers like Webhooks.\\\\n  * Greater flexibility with your custom evaluations, when your needs go beyond what\\\\u2019s possible with the Langfuse UI\\\\n  * Version control for your custom evaluations\\\\n  * The ability to evaluate data using existing evaluation frameworks\\\\nIf your use case meets any of this situations, let\\\\u2019s go ahead and implement your first external evaluation pipeline!\\\\n* * *\\\\nBy the end of this cookbook, you\\\\u2019ll be able to:\\\\n  * Create a synthetic dataset to test your models.\\\\n  * Use the Langfuse client to gather and filter traces of previous model runs\\\\n  * Evaluate these traces offline and incrementally\\\\n  * Add scores to existing Langfuse traces\\\\nConceptually, we will implement the following architecture:\\\\n* * *\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n**Note** : While we\\\\u2019re using a Jupyter notebook for this cookbook, in production you\\\\u2019d use your preferred orchestration tool. Just make sure to extract the code into a .py file and ensure all dependencies are available at runtime.\\\\n## (Prep-work) Loading synthetic traces to Langfuse\\\\nIn this demo, we\\\\u2019ll build a mock application: a science communicator LLM that explains any topic in an engaging and approachable way.\\\\nSince we don\\\\u2019t have real user data, our first step is to create a synthetic dataset. We\\\\u2019ll generate a variety of potential questions that real users might ask. While this is a great way to kickstart your LLM development, collecting real user queries as soon as possible is invaluable.\\\\nYou can get your Langfuse API keys [here](https://cloud.langfuse.com/) and OpenAI API key [here](https://platform.openai.com/api-keys)\\\\n    \\\\n    \\\\n```\\\\n%pip install langfuse openai deepeval --upgrade\\\\n```\\\\n    \\\\n    \\\\n```\\\\nimport os\\\\n \\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\nos.environ[\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\"] = \\\\\\\"pk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\"] = \\\\\\\"sk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddea\\\\ud83c\\\\uddfa EU region\\\\n///# os.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://us.cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddfa\\\\ud83c\\\\uddf8 US region\\\\n \\\\n///# Your openai key\\\\nos.environ[\\\\\\\"OPENAI_API_KEY\\\\\\\"] = \\\\\\\"sk-proj-...\\\\\\\"\\\\n```\\\\nLet\\\\u2019s go ahead and generate a list of topic suggestions that we can later query to our application.\\\\n    \\\\n    \\\\n```\\\\nimport openai\\\\n \\\\ntopic_suggestion = \\\\\\\"\\\\\\\"\\\\\\\" You're a world-class journalist, specialized\\\\nin figuring out which are the topics that excite people the most.\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nBATCH_SIZE = 10\\\\nTOTAL_TRACES = 50\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nnow = datetime.now()\\\\nfive_am_today = datetime(now.year, now.month, now.day, 5, 0)\\\\nfive_am_yesterday = five_am_today - timedelta(days=1)\\\\n \\\\ntraces_batch = langfuse.api.trace.list(page=1,\\\\n limit=BATCH_SIZE,\\\\n tags=\\\\\\\"ext_eval_pipelines\\\\\\\",\\\\n from_timestamp=five_am_yesterday,\\\\n to_timestamp=datetime.now()\\\\n ).data\\\\n \\\\nprint(f\\\\\\\"Traces in first batch: {len(traces_batch)}\\\\\\\")\\\\n```\\\\nTraces in first batch: 10\\\\n## 2\\\\\\\\. Run your evaluations\\\\nLangfuse can handle numerical, boolean and categorical (`string`) scores. Wrapping your custom evaluation logic in a function is often a good practice. Evaluation functions should take a `trace` as input and yield a valid score. Let\\\\u2019s begin with a simple example using a categorical score.\\\\n### 2.1. Categoric Evaluations\\\\nWhen analyzing the outputs of your LLM applications, you may want to evaluate traits that are best defined qualitatively, such as sentiment, tonality or text complexity (Grade level).\\\\nWe\\\\u2019re building a science educator LLM that should sound engaging and positive. To ensure it hits the right notes, we\\\\u2019ll evaluate the tone of its outputs to see if they match our intent. We\\\\u2019ll draft an evaluation prompt ourselves (no library) to identify the three main tones in each model output.\\\\n    \\\\n    \\\\n```\\\\ntemplate_tone_eval = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nYou're an expert in human emotional intelligence. You can identify with ease the\\\\n tone in human-written text. Your task is to identify the tones present in a\\\\n piece of <text/> with precission. Your output is a comma separated list of three\\\\n tones. PRINT THE LIST ALONE, NOTHING ELSE.\\\\n \\\\n<possible_tones>\\\\nneutral, confident, joyful, optimistic, friendly, urgent, analytical, respectful\\\\n</possible_tones>\\\\n \\\\n<example_1>\\\\nInput: Citizen science plays a crucial role in research by involving everyday\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\npeople in scientific projects. This collaboration allows researchers to collect\\\\nvast amounts of data that would be impossible to gather on their own. Citizen\\\\nscientists contribute valuable observations and insights that can lead to new\\\\ndiscoveries and advancements in various fields. By participating in citizen\\\\nscience projects, individuals can actively contribute to scientific research\\\\nand make a meaningful impact on our understanding of the world around us.\\\\n \\\\nOutput: respectful,optimistic,confident\\\\n</example_1>\\\\n \\\\n<example_2>\\\\nInput: Bionics is a field that combines biology and engineering to create\\\\ndevices that can enhance human abilities. By merging humans and machines,\\\\nbionics aims to improve quality of life for individuals with disabilities\\\\nor enhance performance for others. These technologies often mimic natural\\\\nprocesses in the body to create seamless integration. Overall, bionics holds\\\\ngreat potential for revolutionizing healthcare and technology in the future.\\\\n \\\\nOutput: optimistic,confident,analytical\\\\n</example_2>\\\\n \\\\n<example_3>\\\\nInput: Social media can have both positive and negative impacts on mental\\\\nhealth. On the positive side, it can help people connect, share experiences,\\\\nand find support. However, excessive use of social media can also lead to\\\\nfeelings of inadequacy, loneliness, and anxiety. It's important to find a\\\\nbalance and be mindful of how social media affects your mental well-being.\\\\nRemember, it's okay to take breaks and prioritize your mental health.\\\\n \\\\nOutput: friendly,neutral,respectful\\\\n</example_3>\\\\n \\\\n<text>\\\\n{text}\\\\n</text>\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n \\\\n \\\\ntest_tone_score = openai.chat.completions.create(\\\\n messages=[\\\\n {\\\\n \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\",\\\\n \\\\\\\"content\\\\\\\": template_tone_eval.format(\\\\n text=traces_batch[1].output),\\\\n }\\\\n ],\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n model=\\\\\\\"gpt-4o\\\\\\\",\\\\n \\\\n temperature=0\\\\n).choices[0].message.content\\\\nprint(f\\\\\\\"User query: {traces_batch[1].input['args'][0]}\\\\\\\")\\\\nprint(f\\\\\\\"Model answer: {traces_batch[1].output}\\\\\\\")\\\\nprint(f\\\\\\\"Dominant tones: {test_tone_score}\\\\\\\")\\\\n```\\\\nIdentifying human intents and tones can be tricky for language models. To handle this, we used a multi-shot prompt, which means giving the model several examples to learn from. Now let\\\\u2019s wrap our code in an evaluation function for convenience.\\\\n    \\\\n    \\\\n```\\\\ndef tone_score(trace):\\\\n return openai.chat.completions.create(\\\\n messages=[\\\\n {\\\\n \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\",\\\\n \\\\\\\"content\\\\\\\": template_tone_eval.format(text=trace.output),\\\\n }\\\\n ],\\\\n model=\\\\\\\"gpt-4o\\\\\\\",\\\\n temperature=0\\\\n ).choices[0].message.content\\\\n \\\\ntone_score(traces_batch[1])\\\\n```\\\\nGreat! Now let\\\\u2019s go ahead and create a numeric evaluation score.\\\\n### 2.2. Numeric Evaluations\\\\nIn this cookbook, we\\\\u2019ll use the `Deepeval` framework ([docs](https://docs.confident-ai.com/docs/getting-started)) to handle our numeric evaluations. Deepeval provides scores ranging from zero to one for many common LLM metrics. Plus, you can create custom metrics by simply describing them in plain language. To ensure our app\\\\u2019s responses are joyful and engaging, we\\\\u2019ll define a custom \\\\u2018joyfulness\\\\u2019 score.\\\\nYou can use any evaluation library. These are popular ones:\\\\n  * OpenAI Evals ([GitHub](https://github.com/openai/evals))\\\\n  * Langchain Evaluators\\\\n  * [RAGAS](https://docs.ragas.io/en/latest/concepts/metrics/index.html) for RAG applications\\\\n    \\\\n    \\\\n```\\\\nfrom deepeval.metrics import GEval\\\\nfrom deepeval.test_case import LLMTestCaseParams, LLMTestCase\\\\n \\\\ndef joyfulness_score(trace):\\\\n joyfulness_metric = GEval(\\\\n name=\\\\\\\"Correctness\\\\\\\",\\\\n criteria=\\\\\\\"Determine whether the output is engaging and fun.\\\\\\\",\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\\\\n )\\\\n test_case = LLMTestCase(\\\\n input=trace.input[\\\\\\\"args\\\\\\\"],\\\\n actual_output=trace.output)\\\\n \\\\n joyfulness_metric.measure(test_case)\\\\n \\\\n print(f\\\\\\\"Score: {joyfulness_metric.score}\\\\\\\")\\\\n print(f\\\\\\\"Reason: {joyfulness_metric.reason}\\\\\\\")\\\\n \\\\n return {\\\\\\\"score\\\\\\\": joyfulness_metric.score, \\\\\\\"reason\\\\\\\": joyfulness_metric.reason}\\\\n \\\\njoyfulness_score(traces_batch[1])\\\\n```\\\\nUnder the hood, GEval uses chain of thought (CoT) prompting to formulate a set of criteria for scoring prompts. When developing your own metrics, it\\\\u2019s important to review the reasoning behind these scores. This helps ensure that the model evaluates the traces just as you intended when you wrote the evaluation prompt.\\\\nOur eval function returns a dictionary with both the score and the model\\\\u2019s reasoning. We do this as we\\\\u2019ll persist the reasoning with every langfuse score, ensuring interpretability.\\\\nNow we\\\\u2019re done with defining our evaluation functions. Let\\\\u2019s push those scores back to Langfuse!\\\\n## 3\\\\\\\\. Pushing Scores to Langfuse\\\\nNow that we have our evaluation functions ready, it\\\\u2019s time to put them to work. Use the Langfuse client to add scores to existing traces.\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_score(\\\\n trace_id=traces_batch[1].id,\\\\n name=\\\\\\\"tone\\\\\\\",\\\\n value=joyfulness_score(traces_batch[1])[\\\\\\\"score\\\\\\\"],\\\\n comment=joyfulness_score(traces_batch[1])[\\\\\\\"reason\\\\\\\"]\\\\n)\\\\n```\\\\nAnd thus, you\\\\u2019ve added your first externally-evaluated score to Langfuse! Just 49 more to go \\\\ud83d\\\\ude01. But don\\\\u2019t worry \\\\u2014 our solutions are easy to scale.\\\\n## 4\\\\\\\\. Putting everything together\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nUntil now, we went through each of the necessary steps to build an external evaluation pipeline: Fetching traces, running the evaluations, and persisting the scores to Langfuse. Let\\\\u2019s sum it up into a compact script that you could run in your evaluation pipeline.\\\\nWe\\\\u2019ll fetch the data in batches of 10 traces and then iterate through each trace to score it and push the scores back to Langfuse. Note that this batch size is for demonstration purposes. In a production setup, you might want to process multiple batches in parallel to speed things up. Batching not only reduces the memory load on your system but also allows you to create checkpoints, so you can easily resume if something goes wrong.\\\\n    \\\\n    \\\\n```\\\\nimport math\\\\n \\\\nfor page_number in range(1, math.ceil(TOTAL_TRACES/BATCH_SIZE)):\\\\n \\\\n traces_batch = langfuse.api.trace.list(\\\\n tags=\\\\\\\"ext_eval_pipelines\\\\\\\",\\\\n page=page_number,\\\\n from_timestamp=five_am_yesterday,\\\\n to_timestamp=five_am_today,\\\\n limit=BATCH_SIZE\\\\n ).data\\\\n \\\\n for trace in traces_batch:\\\\n print(f\\\\\\\"Processing {trace.name}\\\\\\\")\\\\n \\\\n if trace.output is None:\\\\n print(f\\\\\\\"Warning: \\\\\\\\n Trace {trace.name} had no generated output, \\\\\\\\\\\\n it was skipped\\\\\\\")\\\\n continue\\\\n \\\\n langfuse.create_score(\\\\n trace_id=trace.id,\\\\n name=\\\\\\\"tone\\\\\\\",\\\\n value=tone_score(trace)\\\\n )\\\\n \\\\n jscore = joyfulness_score(trace)\\\\n langfuse.create_score(\\\\n trace_id=trace.id,\\\\n name=\\\\\\\"joyfulness\\\\\\\",\\\\n value=jscore[\\\\\\\"score\\\\\\\"],\\\\n comment=jscore[\\\\\\\"reason\\\\\\\"]\\\\n )\\\\n \\\\n print(f\\\\\\\"Batch {page_number} processed \\\\ud83d\\\\ude80 \\\\\\\\n\\\\\\\")\\\\n```\\\\nIf your pipeline ran successfully, you should see your score in the Langfuse UI. \\\\nAnd that\\\\u2019s it! You\\\\u2019re now ready to integrate these lines into your preferred orchestration tool to ensure they run at the right times.\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nTo achieve our original goal of running the script every day at 5 am, simply schedule a Cron task in your chosen environment with the rule `cron(0 5 * * ? *)`.\\\\nThanks for coding along! I hope you enjoyed the tutorial and found it helpful.\\\\nLast updated on August 20, 2025\\\\n[Example - Tracing and Evaluation for the OpenAI-Agents SDK](/guides/cookbook/example_evaluating_openai_agents \\\\\\\"Example - Tracing and Evaluation for the OpenAI-Agents SDK\\\\\\\")\\\\n[Guide - Building an intent classification pipeline](/guides/cookbook/example_intent_classification_pipeline \\\\\\\"Guide - Building an intent classification pipeline\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"github_discussion\\\", \\\"url\\\": \\\"https://github.com/orgs/langfuse/discussions/7572\\\", \\\"title\\\": \\\"LLM-as-a-Judge Evaluators for Dataset doesn't work as expected.\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\n\\\\n---\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=hallucination) | [Report \\\\ud83d\\\\udc1b](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\n\\\\n        Reply #5 to comment #1 from dosubot on June 30, 2025:\\\\n\\\\n        I am using ui and variable mapping is as follows:\\\\r\\\\n\\\\r\\\\n{{search_query}} is mapped to Object: Dataset item, Object Variable: Input, JsonPath: <empty>\\\\r\\\\n{{summary}} is mapped to Object: Dataset item, Object Variable: Metadata, JsonPath: <empty>\\\\r\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{{expected}} is mapped to Object: Dataset item,  Object Variable: Expected Output, JsonPath: <empty>\\\\r\\\\n\\\\r\\\\nA single row format example from my dataset: \\\\r\\\\nInput column: \\\\\\\"Test input\\\\\\\", Expected Output column: \\\\\\\"Test expected output\\\\\\\", Metadata column: \\\\\\\"Test metadata\\\\\\\"\\\\n\\\\n        Reply #6 to comment #1 from dosubot on June 30, 2025:\\\\n\\\\n        <!-- Answer -->\\\\nYour mapping setup looks correct, but the backend eligibility logic is strict: if any mapped variable can't be resolved (for example, if the field is empty or the type doesn't match expectations), the item is silently skipped and you get \\\\\\\"0 elements\\\\\\\" processed. \\\\n\\\\nOne likely issue is the Metadata field\\\\u2014if it's a plain string (like \\\\\\\"Test metadata\\\\\\\"), but the system expects a JSON object (since many Langfuse examples use objects for metadata), this can cause the mapping to fail even if JsonPath is empty. When JsonPath is empty, the backend uses the top-level value as-is, but if the field is a string and the mapping expects an object (or vice versa), the item is excluded from evaluation\\\\u00a0[see code logic](https://github.com/langfuse/langfuse/blob/98cc1bb7a1759e29de3dd818f6a500df48fb5e82/worker/src/features/evaluation/evalService.ts).\\\\n\\\\nTry this: map all variables to fields you know are plain strings (like Input or Expected Output) and see if the evaluation processes any items. If it works, the issue is with the Metadata field's type. To confirm, create a dataset item where Metadata is a JSON object (e.g., {\\\\\\\"meta\\\\\\\": \\\\\\\"value\\\\\\\"}) and try mapping with and without a JsonPath. This will help isolate the problem.\\\\n\\\\nIf you still get 0 elements, export a sample of your dataset as JSON and share it\\\\u2014there may be a subtle data or mapping mismatch. \\\\n\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nMore details on dataset item structure and mapping: [Langfuse docs](https://langfuse.com/docs/datasets/get-started).\\\\n\\\\n\\\\n<!-- Dosu Comment Footer -->\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\n\\\\n---\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=hallucination) | [Report \\\\ud83d\\\\udc1b](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\n\\\\n        Reply #7 to comment #1 from dosubot on June 30, 2025:\\\\n\\\\n        I have created a new dataset with single row in json format, so columns Input, Expected Output and Metadata have following values respectively but still the same issue:\\\\r\\\\n{ \\\\\\\"search_query\\\\\\\": \\\\\\\"What's MLflow?\\\\\\\" }\\\\r\\\\n\\\\r\\\\n{ \\\\\\\"expected\\\\\\\": \\\\\\\"MLflow tracks ML experiments.\\\\\\\" } \\\\r\\\\n\\\\r\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2023-09-25-datasets\\\", \\\"title\\\": \\\"Datasets (beta)\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Datasets (beta) - Langfuse\\\\n[\\\\u2190 Back to changelog](/changelog#2023-09-25-datasets)\\\\nSeptember 25, 2023\\\\n# Datasets (beta)\\\\n[Marc Klingen](https://twitter.com/marcklingen)\\\\nCollect sets of inputs and expected outputs in Langfuse to evaluate your LLM app. Use evaluations to benchmark different experiments.\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\n_Run experiment on dataset_\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\n    \\\\n    \\\\n```\\\\n \\\\ndataset = langfuse.get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\nfor item in dataset.items:\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\n # output also returned as it is used to evaluate the run\\\\n generation, output = my_llm_application.run(item.input)\\\\n \\\\n # link the execution trace to the dataset item and give it a run_name\\\\n item.link(generation, \\\\\\\"<run_name>\\\\\\\")\\\\n \\\\n # optionally, evaluate the output to compare different runs more easily\\\\n generation.score(\\\\n name=\\\\\\\"<example_eval>\\\\\\\",\\\\n # any float value\\\\n value=my_eval_fn(\\\\n item.input,\\\\n output,\\\\n item.expected_output\\\\n )\\\\n )\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst dataset = await langfuse.getDataset(\\\\\\\"<dataset_name>\\\\\\\");\\\\n \\\\nfor (const item of dataset.items) {\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\n // output also returned as it is used to evaluate the run\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\n \\\\n // link the execution trace to the dataset item and give it a run_name\\\\n await item.link(generation, \\\\\\\"<run_name>\\\\\\\");\\\\n \\\\n // optionally, evaluate the output to compare different runs more easily\\\\n generation.score({\\\\n name: \\\\\\\"<score_name>\\\\\\\",\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\n });\\\\n}\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\", \\\"title\\\": \\\"Troubleshooting and FAQ\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\(\\\\\\\\)` method\\\\\\\")\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\")\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\"Langfuse Support: \\\\\\\\[Experiment\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\")\\\\n[Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing\\\\\\\")\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\")\\\\n[Results for some data items not present when comparing experiments](https://github.com/orgs/langfuse/discussions/5928 \\\\\\\"Langfuse Support: Results for some data items not present when comparing experiments\\\\\\\")\\\\n[Deleting Metrics for Langfuse](https://github.com/orgs/langfuse/discussions/5849 \\\\\\\"Langfuse Support: Deleting Metrics for Langfuse\\\\\\\")\\\\n[Discrepancies between dataset items found in the UI vs retrieved from the SDK/API](https://github.com/orgs/langfuse/discussions/5822 \\\\\\\"Langfuse Support: Discrepancies between dataset items found in the UI vs retrieved from the SDK/API\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Request batch adding of multiple traces to datasets](https://github.com/orgs/langfuse/discussions/8526 \\\\\\\"Langfuse Ideas: Request batch adding of multiple traces to datasets\\\\\\\")\\\\n[How to categorize topics and create a pie chart?](https://github.com/orgs/langfuse/discussions/8512 \\\\\\\"Langfuse Ideas: How to categorize topics and create a pie chart?\\\\\\\")\\\\n[Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window](https://github.com/orgs/langfuse/discussions/8485 \\\\\\\"Langfuse Ideas: Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window\\\\\\\")\\\\n[feat(LLM-as-a-judge): support stratified sampling by trace property](https://github.com/orgs/langfuse/discussions/8480 \\\\\\\"Langfuse Ideas: feat\\\\\\\\(LLM-as-a-judge\\\\\\\\): support stratified sampling by trace property\\\\\\\")\\\\n[Bulk Evaluation Does not Displays the status of the dataset (Like for E.g. processing, evaluated, executed)](https://github.com/orgs/langfuse/discussions/8410 \\\\\\\"Langfuse Ideas: Bulk Evaluation Does not Displays the status of the dataset \\\\\\\\(Like for E.g. processing, evaluated, executed\\\\\\\\)\\\\\\\")\\\\n[Annotation Queue Creation API](https://github.com/orgs/langfuse/discussions/8372 \\\\\\\"Langfuse Ideas: Annotation Queue Creation API\\\\\\\")\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\"Langfuse Ideas: \\\\\\\\[Langfuse Cloud\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\")\\\\n[feat(evals): allow canceling a running evaluator with pending evaluation jobs](https://github.com/orgs/langfuse/discussions/8310 \\\\\\\"Langfuse Ideas: feat\\\\\\\\(evals\\\\\\\\): allow canceling a running evaluator with pending evaluation jobs\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Support Description on Dataset Items](https://github.com/orgs/langfuse/discussions/6011 \\\\\\\"Langfuse Ideas: Support Description on Dataset Items\\\\\\\")\\\\n[Alta Integration](https://github.com/orgs/langfuse/discussions/5957 \\\\\\\"Langfuse Ideas: Alta Integration\\\\\\\")\\\\n[Add run name to columns when looking at a specific dataset item](https://github.com/orgs/langfuse/discussions/5929 \\\\\\\"Langfuse Ideas: Add run name to columns when looking at a specific dataset item\\\\\\\")\\\\n[Delete multiple dataset runs](https://github.com/orgs/langfuse/discussions/5893 \\\\\\\"Langfuse Ideas: Delete multiple dataset runs\\\\\\\")\\\\n[Navigation between items in a dataset run is confusing - context of the selected dataset run is lost](https://github.com/orgs/langfuse/discussions/5892 \\\\\\\"Langfuse Ideas: Navigation between items in a dataset run is confusing - context of the selected dataset run is lost\\\\\\\")\\\\n[Enhanced score distribution visualization in experiment analysis](https://github.com/orgs/langfuse/discussions/5819 \\\\\\\"Langfuse Ideas: Enhanced score distribution visualization in experiment analysis\\\\\\\")\\\\n[Multi-step Prompt Experiments and Playground](https://github.com/orgs/langfuse/discussions/5812 \\\\\\\"Langfuse Ideas: Multi-step Prompt Experiments and Playground\\\\\\\")\\\\n[Option to add trace to new dataset](https://github.com/orgs/langfuse/discussions/5756 \\\\\\\"Langfuse Ideas: Option to add trace to new dataset\\\\\\\")\\\\n[Simplified UI for Scoring](https://github.com/orgs/langfuse/discussions/5721 \\\\\\\"Langfuse Ideas: Simplified UI for Scoring\\\\\\\")\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Datasets: Add selection of traces to a dataset](https://github.com/orgs/langfuse/discussions/4353 \\\\\\\"Langfuse Ideas: Datasets: Add selection of traces to a dataset\\\\\\\")\\\\n[Multi-user annotation capability in Annotation Queues](https://github.com/orgs/langfuse/discussions/4348 \\\\\\\"Langfuse Ideas: Multi-user annotation capability in Annotation Queues\\\\\\\")\\\\n[Multi-turn / session experiments in datasets](https://github.com/orgs/langfuse/discussions/4208 \\\\\\\"Langfuse Ideas: Multi-turn / session experiments in datasets\\\\\\\")\\\\n[Enable to use variable of prompt on evaluator.](https://github.com/orgs/langfuse/discussions/4121 \\\\\\\"Langfuse Ideas: Enable to use variable of prompt on evaluator.\\\\\\\")\\\\n[Sessions Table: Scores Column](https://github.com/orgs/langfuse/discussions/4120 \\\\\\\"Langfuse Ideas: Sessions Table: Scores Column\\\\\\\")\\\\n[Add new filters for the LLM as a Judge Evaluation (other scores and cost)](https://github.com/orgs/langfuse/discussions/4106 \\\\\\\"Langfuse Ideas: Add new filters for the LLM as a Judge Evaluation \\\\\\\\(other scores and cost\\\\\\\\)\\\\\\\")\\\\n[Export dataset run table](https://github.com/orgs/langfuse/discussions/4077 \\\\\\\"Langfuse Ideas: Export dataset run table\\\\\\\")\\\\n[feat: support adding trace tags in annotation queue view](https://github.com/orgs/langfuse/discussions/4037 \\\\\\\"Langfuse Ideas: feat: support adding trace tags in annotation queue view\\\\\\\")\\\\n[Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times](https://github.com/orgs/langfuse/discussions/4035 \\\\\\\"Langfuse Ideas: Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times\\\\\\\")\\\\n[Diff support for dataset runs view](https://github.com/orgs/langfuse/discussions/4025 \\\\\\\"Langfuse Ideas: Diff support for dataset runs view\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Create Support for gemini models in playground](https://github.com/orgs/langfuse/discussions/4019 \\\\\\\"Langfuse Ideas: Create Support for gemini models in playground\\\\\\\")\\\\n[Change AWS access pattern for Bedrock LLM usage, assume role](https://github.com/orgs/langfuse/discussions/3988 \\\\\\\"Langfuse Ideas: Change AWS access pattern for Bedrock LLM usage, assume role\\\\\\\")\\\\n[Add ability to export and import evaluators between projects](https://github.com/orgs/langfuse/discussions/3970 \\\\\\\"Langfuse Ideas: Add ability to export and import evaluators between projects\\\\\\\")\\\\n[feat: Folder structure for dataset organisation](https://github.com/orgs/langfuse/discussions/3935 \\\\\\\"Langfuse Ideas: feat: Folder structure for dataset organisation\\\\\\\")\\\\n[Model-based evaluations triggered by observations](https://github.com/orgs/langfuse/discussions/3918 \\\\\\\"Langfuse Ideas: Model-based evaluations triggered by observations\\\\\\\")\\\\n[Scores: Conditional Annotation](https://github.com/orgs/langfuse/discussions/3842 \\\\\\\"Langfuse Ideas: Scores: Conditional Annotation\\\\\\\")\\\\n[Annotation Queues: define optional/mandatory score configs by queue](https://github.com/orgs/langfuse/discussions/3841 \\\\\\\"Langfuse Ideas: Annotation Queues: define optional/mandatory score configs by queue\\\\\\\")\\\\n[Scores: support for recording multiple choice selection as score value](https://github.com/orgs/langfuse/discussions/3840 \\\\\\\"Langfuse Ideas: Scores: support for recording multiple choice selection as score value\\\\\\\")\\\\n[Filter by status in dataset items table](https://github.com/orgs/langfuse/discussions/3818 \\\\\\\"Langfuse Ideas: Filter by status in dataset items table\\\\\\\")\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/overview\\\", \\\"title\\\": \\\"Evaluation Overview\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Evaluation of LLM Applications - Langfuse\\\\nDocsEvaluationOverview\\\\nCopy as Markdown\\\\n# Evaluation Overview\\\\nEvaluation is a critical aspect of developing and deploying LLM applications. Usually, teams use a multitude of different evaluation methods to score the performance of their AI application depending on the use case and the stage of the development process.\\\\n\\\\ud83c\\\\udfa5\\\\n[ **Watch this walkthrough**](/watch-demo?tab=evaluation) of Langfuse Evaluation and how to use it to improve your LLM application.\\\\n## Why use LLM Evaluation?\\\\nLLM evaluation is crucial for improving the accuracy and robustness of language models, ultimately enhancing the user experience and trust in your AI application. Here are the key benefits:\\\\n  * **Quality Assurance** : Detect hallucinations, factual inaccuracies, and inconsistent outputs to ensure your AI app delivers reliable results\\\\n  * **Performance Monitoring** : Measure response quality, relevance, and user satisfaction across different scenarios and edge cases\\\\n  * **Continuous Improvement** : Identify areas for enhancement and track improvements over time through structured evaluation metrics\\\\n  * **User Trust** : Build confidence in your AI application by demonstrating consistent, high-quality outputs through systematic evaluation\\\\n  * **Risk Mitigation** : Catch potential issues before they reach production users, reducing the likelihood of poor user experiences or reputational damage\\\\n## Online & Offline Evaluation\\\\n**Offline Evaluation involves**\\\\n  * Evaluating the application in a controlled setting\\\\n  * Typically using curated test Datasets instead of live user queries\\\\n  * Heavily used during development (can be part of CI/CD pipelines) to measure improvements / regressions\\\\n  * Repeatable and you can get clear accuracy metrics since you have ground truth.\\\\n**Online Evaluation involves**\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Python SDK: Pass ScoreConfig instead of config_id when creating scores](https://github.com/orgs/langfuse/discussions/8623 \\\\\\\"Langfuse Ideas: Python SDK: Pass ScoreConfig instead of config_id when creating scores\\\\\\\")\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\"Langfuse Ideas: \\\\\\\\[Langfuse Cloud\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\")\\\\n[Score Configs: Allow editing the categories of a categorical score](https://github.com/orgs/langfuse/discussions/8259 \\\\\\\"Langfuse Ideas: Score Configs: Allow editing the categories of a categorical score\\\\\\\")\\\\n[UI-LLM as a Jury](https://github.com/orgs/langfuse/discussions/8195 \\\\\\\"Langfuse Ideas: UI-LLM as a Jury\\\\\\\")\\\\n[Enable Immediate Score Management for User Feedback](https://github.com/orgs/langfuse/discussions/7686 \\\\\\\"Langfuse Ideas: Enable Immediate Score Management for User Feedback\\\\\\\")\\\\n[Filter by scores in session view](https://github.com/orgs/langfuse/discussions/7528 \\\\\\\"Langfuse Ideas: Filter by scores in session view\\\\\\\")\\\\n[Support break lines on evaluation run tooltip hint](https://github.com/orgs/langfuse/discussions/7452 \\\\\\\"Langfuse Ideas: Support break lines on evaluation run tooltip hint\\\\\\\")\\\\n[Support new lines when storing / displaying score comments](https://github.com/orgs/langfuse/discussions/6473 \\\\\\\"Langfuse Ideas: Support new lines when storing / displaying score comments\\\\\\\")\\\\n[Evaluator: Filter for Scores](https://github.com/orgs/langfuse/discussions/6236 \\\\\\\"Langfuse Ideas: Evaluator: Filter for Scores\\\\\\\")\\\\n[Code-based custom evaluators](https://github.com/orgs/langfuse/discussions/6087 \\\\\\\"Langfuse Ideas: Code-based custom evaluators\\\\\\\")\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Session-level scores](https://github.com/orgs/langfuse/discussions/2728 \\\\\\\"Langfuse Ideas: Session-level scores\\\\\\\")\\\\n[Scoring dataset runs, e.g. precision, recall, f-value](https://github.com/orgs/langfuse/discussions/2511 \\\\\\\"Langfuse Ideas: Scoring dataset runs, e.g. precision, recall, f-value\\\\\\\")\\\\n[Adding userId / author to score (custom metadata)](https://github.com/orgs/langfuse/discussions/2469 \\\\\\\"Langfuse Ideas: Adding userId / author to score \\\\\\\\(custom metadata\\\\\\\\)\\\\\\\")\\\\n[Add string data type in score config](https://github.com/orgs/langfuse/discussions/2402 \\\\\\\"Langfuse Ideas: Add string data type in score config\\\\\\\")\\\\n[API to delete scores](https://github.com/orgs/langfuse/discussions/1133 \\\\\\\"Langfuse Ideas: API to delete scores\\\\\\\")\\\\nGitHubSupportGitHubIdeas\\\\nUpvotes[GitHubNew](https://github.com/orgs/langfuse/discussions/new/choose)\\\\n  * 9votes\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486)\\\\nmsanand\\\\u20223/25/2024\\\\u2022\\\\n2Resolved\\\\n  * 4votes\\\\n[Filter Categorical Score Values](https://github.com/orgs/langfuse/discussions/3797)\\\\nalabrashJr\\\\u202210/17/2024\\\\u2022\\\\n3Resolved\\\\n  * 3votes\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590)\\\\nanuras\\\\u20228/18/2025\\\\u2022\\\\n1Resolved\\\\n  * 3votes\\\\n[Support for Metric Calculation (Precision@K, Recall@K) and Adding Custom Metrics Use Case Overview](https://github.com/orgs/langfuse/discussions/5215)\\\\nsrimantacse\\\\u20221/27/2025\\\\u2022\\\\n2Resolved\\\\n  * 2votes\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556)\\\\nflabbergastedbd\\\\u20228/15/2025\\\\u2022\\\\n1\\\\n  * 1votes\\\"}], \\\"type\\\": \\\"content\\\"}}]}\"}],\"isError\":false}",
        "modelParameters": {
          "maxSteps": null,
          "toolChoice": null,
          "maxTokens": null,
          "finishReason": null,
          "system": null,
          "maxRetries": null,
          "mode": null,
          "temperature": null
        },
        "completionStartTime": null,
        "promptId": null,
        "createdAt": "2025-09-19T06:51:30.170Z",
        "updatedAt": "2025-09-19T06:51:30.170Z",
        "usageDetails": {},
        "costDetails": {},
        "providedCostDetails": {},
        "model": null,
        "internalModelId": null,
        "promptName": null,
        "promptVersion": null,
        "latency": 4800,
        "timeToFirstToken": null,
        "inputCost": null,
        "outputCost": null,
        "totalCost": 0,
        "inputUsage": 0,
        "outputUsage": 0,
        "totalUsage": 0,
        "internalModel": null
      },
      {
        "id": "0f202199b1c8adf7",
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "SPAN",
        "environment": "default",
        "parentObservationId": "2126822ce3db7c07",
        "startTime": "2025-09-19T06:51:14.796Z",
        "endTime": "2025-09-19T06:51:15.715Z",
        "name": "create-mcp-client",
        "metadata": "{}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "modelParameters": null,
        "completionStartTime": null,
        "promptId": null,
        "createdAt": "2025-09-19T06:51:19.874Z",
        "updatedAt": "2025-09-19T06:51:19.875Z",
        "usageDetails": {},
        "costDetails": {},
        "providedCostDetails": {},
        "model": null,
        "internalModelId": null,
        "promptName": null,
        "promptVersion": null,
        "latency": 919,
        "timeToFirstToken": null,
        "inputCost": null,
        "outputCost": null,
        "totalCost": 0,
        "inputUsage": 0,
        "outputUsage": 0,
        "totalUsage": 0
      },
      {
        "id": "86931488451a1240",
        "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "SPAN",
        "environment": "default",
        "parentObservationId": "2126822ce3db7c07",
        "startTime": "2025-09-19T06:51:14.795Z",
        "endTime": "2025-09-19T06:51:14.795Z",
        "name": "get-langfuse-prompt",
        "metadata": "{\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"langfuse-sdk\",\"version\":\"4.0.0\",\"attributes\":{}}}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "input": "langfuse-docs-assistant-text",
        "output": "\"{\\\"name\\\":\\\"langfuse-docs-assistant-text\\\",\\\"prompt\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"version\\\":10,\\\"isFallback\\\":false,\\\"tags\\\":[],\\\"labels\\\":[\\\"production\\\",\\\"latest\\\"],\\\"type\\\":\\\"text\\\",\\\"config\\\":{\\\"model\\\":\\\"gpt-5\\\",\\\"reasoningSummary\\\":\\\"detailed\\\",\\\"textVerbosity\\\":\\\"low\\\",\\\"reasoningEffort\\\":\\\"low\\\"}}\"",
        "modelParameters": null,
        "completionStartTime": null,
        "promptId": null,
        "createdAt": "2025-09-19T06:51:19.874Z",
        "updatedAt": "2025-09-19T06:51:19.875Z",
        "usageDetails": {},
        "costDetails": {},
        "providedCostDetails": {},
        "model": null,
        "internalModelId": null,
        "promptName": null,
        "promptVersion": null,
        "latency": 0,
        "timeToFirstToken": null,
        "inputCost": null,
        "outputCost": null,
        "totalCost": 0,
        "inputUsage": 0,
        "outputUsage": 0,
        "totalUsage": 0,
        "internalModel": null
      }
    ]
  },
  "observations": [
    {
      "id": "acf76ba316194a49",
      "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "GENERATION",
      "environment": "default",
      "parentObservationId": "2126822ce3db7c07",
      "startTime": "2025-09-19T06:51:15.777Z",
      "endTime": "2025-09-19T06:52:04.063Z",
      "name": "ai.streamText",
      "metadata": "{\"attributes\":{\"operation.name\":\"ai.streamText\",\"ai.operationId\":\"ai.streamText\",\"ai.model.provider\":\"openai.responses\",\"ai.model.id\":\"gpt-5\",\"ai.settings.maxRetries\":\"2\",\"ai.telemetry.metadata.langfusePrompt\":\"{\\\"name\\\":\\\"langfuse-docs-assistant-text\\\",\\\"prompt\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"version\\\":10,\\\"isFallback\\\":false,\\\"tags\\\":[],\\\"labels\\\":[\\\"production\\\",\\\"latest\\\"],\\\"type\\\":\\\"text\\\",\\\"config\\\":{\\\"model\\\":\\\"gpt-5\\\",\\\"reasoningSummary\\\":\\\"detailed\\\",\\\"textVerbosity\\\":\\\"low\\\",\\\"reasoningEffort\\\":\\\"low\\\"}}\",\"ai.prompt\":\"{\\\"system\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"messages\\\":[{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"**üëã Do you have any questions about Langfuse? Ask me!**\\\\n\\\\n_‚ö†Ô∏è Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\\\"}]},{\\\"role\\\":\\\"user\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"How to use langfuse for N+1 evaluations? \\\"}]}]}\",\"ai.response.finishReason\":\"stop\",\"ai.response.text\":\"# N+1 evaluations in Langfuse\\n\\nDo you want to compare N model/prompt variants on the same dataset (A/B/N), each scored by one or more evaluators? And which SDK are you using (Python or JS/TS)?\\n\\n## Pattern: A/B/N (N variants) on one dataset\\nRun one experiment per variant on the same dataset, attach identical evaluators, then compare runs in the UI.\\n\\n_Prerequisites_\\n- Create/choose a dataset: see Datasets ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\\n\\n### Python (minimal)\\n_python_example.py_\\n```python\\nfrom langfuse import get_client, Evaluation\\n\\nlf = get_client()\\ndataset = lf.get_dataset(\\\"my-eval-dataset\\\")\\n\\ndef task(*, item, variant, **_):\\n    # call your app/model with the variant (prompt/model/params)\\n    # return plain output string/object\\n    return f\\\"answer by {variant} to: {item['input']}\\\"\\n\\ndef exact_match(*, input, output, expected_output, **_):\\n    v = 1.0 if expected_output and str(expected_output).lower() in str(output).lower() else 0.0\\n    return Evaluation(name=\\\"accuracy\\\", value=v)\\n\\nfor variant in [\\\"prompt_v1\\\", \\\"prompt_v2\\\", \\\"prompt_v3\\\"]:\\n    result = dataset.run_experiment(\\n        name=f\\\"abn-{variant}\\\",\\n        description=\\\"A/B/N on same dataset\\\",\\n        task=lambda *, item, **kw: task(item=item, variant=variant),\\n        evaluators=[exact_match],\\n        metadata={\\\"variant\\\": variant},\\n    )\\n    print(result.format())\\n```\\n\\n### JS/TS (minimal)\\n_abn.ts_\\n```ts\\nimport { LangfuseClient, ExperimentItem } from \\\"@langfuse/client\\\";\\n\\nconst lf = new LangfuseClient();\\nconst dataset = await lf.dataset.get(\\\"my-eval-dataset\\\");\\n\\nconst task = async (item: ExperimentItem, variant: string) => {\\n  return `answer by ${variant} to: ${item.input}`;\\n};\\n\\nconst accuracy = async ({ output, expectedOutput }: any) => ({\\n  name: \\\"accuracy\\\",\\n  value: expectedOutput && String(output).toLowerCase()\\n    .includes(String(expectedOutput).toLowerCase()) ? 1.0 : 0.0,\\n});\\n\\nfor (const variant of [\\\"prompt_v1\\\", \\\"prompt_v2\\\", \\\"prompt_v3\\\"]) {\\n  const res = await dataset.runExperiment({\\n    name: `abn-${variant}`,\\n    description: \\\"A/B/N on same dataset\\\",\\n    task: (item) => task(item, variant),\\n    evaluators: [accuracy],\\n    metadata: { variant },\\n  });\\n  console.log(await res.format());\\n}\\n```\\n\\n- This creates N dataset runs that are directly comparable. See Remote Dataset Runs ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\\n\\n## Add an ‚Äú+1‚Äù judge\\nAttach one or more LLM-as-a-judge or custom evaluators to all runs.\\n\\n- Python evaluator example: [Evaluations (Python)](https://langfuse.com/docs/observability/sdk/python/evaluation)\\n- JS Autoevals integration: [Remote Runs ‚Üí Autoevals](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)\\n\\nJS Autoevals (optional):\\n```ts\\nimport { Factuality } from \\\"autoevals\\\";\\nimport { createEvaluatorFromAutoevals } from \\\"@langfuse/client\\\";\\nconst factuality = createEvaluatorFromAutoevals(Factuality());\\n/* pass evaluators: [factuality, ...] in runExperiment */\\n```\\n\\n## Notes\\n- Running multiple predictions per item in a single run is not the recommended path; prefer one dataset run per variant and compare runs in the UI. The SDKs are built to ‚Äúrun multiple experiments on the same dataset in parallel‚Äù and attach the same evaluators for apples-to-apples comparison ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\\n\\n## Links\\n- Datasets: create and populate ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\\n- Remote Dataset Runs and evaluators ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run))\\n- Python evaluations API ([docs](https://langfuse.com/docs/observability/sdk/python/evaluation))\",\"ai.response.providerMetadata\":\"{\\\"openai\\\":{\\\"responseId\\\":\\\"resp_68ccfd6dc2108194813f993e99c0af9805b0adca4249e773\\\",\\\"logprobs\\\":[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]}}\",\"ai.usage.inputTokens\":\"47743\",\"ai.usage.outputTokens\":\"1655\",\"ai.usage.totalTokens\":\"49398\",\"ai.usage.reasoningTokens\":\"640\",\"ai.usage.cachedInputTokens\":\"1024\"},\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"ai\",\"attributes\":{}}}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "input": "{\"system\":\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\n\\n## Guidelines\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\n- Answer the question using only this and only this information, outputted in markdown format.\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\n\\n## Scope & Safety\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\n\\n## Clarifying Questions\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\n\\n## Pricing & Discounts\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\n\\n## Markdown Style Guide\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\",\"messages\":[{\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"**üëã Do you have any questions about Langfuse? Ask me!**\\n\\n_‚ö†Ô∏è Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\"}]},{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"How to use langfuse for N+1 evaluations? \"}]}]}",
      "output": "# N+1 evaluations in Langfuse\n\nDo you want to compare N model/prompt variants on the same dataset (A/B/N), each scored by one or more evaluators? And which SDK are you using (Python or JS/TS)?\n\n## Pattern: A/B/N (N variants) on one dataset\nRun one experiment per variant on the same dataset, attach identical evaluators, then compare runs in the UI.\n\n_Prerequisites_\n- Create/choose a dataset: see Datasets ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\n\n### Python (minimal)\n_python_example.py_\n```python\nfrom langfuse import get_client, Evaluation\n\nlf = get_client()\ndataset = lf.get_dataset(\"my-eval-dataset\")\n\ndef task(*, item, variant, **_):\n    # call your app/model with the variant (prompt/model/params)\n    # return plain output string/object\n    return f\"answer by {variant} to: {item['input']}\"\n\ndef exact_match(*, input, output, expected_output, **_):\n    v = 1.0 if expected_output and str(expected_output).lower() in str(output).lower() else 0.0\n    return Evaluation(name=\"accuracy\", value=v)\n\nfor variant in [\"prompt_v1\", \"prompt_v2\", \"prompt_v3\"]:\n    result = dataset.run_experiment(\n        name=f\"abn-{variant}\",\n        description=\"A/B/N on same dataset\",\n        task=lambda *, item, **kw: task(item=item, variant=variant),\n        evaluators=[exact_match],\n        metadata={\"variant\": variant},\n    )\n    print(result.format())\n```\n\n### JS/TS (minimal)\n_abn.ts_\n```ts\nimport { LangfuseClient, ExperimentItem } from \"@langfuse/client\";\n\nconst lf = new LangfuseClient();\nconst dataset = await lf.dataset.get(\"my-eval-dataset\");\n\nconst task = async (item: ExperimentItem, variant: string) => {\n  return `answer by ${variant} to: ${item.input}`;\n};\n\nconst accuracy = async ({ output, expectedOutput }: any) => ({\n  name: \"accuracy\",\n  value: expectedOutput && String(output).toLowerCase()\n    .includes(String(expectedOutput).toLowerCase()) ? 1.0 : 0.0,\n});\n\nfor (const variant of [\"prompt_v1\", \"prompt_v2\", \"prompt_v3\"]) {\n  const res = await dataset.runExperiment({\n    name: `abn-${variant}`,\n    description: \"A/B/N on same dataset\",\n    task: (item) => task(item, variant),\n    evaluators: [accuracy],\n    metadata: { variant },\n  });\n  console.log(await res.format());\n}\n```\n\n- This creates N dataset runs that are directly comparable. See Remote Dataset Runs ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\n\n## Add an ‚Äú+1‚Äù judge\nAttach one or more LLM-as-a-judge or custom evaluators to all runs.\n\n- Python evaluator example: [Evaluations (Python)](https://langfuse.com/docs/observability/sdk/python/evaluation)\n- JS Autoevals integration: [Remote Runs ‚Üí Autoevals](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)\n\nJS Autoevals (optional):\n```ts\nimport { Factuality } from \"autoevals\";\nimport { createEvaluatorFromAutoevals } from \"@langfuse/client\";\nconst factuality = createEvaluatorFromAutoevals(Factuality());\n/* pass evaluators: [factuality, ...] in runExperiment */\n```\n\n## Notes\n- Running multiple predictions per item in a single run is not the recommended path; prefer one dataset run per variant and compare runs in the UI. The SDKs are built to ‚Äúrun multiple experiments on the same dataset in parallel‚Äù and attach the same evaluators for apples-to-apples comparison ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\n\n## Links\n- Datasets: create and populate ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\n- Remote Dataset Runs and evaluators ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run))\n- Python evaluations API ([docs](https://langfuse.com/docs/observability/sdk/python/evaluation))",
      "modelParameters": {
        "maxSteps": null,
        "toolChoice": null,
        "maxTokens": null,
        "finishReason": null,
        "system": "openai.responses",
        "maxRetries": "2",
        "mode": null,
        "temperature": null
      },
      "completionStartTime": null,
      "promptId": "5422a9e9-19cf-4ba8-bbd4-2fe0108eefc5",
      "createdAt": "2025-09-19T06:52:04.200Z",
      "updatedAt": "2025-09-19T06:52:04.204Z",
      "usageDetails": {},
      "costDetails": {},
      "providedCostDetails": {},
      "model": null,
      "internalModelId": null,
      "promptName": "langfuse-docs-assistant-text",
      "promptVersion": 10,
      "latency": 48286,
      "timeToFirstToken": null,
      "inputCost": null,
      "outputCost": null,
      "totalCost": 0,
      "inputUsage": 0,
      "outputUsage": 0,
      "totalUsage": 0,
      "internalModel": null
    },
    {
      "id": "fe102fec4ed78754",
      "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "GENERATION",
      "environment": "default",
      "parentObservationId": "acf76ba316194a49",
      "startTime": "2025-09-19T06:51:25.036Z",
      "endTime": "2025-09-19T06:52:04.056Z",
      "name": "ai.streamText.doStream",
      "metadata": "{}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "modelParameters": {
        "maxSteps": null,
        "toolChoice": "{\"type\":\"auto\"}",
        "maxTokens": null,
        "finishReason": "stop",
        "system": "openai.responses",
        "maxRetries": "2",
        "mode": null,
        "temperature": null
      },
      "completionStartTime": "2025-09-19T06:51:26.084Z",
      "promptId": "5422a9e9-19cf-4ba8-bbd4-2fe0108eefc5",
      "createdAt": "2025-09-19T06:52:04.200Z",
      "updatedAt": "2025-09-19T06:52:04.202Z",
      "usageDetails": {
        "input": 46619,
        "output": 1576,
        "input_cached_tokens": 0,
        "total": 48195
      },
      "costDetails": {
        "input": 0.05827375,
        "output": 0.01576,
        "input_cached_tokens": 0,
        "total": 0.07403375
      },
      "providedCostDetails": {},
      "model": "gpt-5",
      "internalModelId": "38c3822a-09a3-457b-b200-2c6f17f7cf2f",
      "promptName": "langfuse-docs-assistant-text",
      "promptVersion": 10,
      "latency": 39020,
      "timeToFirstToken": 1.048,
      "inputCost": 0.05827375,
      "outputCost": 0.01576,
      "totalCost": 0.07403375,
      "inputUsage": 46619,
      "outputUsage": 1576,
      "totalUsage": 48195
    },
    {
      "id": "2126822ce3db7c07",
      "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "GENERATION",
      "environment": "default",
      "parentObservationId": null,
      "startTime": "2025-09-19T06:51:14.794Z",
      "endTime": "2025-09-19T06:52:04.062Z",
      "name": "handle-chatbot-message",
      "metadata": "{\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"langfuse-sdk\",\"version\":\"4.0.0\",\"attributes\":{}}}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "input": "How to use langfuse for N+1 evaluations? ",
      "output": "# N+1 evaluations in Langfuse\n\nDo you want to compare N model/prompt variants on the same dataset (A/B/N), each scored by one or more evaluators? And which SDK are you using (Python or JS/TS)?\n\n## Pattern: A/B/N (N variants) on one dataset\nRun one experiment per variant on the same dataset, attach identical evaluators, then compare runs in the UI.\n\n_Prerequisites_\n- Create/choose a dataset: see Datasets ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\n\n### Python (minimal)\n_python_example.py_\n```python\nfrom langfuse import get_client, Evaluation\n\nlf = get_client()\ndataset = lf.get_dataset(\"my-eval-dataset\")\n\ndef task(*, item, variant, **_):\n    # call your app/model with the variant (prompt/model/params)\n    # return plain output string/object\n    return f\"answer by {variant} to: {item['input']}\"\n\ndef exact_match(*, input, output, expected_output, **_):\n    v = 1.0 if expected_output and str(expected_output).lower() in str(output).lower() else 0.0\n    return Evaluation(name=\"accuracy\", value=v)\n\nfor variant in [\"prompt_v1\", \"prompt_v2\", \"prompt_v3\"]:\n    result = dataset.run_experiment(\n        name=f\"abn-{variant}\",\n        description=\"A/B/N on same dataset\",\n        task=lambda *, item, **kw: task(item=item, variant=variant),\n        evaluators=[exact_match],\n        metadata={\"variant\": variant},\n    )\n    print(result.format())\n```\n\n### JS/TS (minimal)\n_abn.ts_\n```ts\nimport { LangfuseClient, ExperimentItem } from \"@langfuse/client\";\n\nconst lf = new LangfuseClient();\nconst dataset = await lf.dataset.get(\"my-eval-dataset\");\n\nconst task = async (item: ExperimentItem, variant: string) => {\n  return `answer by ${variant} to: ${item.input}`;\n};\n\nconst accuracy = async ({ output, expectedOutput }: any) => ({\n  name: \"accuracy\",\n  value: expectedOutput && String(output).toLowerCase()\n    .includes(String(expectedOutput).toLowerCase()) ? 1.0 : 0.0,\n});\n\nfor (const variant of [\"prompt_v1\", \"prompt_v2\", \"prompt_v3\"]) {\n  const res = await dataset.runExperiment({\n    name: `abn-${variant}`,\n    description: \"A/B/N on same dataset\",\n    task: (item) => task(item, variant),\n    evaluators: [accuracy],\n    metadata: { variant },\n  });\n  console.log(await res.format());\n}\n```\n\n- This creates N dataset runs that are directly comparable. See Remote Dataset Runs ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\n\n## Add an ‚Äú+1‚Äù judge\nAttach one or more LLM-as-a-judge or custom evaluators to all runs.\n\n- Python evaluator example: [Evaluations (Python)](https://langfuse.com/docs/observability/sdk/python/evaluation)\n- JS Autoevals integration: [Remote Runs ‚Üí Autoevals](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)\n\nJS Autoevals (optional):\n```ts\nimport { Factuality } from \"autoevals\";\nimport { createEvaluatorFromAutoevals } from \"@langfuse/client\";\nconst factuality = createEvaluatorFromAutoevals(Factuality());\n/* pass evaluators: [factuality, ...] in runExperiment */\n```\n\n## Notes\n- Running multiple predictions per item in a single run is not the recommended path; prefer one dataset run per variant and compare runs in the UI. The SDKs are built to ‚Äúrun multiple experiments on the same dataset in parallel‚Äù and attach the same evaluators for apples-to-apples comparison ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run)).\n\n## Links\n- Datasets: create and populate ([docs](https://langfuse.com/docs/evaluation/dataset-runs/datasets))\n- Remote Dataset Runs and evaluators ([docs](https://langfuse.com/docs/evaluation/dataset-runs/remote-run))\n- Python evaluations API ([docs](https://langfuse.com/docs/observability/sdk/python/evaluation))",
      "modelParameters": {},
      "completionStartTime": null,
      "promptId": null,
      "createdAt": "2025-09-19T06:52:04.200Z",
      "updatedAt": "2025-09-19T06:52:04.200Z",
      "usageDetails": {},
      "costDetails": {},
      "providedCostDetails": {},
      "model": null,
      "internalModelId": null,
      "promptName": null,
      "promptVersion": null,
      "latency": 49268,
      "timeToFirstToken": null,
      "inputCost": null,
      "outputCost": null,
      "totalCost": 0,
      "inputUsage": 0,
      "outputUsage": 0,
      "totalUsage": 0,
      "internalModel": null
    },
    {
      "id": "741b099d7586a62f",
      "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "GENERATION",
      "environment": "default",
      "parentObservationId": "acf76ba316194a49",
      "startTime": "2025-09-19T06:51:15.780Z",
      "endTime": "2025-09-19T06:51:25.033Z",
      "name": "ai.streamText.doStream",
      "metadata": "{\"tools\":[\"{\\\"type\\\":\\\"function\\\",\\\"name\\\":\\\"searchLangfuseDocs\\\",\\\"description\\\":\\\"Semantic search (RAG) over the Langfuse documentation. Use this whenever the user asks a broader question that cannot be answered by a specific single page. Returns a concise answer synthesized from relevant docs. The raw provider response is included in _meta. Prefer this before guessing. If a specific page is needed call getLangfuseDocsPage first.\\\",\\\"inputSchema\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The user‚Äôs question in natural language. Include helpful context like SDK/language (e.g., Python v3, JS v4), self-hosted vs cloud, and short error messages (trim long stack traces). Keep under ~600 characters.\\\"}},\\\"required\\\":[\\\"query\\\"],\\\"additionalProperties\\\":false,\\\"$schema\\\":\\\"http://json-schema.org/draft-07/schema#\\\"}}\",\"{\\\"type\\\":\\\"function\\\",\\\"name\\\":\\\"getLangfuseDocsPage\\\",\\\"description\\\":\\\"Fetch the raw Markdown for a single Langfuse docs page. Accepts a docs path (e.g., /docs/observability/overview) or a full https://langfuse.com URL. Returns the exact Markdown (may include front matter). Use when you need a specific page content (Integration, Features, API, etc.) or code samples. Prefer searchLangfuseDocs for broader questions where there is not one specific page about it.\\\",\\\"inputSchema\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"pathOrUrl\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"Docs path starting with ‚Äú/‚Äù (e.g., /docs/observability/overview) or a full URL on https://langfuse.com. Do not include anchors (#...) or queries (?foo=bar) ‚Äî they will be ignored.\\\"}},\\\"required\\\":[\\\"pathOrUrl\\\"],\\\"additionalProperties\\\":false,\\\"$schema\\\":\\\"http://json-schema.org/draft-07/schema#\\\"}}\",\"{\\\"type\\\":\\\"function\\\",\\\"name\\\":\\\"getLangfuseOverview\\\",\\\"description\\\":\\\"Get a high-level, machine-readable index by downloading https://langfuse.com/llms.txt. Use this at the start of a session when needed to discover key docs endpoints or to seed follow-up calls to searchLangfuseDocs or getLangfuseDocsPage.Returns the plain text contents of llms.txt. Avoid repeated calls within the same session.\\\",\\\"inputSchema\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{},\\\"additionalProperties\\\":false,\\\"$schema\\\":\\\"http://json-schema.org/draft-07/schema#\\\"}}\"],\"attributes\":{\"operation.name\":\"ai.streamText.doStream\",\"ai.operationId\":\"ai.streamText.doStream\",\"ai.model.provider\":\"openai.responses\",\"ai.model.id\":\"gpt-5\",\"ai.settings.maxRetries\":\"2\",\"ai.telemetry.metadata.langfusePrompt\":\"{\\\"name\\\":\\\"langfuse-docs-assistant-text\\\",\\\"prompt\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"version\\\":10,\\\"isFallback\\\":false,\\\"tags\\\":[],\\\"labels\\\":[\\\"production\\\",\\\"latest\\\"],\\\"type\\\":\\\"text\\\",\\\"config\\\":{\\\"model\\\":\\\"gpt-5\\\",\\\"reasoningSummary\\\":\\\"detailed\\\",\\\"textVerbosity\\\":\\\"low\\\",\\\"reasoningEffort\\\":\\\"low\\\"}}\",\"ai.prompt.messages\":\"[{\\\"role\\\":\\\"system\\\",\\\"content\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\\\"},{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"**üëã Do you have any questions about Langfuse? Ask me!**\\\\n\\\\n_‚ö†Ô∏è Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\\\"}]},{\\\"role\\\":\\\"user\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"How to use langfuse for N+1 evaluations? \\\"}]}]\",\"ai.prompt.tools\":\"[\\\"{\\\\\\\"type\\\\\\\":\\\\\\\"function\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"searchLangfuseDocs\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Semantic search (RAG) over the Langfuse documentation. Use this whenever the user asks a broader question that cannot be answered by a specific single page. Returns a concise answer synthesized from relevant docs. The raw provider response is included in _meta. Prefer this before guessing. If a specific page is needed call getLangfuseDocsPage first.\\\\\\\",\\\\\\\"inputSchema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"query\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The user‚Äôs question in natural language. Include helpful context like SDK/language (e.g., Python v3, JS v4), self-hosted vs cloud, and short error messages (trim long stack traces). Keep under ~600 characters.\\\\\\\"}},\\\\\\\"required\\\\\\\":[\\\\\\\"query\\\\\\\"],\\\\\\\"additionalProperties\\\\\\\":false,\\\\\\\"$schema\\\\\\\":\\\\\\\"http://json-schema.org/draft-07/schema#\\\\\\\"}}\\\",\\\"{\\\\\\\"type\\\\\\\":\\\\\\\"function\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"getLangfuseDocsPage\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Fetch the raw Markdown for a single Langfuse docs page. Accepts a docs path (e.g., /docs/observability/overview) or a full https://langfuse.com URL. Returns the exact Markdown (may include front matter). Use when you need a specific page content (Integration, Features, API, etc.) or code samples. Prefer searchLangfuseDocs for broader questions where there is not one specific page about it.\\\\\\\",\\\\\\\"inputSchema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"pathOrUrl\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Docs path starting with ‚Äú/‚Äù (e.g., /docs/observability/overview) or a full URL on https://langfuse.com. Do not include anchors (#...) or queries (?foo=bar) ‚Äî they will be ignored.\\\\\\\"}},\\\\\\\"required\\\\\\\":[\\\\\\\"pathOrUrl\\\\\\\"],\\\\\\\"additionalProperties\\\\\\\":false,\\\\\\\"$schema\\\\\\\":\\\\\\\"http://json-schema.org/draft-07/schema#\\\\\\\"}}\\\",\\\"{\\\\\\\"type\\\\\\\":\\\\\\\"function\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"getLangfuseOverview\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Get a high-level, machine-readable index by downloading https://langfuse.com/llms.txt. Use this at the start of a session when needed to discover key docs endpoints or to seed follow-up calls to searchLangfuseDocs or getLangfuseDocsPage.Returns the plain text contents of llms.txt. Avoid repeated calls within the same session.\\\\\\\",\\\\\\\"inputSchema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{},\\\\\\\"additionalProperties\\\\\\\":false,\\\\\\\"$schema\\\\\\\":\\\\\\\"http://json-schema.org/draft-07/schema#\\\\\\\"}}\\\"]\",\"ai.prompt.toolChoice\":\"{\\\"type\\\":\\\"auto\\\"}\",\"gen_ai.system\":\"openai.responses\",\"gen_ai.request.model\":\"gpt-5\",\"ai.response.msToFirstChunk\":\"773.1660110000521\",\"ai.response.msToFinish\":\"9247.992744000047\",\"ai.response.avgOutputTokensPerSecond\":\"8.542394245632812\",\"ai.response.finishReason\":\"tool-calls\",\"ai.response.text\":\"I‚Äôll look up the Langfuse docs for ‚ÄúN+1 evaluations‚Äù to share the recommended approach and a minimal example.\",\"ai.response.toolCalls\":\"[{\\\"type\\\":\\\"tool-call\\\",\\\"toolCallId\\\":\\\"call_AOk8le1i4tNgzuTtytNcqD5Q\\\",\\\"toolName\\\":\\\"searchLangfuseDocs\\\",\\\"input\\\":{\\\"query\\\":\\\"N+1 evaluations Langfuse how to implement n+1 evaluation pattern batch eval per prediction vs per dataset Python JS\\\"},\\\"providerMetadata\\\":{\\\"openai\\\":{\\\"itemId\\\":\\\"fc_68ccfd67f814819498c9c64fbb7e948c05b0adca4249e773\\\"}},\\\"dynamic\\\":true}]\",\"ai.response.id\":\"resp_68ccfd645cb08194a7464f25c6b45fe705b0adca4249e773\",\"ai.response.model\":\"gpt-5-2025-08-07\",\"ai.response.timestamp\":\"2025-09-19T06:51:16.000Z\",\"ai.response.providerMetadata\":\"{\\\"openai\\\":{\\\"responseId\\\":\\\"resp_68ccfd645cb08194a7464f25c6b45fe705b0adca4249e773\\\",\\\"logprobs\\\":[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]}}\",\"ai.usage.inputTokens\":\"1124\",\"ai.usage.outputTokens\":\"79\",\"ai.usage.totalTokens\":\"1203\",\"ai.usage.reasoningTokens\":\"{\\\"intValue\\\":0}\",\"ai.usage.cachedInputTokens\":\"1024\",\"gen_ai.response.finish_reasons\":\"[\\\"tool-calls\\\"]\",\"gen_ai.response.id\":\"resp_68ccfd645cb08194a7464f25c6b45fe705b0adca4249e773\",\"gen_ai.response.model\":\"gpt-5-2025-08-07\",\"gen_ai.usage.input_tokens\":\"1124\",\"gen_ai.usage.output_tokens\":\"79\"},\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"ai\",\"attributes\":{}}}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "input": "[{\"role\":\"system\",\"content\":\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\n\\n## Guidelines\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\n- Answer the question using only this and only this information, outputted in markdown format.\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\n\\n## Scope & Safety\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\n\\n## Clarifying Questions\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\n\\n## Pricing & Discounts\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\n\\n## Markdown Style Guide\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\"},{\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"**üëã Do you have any questions about Langfuse? Ask me!**\\n\\n_‚ö†Ô∏è Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\"}]},{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"How to use langfuse for N+1 evaluations? \"}]}]",
      "output": "I‚Äôll look up the Langfuse docs for ‚ÄúN+1 evaluations‚Äù to share the recommended approach and a minimal example.",
      "modelParameters": {
        "maxSteps": null,
        "toolChoice": "{\"type\":\"auto\"}",
        "maxTokens": null,
        "finishReason": "tool-calls",
        "system": "openai.responses",
        "maxRetries": "2",
        "mode": null,
        "temperature": null
      },
      "completionStartTime": "2025-09-19T06:51:16.554Z",
      "promptId": "5422a9e9-19cf-4ba8-bbd4-2fe0108eefc5",
      "createdAt": "2025-09-19T06:51:30.170Z",
      "updatedAt": "2025-09-19T06:51:30.172Z",
      "usageDetails": {
        "input": 1124,
        "output": 79,
        "output_reasoning_tokens": 0,
        "total": 1203
      },
      "costDetails": {
        "input": 0.001405,
        "output": 0.00079,
        "output_reasoning_tokens": 0,
        "total": 0.002195
      },
      "providedCostDetails": {},
      "model": "gpt-5",
      "internalModelId": "38c3822a-09a3-457b-b200-2c6f17f7cf2f",
      "promptName": "langfuse-docs-assistant-text",
      "promptVersion": 10,
      "latency": 9253,
      "timeToFirstToken": 0.774,
      "inputCost": 0.001405,
      "outputCost": 0.00079,
      "totalCost": 0.002195,
      "inputUsage": 1124,
      "outputUsage": 79,
      "totalUsage": 1203,
      "internalModel": "38c3822a-09a3-457b-b200-2c6f17f7cf2f"
    },
    {
      "id": "e704e7eaa8a70d4d",
      "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "TOOL",
      "environment": "default",
      "parentObservationId": "acf76ba316194a49",
      "startTime": "2025-09-19T06:51:20.208Z",
      "endTime": "2025-09-19T06:51:25.008Z",
      "name": "ai.toolCall",
      "metadata": "{\"attributes\":{\"operation.name\":\"ai.toolCall\",\"ai.operationId\":\"ai.toolCall\",\"ai.toolCall.name\":\"searchLangfuseDocs\",\"ai.toolCall.id\":\"call_AOk8le1i4tNgzuTtytNcqD5Q\",\"ai.toolCall.args\":\"{\\\"query\\\":\\\"N+1 evaluations Langfuse how to implement n+1 evaluation pattern batch eval per prediction vs per dataset Python JS\\\"}\",\"ai.toolCall.result\":\"{\\\"_meta\\\":{\\\"id\\\":\\\"inkeep-chatcmpl-40fc3a6d-a341-4906-ba8f-eecf1570e0e3\\\",\\\"model\\\":\\\"inkeep-rag\\\",\\\"choices\\\":[{\\\"finish_reason\\\":\\\"stop\\\",\\\"log_probs\\\":null,\\\"index\\\":0,\\\"message\\\":{\\\"content\\\":\\\"{\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Troubleshooting and FAQ\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[API - /scores not respecting value when operator '='](https://github.com/orgs/langfuse/discussions/8770 \\\\\\\\\\\\\\\"Langfuse Support: API - /scores not respecting value when operator '='\\\\\\\\\\\\\\\")\\\\\\\\n[Does LangFuse support evaluations on an existing dataset (.csv)](https://github.com/orgs/langfuse/discussions/8665 \\\\\\\\\\\\\\\"Langfuse Support: Does LangFuse support evaluations on an existing dataset \\\\\\\\\\\\\\\\(.csv\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[deleting evaluators](https://github.com/orgs/langfuse/discussions/8640 \\\\\\\\\\\\\\\"Langfuse Support: deleting evaluators\\\\\\\\\\\\\\\")\\\\\\\\n[Not able to import my LLM-as-a-Judge evals](https://github.com/orgs/langfuse/discussions/8636 \\\\\\\\\\\\\\\"Langfuse Support: Not able to import my LLM-as-a-Judge evals\\\\\\\\\\\\\\\")\\\\\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590 \\\\\\\\\\\\\\\"Langfuse Support: How to get experiment run scores programmatically?\\\\\\\\\\\\\\\")\\\\\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556 \\\\\\\\\\\\\\\"Langfuse Support: Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset runs restore from backups](https://github.com/orgs/langfuse/discussions/8534 \\\\\\\\\\\\\\\"Langfuse Support: Dataset runs restore from backups\\\\\\\\\\\\\\\")\\\\\\\\n[Getting scores efficiently via API for analytics purposes](https://github.com/orgs/langfuse/discussions/8520 \\\\\\\\\\\\\\\"Langfuse Support: Getting scores efficiently via API for analytics purposes\\\\\\\\\\\\\\\")\\\\\\\\n[run experiment on dataset](https://github.com/orgs/langfuse/discussions/8433 \\\\\\\\\\\\\\\"Langfuse Support: run experiment on dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Experiments on Datasets with Human Annotated Labels?](https://github.com/orgs/langfuse/discussions/8414 \\\\\\\\\\\\\\\"Langfuse Support: Experiments on Datasets with Human Annotated Labels?\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[How to Recalculate Total Score on Dashboard After Updating User-Defined Model?](https://github.com/orgs/langfuse/discussions/8375 \\\\\\\\\\\\\\\"Langfuse Support: How to Recalculate Total Score on Dashboard After Updating User-Defined Model?\\\\\\\\\\\\\\\")\\\\\\\\n[How to filter by Categorical Scores in custom dashboard?](https://github.com/orgs/langfuse/discussions/8356 \\\\\\\\\\\\\\\"Langfuse Support: How to filter by Categorical Scores in custom dashboard?\\\\\\\\\\\\\\\")\\\\\\\\n[Running scheduled evals utilising LangFuse Datasets & Evaluators](https://github.com/orgs/langfuse/discussions/8355 \\\\\\\\\\\\\\\"Langfuse Support: Running scheduled evals utilising LangFuse Datasets & Evaluators\\\\\\\\\\\\\\\")\\\\\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\\\\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\\\\\\\\\(\\\\\\\\\\\\\\\\)` method\\\\\\\\\\\\\\\")\\\\\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\\\\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\\\\\\\\\")\\\\\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\\\\\\\\\"Langfuse Support: \\\\\\\\\\\\\\\\[Experiment\\\\\\\\\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\\\\\\\\\")\\\\\\\\n[Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\\\\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing\\\\\\\\\\\\\\\")\\\\\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\\\\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[2 traces generated instead of 1](https://github.com/orgs/langfuse/discussions/2244 \\\\\\\\\\\\\\\"Langfuse Support: 2 traces generated instead of 1\\\\\\\\\\\\\\\")\\\\\\\\n[Evaluations Not Available in Self-Hosted Version?](https://github.com/orgs/langfuse/discussions/2130 \\\\\\\\\\\\\\\"Langfuse Support: Evaluations Not Available in Self-Hosted Version?\\\\\\\\\\\\\\\")\\\\\\\\n[Deleting Duplicate Items in a Dataset](https://github.com/orgs/langfuse/discussions/2099 \\\\\\\\\\\\\\\"Langfuse Support: Deleting Duplicate Items in a Dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Availability of evals when self-hosting](https://github.com/orgs/langfuse/discussions/2042 \\\\\\\\\\\\\\\"Langfuse Support: Availability of evals when self-hosting\\\\\\\\\\\\\\\")\\\\\\\\n[How to utilize a dataset w/ typescript and langchain integration](https://github.com/orgs/langfuse/discussions/1969 \\\\\\\\\\\\\\\"Langfuse Support: How to utilize a dataset w/ typescript and langchain integration\\\\\\\\\\\\\\\")\\\\\\\\n[Scoring a trace after the LLM chain returns](https://github.com/orgs/langfuse/discussions/1610 \\\\\\\\\\\\\\\"Langfuse Support: Scoring a trace after the LLM chain returns\\\\\\\\\\\\\\\")\\\\\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486 \\\\\\\\\\\\\\\"Langfuse Support: Update/delete score using python sdk\\\\\\\\\\\\\\\")\\\\\\\\n[Linking dataset run items with existing callback handler](https://github.com/orgs/langfuse/discussions/1445 \\\\\\\\\\\\\\\"Langfuse Support: Linking dataset run items with existing callback handler\\\\\\\\\\\\\\\")\\\\\\\\n[Datasets list / by id](https://github.com/orgs/langfuse/discussions/1420 \\\\\\\\\\\\\\\"Langfuse Support: Datasets list / by id\\\\\\\\\\\\\\\")\\\\\\\\n[Run items not appearing when linking to a trace and not a span or a generation](https://github.com/orgs/langfuse/discussions/1357 \\\\\\\\\\\\\\\"Langfuse Support: Run items not appearing when linking to a trace and not a span or a generation\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Option to run Experiments without traces](https://github.com/orgs/langfuse/discussions/8133 \\\\\\\\\\\\\\\"Langfuse Ideas: Option to run Experiments without traces\\\\\\\\\\\\\\\")\\\\\\\\n[Auto-generate dataset items](https://github.com/orgs/langfuse/discussions/8126 \\\\\\\\\\\\\\\"Langfuse Ideas: Auto-generate dataset items\\\\\\\\\\\\\\\")\\\\\\\\n[feature: support for creation of custom) model adapters](https://github.com/orgs/langfuse/discussions/8123 \\\\\\\\\\\\\\\"Langfuse Ideas: feature: support for creation of custom\\\\\\\\\\\\\\\\) model adapters\\\\\\\\\\\\\\\")\\\\\\\\n[Prompt experiment result download.](https://github.com/orgs/langfuse/discussions/8120 \\\\\\\\\\\\\\\"Langfuse Ideas: Prompt experiment result download.\\\\\\\\\\\\\\\")\\\\\\\\n[Multiple predictions per-item in a single run](https://github.com/orgs/langfuse/discussions/8040 \\\\\\\\\\\\\\\"Langfuse Ideas: Multiple predictions per-item in a single run\\\\\\\\\\\\\\\")\\\\\\\\n[Add evaluator(s) to python SDK (FernLangfuse)](https://github.com/orgs/langfuse/discussions/8018 \\\\\\\\\\\\\\\"Langfuse Ideas: Add evaluator\\\\\\\\\\\\\\\\(s\\\\\\\\\\\\\\\\) to python SDK \\\\\\\\\\\\\\\\(FernLangfuse\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[feat(dataset-runs): return all dataset run item scores in a given run](https://github.com/orgs/langfuse/discussions/8011 \\\\\\\\\\\\\\\"Langfuse Ideas: feat\\\\\\\\\\\\\\\\(dataset-runs\\\\\\\\\\\\\\\\): return all dataset run item scores in a given run\\\\\\\\\\\\\\\")\\\\\\\\n[Delete an evaluator from the evaluator library](https://github.com/orgs/langfuse/discussions/7960 \\\\\\\\\\\\\\\"Langfuse Ideas: Delete an evaluator from the evaluator library\\\\\\\\\\\\\\\")\\\\\\\\n[Archive or Delete Evaluators from the Evaluator Library](https://github.com/orgs/langfuse/discussions/7868 \\\\\\\\\\\\\\\"Langfuse Ideas: Archive or Delete Evaluators from the Evaluator Library\\\\\\\\\\\\\\\")\\\\\\\\n[Edit dataset run name, description](https://github.com/orgs/langfuse/discussions/7814 \\\\\\\\\\\\\\\"Langfuse Ideas: Edit dataset run name, description\\\\\\\\\\\\\\\")\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Datasets\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Datasets - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")Dataset RunsDatasets\\\\\\\\nCopy as Markdown\\\\\\\\n# Datasets\\\\\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\\\\\n## Why use datasets?\\\\\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\\\\\n  * Create test cases for your application with real production traces\\\\\\\\n  * Collaboratively create and collect dataset items with your team\\\\\\\\n  * Have a single source of truth for your test data\\\\\\\\n## Get Started\\\\\\\\n### Creating a dataset\\\\\\\\nDatasets have a name which is unique within a project.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset(\\\\\\\\n name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # optional description\\\\\\\\n description=\\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n # optional metadata\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"author\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"date\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { LangfuseClient } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\nconst langfuse = new LangfuseClient()\\\\\\\\n \\\\\\\\nawait langfuse.api.datasets.create({\\\\\\\\n name: \\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n // optional description\\\\\\\\n description: \\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n // optional metadata\\\\\\\\n metadata: {\\\\\\\\n author: \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n date: \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n type: \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\",\\\\\\\\n },\\\\\\\\n});\\\\\\\\n```\\\\\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\\\\\n### Create new dataset items\\\\\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset_item(\\\\\\\\n dataset_name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # any python object or value, optional\\\\\\\\n input={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # any python object or value, optional\\\\\\\\n expected_output={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # metadata, optional\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"llama3\\\\\\\\\\\\\\\",\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/observability/sdk/python/evaluation\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Evaluations\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Evaluations with the Langfuse Python SDK - Langfuse\\\\\\\\nDocs[Observability](/docs/observability/overview \\\\\\\\\\\\\\\"Observability\\\\\\\\\\\\\\\")\\\\\\\\n[SDKs](/docs/observability/sdk/overview \\\\\\\\\\\\\\\"SDKs\\\\\\\\\\\\\\\")\\\\\\\\n[Python](/docs/observability/sdk/python/overview \\\\\\\\\\\\\\\"Python\\\\\\\\\\\\\\\")Evaluation\\\\\\\\nCopy as Markdown\\\\\\\\n# Evaluations\\\\\\\\nThe Python SDK provides ways to evaluate your application. You can add custom scores to your traces and observations, or use the SDK to execute Dataset Runs.\\\\\\\\nThis page shows the evaluation methods that are supported by the Python SDK. Please refer to the [Evaluation documentation](/docs/evaluation/overview) for more information on how to evaluate your application in Langfuse.\\\\\\\\n## Create Scores\\\\\\\\nLangfuseSpan / LangfuseGeneration object methodsContext-aware methodsLow-level method\\\\\\\\n  * `span_or_generation_obj.score()`: Scores the specific observation object.\\\\\\\\n  * `span_or_generation_obj.score_trace()`: Scores the entire trace to which the object belongs.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nwith langfuse.start_as_current_generation(name=\\\\\\\\\\\\\\\"summary_generation\\\\\\\\\\\\\\\") as gen:\\\\\\\\n # ... LLM call ...\\\\\\\\n gen.update(output=\\\\\\\\\\\\\\\"summary text...\\\\\\\\\\\\\\\")\\\\\\\\n # Score this specific generation\\\\\\\\n gen.score(name=\\\\\\\\\\\\\\\"conciseness\\\\\\\\\\\\\\\", value=0.8, data_type=\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\")\\\\\\\\n # Score the overall trace\\\\\\\\n gen.score_trace(name=\\\\\\\\\\\\\\\"user_feedback_rating\\\\\\\\\\\\\\\", value=\\\\\\\\\\\\\\\"positive\\\\\\\\\\\\\\\", data_type=\\\\\\\\\\\\\\\"CATEGORICAL\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\n  * `langfuse.score_current_span()`: Scores the currently active observation in the context.\\\\\\\\n  * `langfuse.score_current_trace()`: Scores the trace of the currently active observation.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nwith langfuse.start_as_current_span(name=\\\\\\\\\\\\\\\"complex_task\\\\\\\\\\\\\\\") as task_span:\\\\\\\\n # ... perform task ...\\\\\\\\n langfuse.score_current_span(name=\\\\\\\\\\\\\\\"task_component_quality\\\\\\\\\\\\\\\", value=True, data_type=\\\\\\\\\\\\\\\"BOOLEAN\\\\\\\\\\\\\\\")\\\\\\\\n # ...\\\\\\\\n if task_is_fully_successful:\\\\\\\\n langfuse.score_current_trace(name=\\\\\\\\\\\\\\\"overall_success\\\\\\\\\\\\\\\", value=1.0, data_type=\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n  * Creates a score for a specified `trace_id` and optionally `observation_id`.\\\\\\\\n  * Useful when IDs are known, or for scoring after the trace/observation has completed.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nlangfuse.create_score(\\\\\\\\n name=\\\\\\\\\\\\\\\"fact_check_accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value=0.95, ///# Can be float for NUMERIC/BOOLEAN, string for CATEGORICAL\\\\\\\\n trace_id=\\\\\\\\\\\\\\\"abcdef1234567890abcdef1234567890\\\\\\\\\\\\\\\",\\\\\\\\n observation_id=\\\\\\\\\\\\\\\"1234567890abcdef\\\\\\\\\\\\\\\", ///# Optional: if scoring a specific observation\\\\\\\\n session_id=\\\\\\\\\\\\\\\"session_123\\\\\\\\\\\\\\\", ///# Optional: if scoring a specific session\\\\\\\\n data_type=\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\", ///# \\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"BOOLEAN\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"CATEGORICAL\\\\\\\\\\\\\\\"\\\\\\\\n comment=\\\\\\\\\\\\\\\"Source verified for 95% of claims.\\\\\\\\\\\\\\\"\\\\\\\\n)\\\\\\\\n```\\\\\\\\n**Score Parameters:**\\\\\\\\nParameter| Type| Description  \\\\\\\\n---|---|---  \\\\\\\\n`name`| `str`| Name of the score (e.g., \\\\\\\\u201crelevance\\\\\\\\u201d, \\\\\\\\u201caccuracy\\\\\\\\u201d). **Required.**  \\\\\\\\n`value`| `Union[float, str]`| Score value. Float for `NUMERIC`/`BOOLEAN`, string for `CATEGORICAL`. **Required.**  \\\\\\\\n`trace_id`| `str`| ID of the trace to associate with (for `create_score`). **Required.**  \\\\\\\\n`observation_id`| `Optional[str]`| ID of the specific observation to score (for `create_score`).  \\\\\\\\n`session_id`| `Optional[str]`| ID of the specific session to score (for `create_score`).  \\\\\\\\n`score_id`| `Optional[str]`| Custom ID for the score (auto-generated if None).  \\\\\\\\n`data_type`| `Optional[ScoreDataType]`| `\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\"`, `\\\\\\\\\\\\\\\"BOOLEAN\\\\\\\\\\\\\\\"`, or `\\\\\\\\\\\\\\\"CATEGORICAL\\\\\\\\\\\\\\\"`. Inferred if not provided based on value type and score config on server.  \\\\\\\\n`comment`| `Optional[str]`| Optional comment or explanation for the score.  \\\\\\\\n`config_id`| `Optional[str]`| Optional ID of a pre-defined score configuration in Langfuse.  \\\\\\\\n  \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThe most powerful way to use datasets is by linking your application\\\\\\\\u2019s executions (traces) to specific dataset items when performing an evaluation run. The `DatasetItemClient.run()` method provides a context manager to streamline this process.\\\\\\\\n**How`item.run()` works:**\\\\\\\\nWhen you use `with item.run(run_name=\\\\\\\\\\\\\\\"your_eval_run_name\\\\\\\\\\\\\\\") as root_span:`:\\\\\\\\n  1. **Trace Creation** : A new Langfuse trace is initiated specifically for processing this dataset item within the context of the named run.\\\\\\\\n  2. **Trace Naming & Metadata**: \\\\\\\\n    * The trace is automatically named (e.g., \\\\\\\\u201cDataset run: your_eval_run_name\\\\\\\\u201d).\\\\\\\\n    * Essential metadata is added to this trace, including `dataset_item_id` (the ID of `item`), `run_name`, and `dataset_id`.\\\\\\\\n  3. **DatasetRunItem Linking** : The SDK makes an API call to Langfuse to create a `DatasetRunItem`. This backend object formally links: \\\\\\\\n    * The `dataset_item_id`\\\\\\\\n    * The `trace_id` of the newly created trace\\\\\\\\n    * The provided `run_name`\\\\\\\\n    * Any `run_metadata` or `run_description` you pass to `item.run()`. This linkage is what populates the \\\\\\\\u201cRuns\\\\\\\\u201d tab for your dataset in the Langfuse UI, allowing you to see all traces associated with a particular evaluation run.\\\\\\\\n  4. **Contextual Span** : The context manager yields `root_span`, which is a `LangfuseSpan` object representing the root span of this new trace.\\\\\\\\n  5. **Automatic Nesting** : Any Langfuse observations (spans or generations) created _inside_ the `with` block will automatically become children of `root_span` and thus part of the trace linked to this dataset item and run.\\\\\\\\n**Example:**\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\ndataset_name = \\\\\\\\\\\\\\\"qna-eval\\\\\\\\\\\\\\\"\\\\\\\\ncurrent_run_name = \\\\\\\\\\\\\\\"qna_model_v3_run_05_20\\\\\\\\\\\\\\\" # Identifies this specific evaluation run\\\\\\\\n \\\\\\\\n///# Assume 'my_qna_app' is your instrumented application function\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"def my_qna_app(question: str, context: str, item_id: str, run_name: str):\\\\\\\\n with langfuse.start_as_current_generation(\\\\\\\\n name=\\\\\\\\\\\\\\\"qna-llm-call\\\\\\\\\\\\\\\",\\\\\\\\n input={\\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\": question, \\\\\\\\\\\\\\\"context\\\\\\\\\\\\\\\": context},\\\\\\\\n metadata={\\\\\\\\\\\\\\\"item_id\\\\\\\\\\\\\\\": item_id, \\\\\\\\\\\\\\\"run\\\\\\\\\\\\\\\": run_name}, ///# Example metadata for the generation\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\"\\\\\\\\n ) as generation:\\\\\\\\n # Simulate LLM call\\\\\\\\n answer = f\\\\\\\\\\\\\\\"Answer to '{question}' using context.\\\\\\\\\\\\\\\" # Replace with actual LLM call\\\\\\\\n generation.update(output={\\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer})\\\\\\\\n \\\\\\\\n # Update the trace with the input and output\\\\\\\\n generation.update_trace(\\\\\\\\n input={\\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\": question, \\\\\\\\\\\\\\\"context\\\\\\\\\\\\\\\": context},\\\\\\\\n output={\\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer},\\\\\\\\n )\\\\\\\\n \\\\\\\\n return answer\\\\\\\\n \\\\\\\\ndataset = langfuse.get_dataset(name=dataset_name) ///# Fetch your pre-populated dataset\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n print(f\\\\\\\\\\\\\\\"Running evaluation for item: {item.id} (Input: {item.input})\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # Use the item.run() context manager\\\\\\\\n with item.run(\\\\\\\\n run_name=current_run_name,\\\\\\\\n run_metadata={\\\\\\\\\\\\\\\"model_provider\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"OpenAI\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"temperature_setting\\\\\\\\\\\\\\\": 0.7},\\\\\\\\n run_description=\\\\\\\\\\\\\\\"Evaluation run for Q&A model v3 on May 20th\\\\\\\\\\\\\\\"\\\\\\\\n ) as root_span: ///# root_span is the root span of the new trace for this item and run.\\\\\\\\n # All subsequent langfuse operations within this block are part of this trace.\\\\\\\\n \\\\\\\\n # Call your application logic\\\\\\\\n generated_answer = my_qna_app(\\\\\\\\n question=item.input[\\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\"],\\\\\\\\n context=item.input[\\\\\\\\\\\\\\\"context\\\\\\\\\\\\\\\"],\\\\\\\\n item_id=item.id,\\\\\\\\n run_name=current_run_name\\\\\\\\n )\\\\\\\\n \\\\\\\\n print(f\\\\\\\\\\\\\\\" Item {item.id} processed. Trace ID: {root_span.trace_id}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # Optionally, score the result against the expected output\\\\\\\\n if item.expected_output and generated_answer == item.expected_output.get(\\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\"):\\\\\\\\n root_span.score_trace(name=\\\\\\\\\\\\\\\"exact_match\\\\\\\\\\\\\\\", value=1.0)\\\\\\\\n else:\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n root_span.score_trace(name=\\\\\\\\\\\\\\\"exact_match\\\\\\\\\\\\\\\", value=0.0)\\\\\\\\n \\\\\\\\nprint(f\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\nFinished processing dataset '{dataset_name}' for run '{current_run_name}'.\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nBy using `item.run()`, you ensure each dataset item\\\\\\\\u2019s processing is neatly encapsulated in its own trace, and these traces are aggregated under the specified `run_name` in the Langfuse UI. This allows for systematic review of results, comparison across runs, and deep dives into individual processing traces.\\\\\\\\nLast updated on August 20, 2025\\\\\\\\n[Instrumentation](/docs/observability/sdk/python/instrumentation \\\\\\\\\\\\\\\"Instrumentation\\\\\\\\\\\\\\\")\\\\\\\\n[Advanced usage](/docs/observability/sdk/python/advanced-usage \\\\\\\\\\\\\\\"Advanced usage\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"All new Datasets, Experimentation and Evaluation documentation\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"## New Datasets and Evaluation documentation\\\\\\\\nBased on this week\\\\\\\\u2019s changes, we have have restructured the documentation to:\\\\\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\\\\\n### Evaluation\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Evaluation documentation](/docs/scores)\\\\\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nPOST /api/public/scores\\\\\\\\n```\\\\\\\\nAdd scores via Python or JS SDK.\\\\\\\\nExample (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.score(\\\\\\\\n trace_id=\\\\\\\\\\\\\\\"123\\\\\\\\\\\\\\\",\\\\\\\\n name=\\\\\\\\\\\\\\\"my_custom_evaluator\\\\\\\\\\\\\\\",\\\\\\\\n value=0.5,\\\\\\\\n)\\\\\\\\n```\\\\\\\\n### Datasets\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Remote Dataset Runs\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Task function that answers geography questions\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n question = item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]\\\\\\\\n response = OpenAI().chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": question}]\\\\\\\\n )\\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n if expected_output and expected_output.lower() in output.lower():\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=1.0)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=0.0)\\\\\\\\n \\\\\\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n accuracies = [\\\\\\\\n eval.value for result in item_results\\\\\\\\n for eval in result.evaluations if eval.name == \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\"\\\\\\\\n ]\\\\\\\\n \\\\\\\\n if not accuracies:\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=None)\\\\\\\\n \\\\\\\\n avg = sum(accuracies) / len(accuracies)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=avg, comment=f\\\\\\\\\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n@pytest.fixture\\\\\\\\ndef langfuse_client():\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Initialize Langfuse client for testing\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n return get_client()\\\\\\\\n \\\\\\\\ndef test_geography_accuracy_passes(langfuse_client):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Test that passes when accuracy is above threshold\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n result = langfuse_client.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Geography Test - Should Pass\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=geography_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy_evaluator]\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Access the run evaluator result directly\\\\\\\\n avg_accuracy = next(\\\\\\\\n eval.value for eval in result.run_evaluations\\\\\\\\n if eval.name == \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\"\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Assert minimum accuracy threshold\\\\\\\\n assert avg_accuracy >= 0.8, f\\\\\\\\\\\\\\\"Average accuracy {avg_accuracy:.2f} below threshold 0.8\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\ndef test_geography_accuracy_fails(langfuse_client):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Example test that demonstrates failure conditions\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n # Use a weaker model or harder questions to demonstrate test failure\\\\\\\\n def failing_task(*, item, **kwargs):\\\\\\\\n # Simulate a task that gives wrong answers\\\\\\\\n return \\\\\\\\\\\\\\\"I don't know\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n result = langfuse_client.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Geography Test - Should Fail\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=failing_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy_evaluator]\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Access the run evaluator result directly\\\\\\\\n avg_accuracy = next(\\\\\\\\n eval.value for eval in result.run_evaluations\\\\\\\\n if eval.name == \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\"\\\\\\\\n )\\\\\\\\n \\\\\\\\n # This test will fail because the task gives wrong answers\\\\\\\\n with pytest.raises(AssertionError):\\\\\\\\n assert avg_accuracy >= 0.8, f\\\\\\\\\\\\\\\"Expected test to fail with low accuracy: {avg_accuracy:.2f}\\\\\\\\\\\\\\\"\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n// test/geography-experiment.test.ts\\\\\\\\nimport { describe, it, expect, beforeAll, afterAll } from 'vitest';\\\\\\\\nimport { OpenAI } from \\\\\\\\\\\\\\\"openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { NodeSDK } from \\\\\\\\\\\\\\\"@opentelemetry/sdk-node\\\\\\\\\\\\\\\";\\\\\\\\nimport { LangfuseClient, ExperimentItem } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\nimport { observeOpenAI } from \\\\\\\\\\\\\\\"@langfuse/openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { LangfuseSpanProcessor } from \\\\\\\\\\\\\\\"@langfuse/otel\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Test data for European capitals\\\\\\\\nconst testData: ExperimentItem[] = [\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\", expected: \\\\\\\\\\\\\\\"Paris\\\\\\\\\\\\\\\" },\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\", expected: \\\\\\\\\\\\\\\"Berlin\\\\\\\\\\\\\\\" },\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of Spain?\\\\\\\\\\\\\\\", expected: \\\\\\\\\\\\\\\"Madrid\\\\\\\\\\\\\\\" },\\\\\\\\n];\\\\\\\\n \\\\\\\\nlet otelSdk: NodeSDK;\\\\\\\\nlet langfuse: LangfuseClient;\\\\\\\\n \\\\\\\\nbeforeAll(async () => {\\\\\\\\n // Initialize OpenTelemetry\\\\\\\\n otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\\\\\n otelSdk.start();\\\\\\\\n \\\\\\\\n // Initialize Langfuse client\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { Factuality, Levenshtein } from \\\\\\\\\\\\\\\"autoevals\\\\\\\\\\\\\\\";\\\\\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\\\\\n \\\\\\\\n// Use with additional parameters\\\\\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\\\\\n Factuality,\\\\\\\\n { model: \\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\" } // Additional AutoEvals parameters\\\\\\\\n);\\\\\\\\n \\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"AutoEvals Integration Test\\\\\\\\\\\\\\\",\\\\\\\\n data: testDataset,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [\\\\\\\\n factualityEvaluator,\\\\\\\\n levenshteinEvaluator,\\\\\\\\n customFactualityEvaluator\\\\\\\\n ]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Remote Dataset Runs\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Remote Dataset Runs via the SDK - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\\\\\\\\\"Dataset Runs\\\\\\\\\\\\\\\")Remote Run\\\\\\\\nCopy as Markdown\\\\\\\\n# Remote Dataset Runs\\\\\\\\nOnce you [created a dataset](/docs/evaluation/dataset-runs/datasets), you can use the dataset to test how your application performs on different inputs. Remote Dataset Runs are used to programmatically loop your applications or prompts through a dataset and optionally apply Evaluation Methods to the results.\\\\\\\\nThey are called \\\\\\\\u201cRemote Dataset Runs\\\\\\\\u201d because they can make use of \\\\\\\\u201cremote\\\\\\\\u201d or external logic and code.\\\\\\\\nOptionally, you can also trigger Remote Dataset Runs via the Langfuse UI which will call them via a webhook.\\\\\\\\n## Why use Remote Dataset Runs?\\\\\\\\n  * Full flexibility to use your own application logic\\\\\\\\n  * Use custom scoring functions to evaluate the outputs\\\\\\\\n  * Run multiple experiments on the same dataset in parallel\\\\\\\\n  * Easy to integrate with your existing evaluation infrastructure\\\\\\\\nSequence Diagram\\\\\\\\n## Experiment runner SDK\\\\\\\\nBoth the Python and JS/TS SDKs provide a high-level abstraction for running an experiment on a dataset. The dataset can be both local or hosted on Langfuse. Using the Experiment runner is the recommended way to run an experiment on a dataset with our SDK.\\\\\\\\nThe experiment runner automatically handles:\\\\\\\\n  * **Concurrent execution** of tasks with configurable limits\\\\\\\\n  * **Automatic tracing** of all executions for observability\\\\\\\\n  * **Flexible evaluation** with both item-level and run-level evaluators\\\\\\\\n  * **Error isolation** so individual failures don\\\\\\\\u2019t stop the experiment\\\\\\\\n  * **Dataset integration** for easy comparison and tracking\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThe experiment runner SDK supports both datasets hosted on Langfuse and datasets hosted locally. If you are using a dataset hosted on Langfuse for your experiment, the SDK will automatically create a dataset run for you that you can inspect and compare in the Langfuse UI. For locally hosted datasets not on Langfuse, only traces and scores (if evaluations are used) are tracked in Langfuse.\\\\\\\\n### Basic Usage\\\\\\\\nStart with the simplest possible experiment to test your task function on local data. If you already have a dataset in Langfuse, see here.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\nfrom langfuse.openai import OpenAI\\\\\\\\n \\\\\\\\n///# Initialize client\\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\n///# Define your task function\\\\\\\\ndef my_task(*, item, **kwargs):\\\\\\\\n question = item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]\\\\\\\\n response = OpenAI().chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4.1\\\\\\\\\\\\\\\", messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": question}]\\\\\\\\n )\\\\\\\\n \\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\n \\\\\\\\n///# Run experiment on local data\\\\\\\\nlocal_data = [\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\"},\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\"},\\\\\\\\n]\\\\\\\\n \\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Geography Quiz\\\\\\\\\\\\\\\",\\\\\\\\n description=\\\\\\\\\\\\\\\"Testing basic functionality\\\\\\\\\\\\\\\",\\\\\\\\n data=local_data,\\\\\\\\n task=my_task,\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# Use format method to display results\\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\nMake sure that OpenTelemetry is properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { OpenAI } from \\\\\\\\\\\\\\\"openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { NodeSDK } from \\\\\\\\\\\\\\\"@opentelemetry/sdk-node\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\nimport {\\\\\\\\n LangfuseClient,\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n ExperimentTask,\\\\\\\\n ExperimentItem,\\\\\\\\n} from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\nimport { observeOpenAI } from \\\\\\\\\\\\\\\"@langfuse/openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { LangfuseSpanProcessor } from \\\\\\\\\\\\\\\"@langfuse/otel\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Initialize OpenTelemetry\\\\\\\\nconst otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\\\\\notelSdk.start();\\\\\\\\n \\\\\\\\n// Initialize client\\\\\\\\nconst langfuse = new LangfuseClient();\\\\\\\\n \\\\\\\\n// Run experiment on local data\\\\\\\\nconst localData: ExperimentItem[] = [\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\" },\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\" },\\\\\\\\n];\\\\\\\\n \\\\\\\\n// Define your task function\\\\\\\\nconst myTask: ExperimentTask = async (item) => {\\\\\\\\n const question = item.input;\\\\\\\\n \\\\\\\\n const response = await observeOpenAI(new OpenAI()).chat.completions.create({\\\\\\\\n model: \\\\\\\\\\\\\\\"gpt-4.1\\\\\\\\\\\\\\\",\\\\\\\\n messages: [\\\\\\\\n {\\\\\\\\n role: \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\",\\\\\\\\n content: question,\\\\\\\\n },\\\\\\\\n ],\\\\\\\\n });\\\\\\\\n \\\\\\\\n return response;\\\\\\\\n};\\\\\\\\n \\\\\\\\n// Run the experiment\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Geography Quiz\\\\\\\\\\\\\\\",\\\\\\\\n description: \\\\\\\\\\\\\\\"Testing basic functionality\\\\\\\\\\\\\\\",\\\\\\\\n data: localData,\\\\\\\\n task: myTask,\\\\\\\\n});\\\\\\\\n \\\\\\\\n// Print formatted result\\\\\\\\nconsole.log(await result.format());\\\\\\\\n \\\\\\\\n// Important: shut down OTEL SDK to deliver traces\\\\\\\\nawait otelSdk.shutdown();\\\\\\\\n```\\\\\\\\n **Note for JS/TS SDK** : OpenTelemetry must be properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\\\\\nWhen running experiments on local data, only traces are created in Langfuse - no dataset runs are generated. Each task execution creates an individual trace for observability and debugging.\\\\\\\\n### Usage with Langfuse Datasets\\\\\\\\nRun experiments directly on datasets stored in Langfuse for automatic tracing and comparison.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n///# Get dataset from Langfuse\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"my-evaluation-dataset\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n///# Run experiment directly on the dataset\\\\\\\\nresult = dataset.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Production Model Test\\\\\\\\\\\\\\\",\\\\\\\\n description=\\\\\\\\\\\\\\\"Monthly evaluation of our production model\\\\\\\\\\\\\\\",\\\\\\\\n task=my_task\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# Use format method to display results\\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n// Get dataset from Langfuse\\\\\\\\nconst dataset = await langfuse.dataset.get(\\\\\\\\\\\\\\\"my-evaluation-dataset\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\n// Run experiment directly on the dataset\\\\\\\\nconst result = await dataset.runExperiment({\\\\\\\\n name: \\\\\\\\\\\\\\\"Production Model Test\\\\\\\\\\\\\\\",\\\\\\\\n description: \\\\\\\\\\\\\\\"Monthly evaluation of our production model\\\\\\\\\\\\\\\",\\\\\\\\n task: myTask\\\\\\\\n});\\\\\\\\n \\\\\\\\n// Use format method to display results\\\\\\\\nconsole.log(await result.format());\\\\\\\\n \\\\\\\\n// Important: shut down OpenTelemetry to ensure traces are sent to Langfuse\\\\\\\\nawait otelSdk.shutdown()\\\\\\\\n```\\\\\\\\nWhen using Langfuse datasets, dataset runs are automatically created in Langfuse and are available for comparison in the UI. This enables tracking experiment performance over time and comparing different approaches on the same dataset.\\\\\\\\n### Advanced Features\\\\\\\\nEnhance your experiments with evaluators and advanced configuration options.\\\\\\\\n#### Evaluators\\\\\\\\nEvaluators assess the quality of task outputs at the item level. They receive the input, metadata, output, and expected output for each item and return evaluation metrics that are reported as scores on the traces in Langfuse.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import Evaluation\\\\\\\\n \\\\\\\\n///# Define evaluation functions\\\\\\\\ndef accuracy_evaluator(*, input, output, expected_output, metadata, **kwargs):\\\\\\\\n if expected_output and expected_output.lower() in output.lower():\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=1.0, comment=\\\\\\\\\\\\\\\"Correct answer found\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=0.0, comment=\\\\\\\\\\\\\\\"Incorrect answer\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\ndef length_evaluator(*, input, output, **kwargs):\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\", value=len(output), comment=f\\\\\\\\\\\\\\\"Response has {len(output)} characters\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n///# Use multiple evaluators\\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Multi-metric Evaluation\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[accuracy_evaluator, length_evaluator]\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n// Define evaluation functions\\\\\\\\nconst accuracyEvaluator = async ({ input, output, expectedOutput }) => {\\\\\\\\n if (expectedOutput && output.toLowerCase().includes(expectedOutput.toLowerCase())) {\\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value: 1.0,\\\\\\\\n comment: \\\\\\\\\\\\\\\"Correct answer found\\\\\\\\\\\\\\\"\\\\\\\\n };\\\\\\\\n }\\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value: 0.0,\\\\\\\\n comment: \\\\\\\\\\\\\\\"Incorrect answer\\\\\\\\\\\\\\\"\\\\\\\\n };\\\\\\\\n};\\\\\\\\n \\\\\\\\nconst lengthEvaluator = async ({ input, output }) => {\\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\",\\\\\\\\n value: output.length,\\\\\\\\n comment: `Response has ${output.length} characters`\\\\\\\\n };\\\\\\\\n};\\\\\\\\n \\\\\\\\n// Use multiple evaluators\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Multi-metric Evaluation\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [accuracyEvaluator, lengthEvaluator]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Run-level Evaluators\\\\\\\\nRun-level evaluators assess the full experiment results and compute aggregate metrics. When run on Langfuse datasets, these scores are attached to the full dataset run for tracking overall experiment performance.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import Evaluation\\\\\\\\n \\\\\\\\ndef average_accuracy(*, item_results, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Calculate average accuracy across all items\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n accuracies = [\\\\\\\\n eval.value for result in item_results\\\\\\\\n for eval in result.evaluations\\\\\\\\n if eval.name == \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\"\\\\\\\\n ]\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n if not accuracies:\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=None)\\\\\\\\n \\\\\\\\n avg = sum(accuracies) / len(accuracies)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=avg, comment=f\\\\\\\\\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Comprehensive Analysis\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy]\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst averageAccuracy = async ({ itemResults }) => {\\\\\\\\n // Calculate average accuracy across all items\\\\\\\\n const accuracies = itemResults\\\\\\\\n .flatMap(result => result.evaluations)\\\\\\\\n .filter(evaluation => evaluation.name === \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\")\\\\\\\\n .map(evaluation => evaluation.value as number);\\\\\\\\n \\\\\\\\n if (accuracies.length === 0) {\\\\\\\\n return { name: \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value: null };\\\\\\\\n }\\\\\\\\n \\\\\\\\n const avg = accuracies.reduce((sum, val) => sum + val, 0) / accuracies.length;\\\\\\\\n \\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value: avg,\\\\\\\\n comment: `Average accuracy: ${(avg * 100).toFixed(1)}%`\\\\\\\\n };\\\\\\\\n};\\\\\\\\n \\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Comprehensive Analysis\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [accuracyEvaluator],\\\\\\\\n runEvaluators: [averageAccuracy]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Async Tasks and Evaluators\\\\\\\\nBoth task functions and evaluators can be asynchronous.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport asyncio\\\\\\\\nfrom langfuse.openai import AsyncOpenAI\\\\\\\\n \\\\\\\\nasync def async_llm_task(*, item, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Async task using OpenAI\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n client = AsyncOpenAI()\\\\\\\\n response = await client.chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]}]\\\\\\\\n )\\\\\\\\n \\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\n///# Works seamlessly with async functions\\\\\\\\nresult = langfuse.run_experiment(\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n name=\\\\\\\\\\\\\\\"Async Experiment\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=async_llm_task,\\\\\\\\n max_concurrency=5 # Control concurrent API calls\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport OpenAI from \\\\\\\\\\\\\\\"openai\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\nconst asyncLlmTask = async (item) => {\\\\\\\\n // Async task using OpenAI\\\\\\\\n const client = new OpenAI();\\\\\\\\n const response = await client.chat.completions.create({\\\\\\\\n model: \\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages: [{ role: \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", content: item.input }]\\\\\\\\n });\\\\\\\\n \\\\\\\\n return response.choices[0].message.content;\\\\\\\\n};\\\\\\\\n \\\\\\\\n// Works seamlessly with async functions\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Async Experiment\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: asyncLlmTask,\\\\\\\\n maxConcurrency: 5 // Control concurrent API calls\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Configuration Options\\\\\\\\nCustomize experiment behavior with various configuration options.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Configurable Experiment\\\\\\\\\\\\\\\",\\\\\\\\n run_name=\\\\\\\\\\\\\\\"Custom Run Name\\\\\\\\\\\\\\\", ///# will be dataset run name if dataset is used\\\\\\\\n description=\\\\\\\\\\\\\\\"Experiment with custom configuration\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy],\\\\\\\\n max_concurrency=10, ///# Max concurrent executions\\\\\\\\n metadata={ ///# Attached to all traces\\\\\\\\n \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"temperature\\\\\\\\\\\\\\\": 0.7,\\\\\\\\n \\\\\\\\\\\\\\\"version\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"v1.2.0\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Configurable Experiment\\\\\\\\\\\\\\\",\\\\\\\\n runName: \\\\\\\\\\\\\\\"Custom Run Name\\\\\\\\\\\\\\\", // will be dataset run name if dataset is used\\\\\\\\n description: \\\\\\\\\\\\\\\"Experiment with custom configuration\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [accuracyEvaluator],\\\\\\\\n runEvaluators: [averageAccuracy],\\\\\\\\n maxConcurrency: 10, // Max concurrent executions\\\\\\\\n metadata: { // Attached to all traces\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n model: \\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n temperature: 0.7,\\\\\\\\n version: \\\\\\\\\\\\\\\"v1.2.0\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Testing in CI Environments\\\\\\\\nIntegrate the experiment runner with testing frameworks like Pytest and Vitest to run automated evaluations in your CI pipeline. Use evaluators to create assertions that can fail tests based on evaluation results.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n///# test_geography_experiment.py\\\\\\\\nimport pytest\\\\\\\\nfrom langfuse import get_client, Evaluation\\\\\\\\nfrom langfuse.openai import OpenAI\\\\\\\\n \\\\\\\\n///# Test data for European capitals\\\\\\\\ntest_data = [\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Paris\\\\\\\\\\\\\\\"},\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Berlin\\\\\\\\\\\\\\\"},\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of Spain?\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Madrid\\\\\\\\\\\\\\\"},\\\\\\\\n]\\\\\\\\n \\\\\\\\ndef geography_task(*, item, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Task function that answers geography questions\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n question = item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]\\\\\\\\n response = OpenAI().chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": question}]\\\\\\\\n )\\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n if expected_output and expected_output.lower() in output.lower():\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=1.0)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=0.0)\\\\\\\\n \\\\\\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n accuracies = [\\\\\\\\n eval.value for result in item_results\\\\\\\\n for eval in result.evaluations if eval.name == \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\"\\\\\\\\n ]\\\\\\\\n \\\\\\\\n if not accuracies:\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=None)\\\\\\\\n \\\\\\\\n avg = sum(accuracies) / len(accuracies)\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThese examples show how to use the experiment runner\\\\\\\\u2019s evaluation results to create meaningful test assertions in your CI pipeline. Tests can fail when accuracy drops below acceptable thresholds, ensuring model quality standards are maintained automatically.\\\\\\\\n### Autoevals Integration\\\\\\\\nAccess pre-built evaluation functions through the [autoevals library](https://github.com/braintrustdata/autoevals) integration.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\nThe Python SDK supports AutoEvals evaluators through direct integration:\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse.experiment import create_evaluator_from_autoevals\\\\\\\\nfrom autoevals.llm import Factuality\\\\\\\\n \\\\\\\\nevaluator = create_evaluator_from_autoevals(Factuality())\\\\\\\\n \\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Autoevals Integration Test\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[evaluator]\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { Factuality, Levenshtein } from \\\\\\\\\\\\\\\"autoevals\\\\\\\\\\\\\\\";\\\\\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\\\\\n \\\\\\\\n// Use with additional parameters\\\\\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\\\\\n Factuality,\\\\\\\\n { model: \\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\" } // Additional AutoEvals parameters\\\\\\\\n);\\\\\\\\n \\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"AutoEvals Integration Test\\\\\\\\\\\\\\\",\\\\\\\\n data: testDataset,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [\\\\\\\\n factualityEvaluator,\\\\\\\\n levenshteinEvaluator,\\\\\\\\n customFactualityEvaluator\\\\\\\\n ]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n## Low-level SDK methods\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\\\\\n comment: \\\\\\\\\\\\\\\"This is a comment\\\\\\\\\\\\\\\", // optional, useful to add reasoning\\\\\\\\n });\\\\\\\\n}\\\\\\\\n \\\\\\\\n// Flush the langfuse client to ensure all score data is sent to the server at the end of the experiment run\\\\\\\\nawait langfuse.flush();\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\nfrom langfuse.langchain import CallbackHandler\\\\\\\\n///#from .app import my_llm_application\\\\\\\\n \\\\\\\\n///# Load the dataset\\\\\\\\ndataset = get_client().get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n///# Initialize the Langfuse handler\\\\\\\\nlangfuse_handler = CallbackHandler()\\\\\\\\n \\\\\\\\n///# Loop over the dataset items\\\\\\\\nfor item in dataset.items:\\\\\\\\n # Use the item.run() context manager for automatic trace linking\\\\\\\\n with item.run(\\\\\\\\n run_name=\\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\",\\\\\\\\n run_description=\\\\\\\\\\\\\\\"My first run\\\\\\\\\\\\\\\",\\\\\\\\n run_metadata={\\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"llama3\\\\\\\\\\\\\\\"},\\\\\\\\n ) as root_span:\\\\\\\\n # Execute your LLM-app against the dataset item input\\\\\\\\n output = my_langchain_chain(item.input, langfuse_handler)\\\\\\\\n \\\\\\\\n # Update top-level trace input and output\\\\\\\\n root_span.update_trace(input=item.input, output=output.content)\\\\\\\\n \\\\\\\\n # Optionally: Add scores computed in your experiment runner, e.g. json equality check\\\\\\\\n root_span.score_trace(\\\\\\\\n name=\\\\\\\\\\\\\\\"<example_eval>\\\\\\\\\\\\\\\",\\\\\\\\n value=my_eval_fn(item.input, output, item.expected_output),\\\\\\\\n comment=\\\\\\\\\\\\\\\"This is a comment\\\\\\\\\\\\\\\", ///# optional, useful to add reasoning\\\\\\\\n )\\\\\\\\n \\\\\\\\n///# Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run\\\\\\\\nget_client().flush()\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { LangfuseClient } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\nimport { CallbackHandler } from \\\\\\\\\\\\\\\"@langfuse/langchain\\\\\\\\\\\\\\\";\\\\\\\\n...\\\\\\\\n \\\\\\\\nconst langfuse = new LangfuseClient()\\\\\\\\nconst runName = \\\\\\\\\\\\\\\"my-dataset-run\\\\\\\\\\\\\\\";\\\\\\\\nfor (const item of dataset.items) {\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/guides/cookbook/example_external_evaluation_pipelines\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline - Langfuse\\\\\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\\\\\\\\\"Cookbooks\\\\\\\\\\\\\\\")External Evaluation Pipelines\\\\\\\\nCopy as Markdown\\\\\\\\nThis is a Jupyter notebook\\\\\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\\\\\n# Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\\\\\\nThis cookbook explains how to build an external evaluation pipeline to measure the performance of your production LLM application using Langfuse.\\\\\\\\nAs a rule of thumb, we encourage you to check first if the [evaluations in the Langfuse UI](https://langfuse.com/docs/scores/model-based-evals) cover your use case. If your needs go beyond these, you can still implement in Langfuse custom evaluation templates without code.\\\\\\\\nConsider implementing an external evaluation pipeline if you need:\\\\\\\\n  * More control over **when** traces get evaluated. You could schedule the pipeline to run at specific times or responding to event-based triggers like Webhooks.\\\\\\\\n  * Greater flexibility with your custom evaluations, when your needs go beyond what\\\\\\\\u2019s possible with the Langfuse UI\\\\\\\\n  * Version control for your custom evaluations\\\\\\\\n  * The ability to evaluate data using existing evaluation frameworks\\\\\\\\nIf your use case meets any of this situations, let\\\\\\\\u2019s go ahead and implement your first external evaluation pipeline!\\\\\\\\n* * *\\\\\\\\nBy the end of this cookbook, you\\\\\\\\u2019ll be able to:\\\\\\\\n  * Create a synthetic dataset to test your models.\\\\\\\\n  * Use the Langfuse client to gather and filter traces of previous model runs\\\\\\\\n  * Evaluate these traces offline and incrementally\\\\\\\\n  * Add scores to existing Langfuse traces\\\\\\\\nConceptually, we will implement the following architecture:\\\\\\\\n* * *\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n**Note** : While we\\\\\\\\u2019re using a Jupyter notebook for this cookbook, in production you\\\\\\\\u2019d use your preferred orchestration tool. Just make sure to extract the code into a .py file and ensure all dependencies are available at runtime.\\\\\\\\n## (Prep-work) Loading synthetic traces to Langfuse\\\\\\\\nIn this demo, we\\\\\\\\u2019ll build a mock application: a science communicator LLM that explains any topic in an engaging and approachable way.\\\\\\\\nSince we don\\\\\\\\u2019t have real user data, our first step is to create a synthetic dataset. We\\\\\\\\u2019ll generate a variety of potential questions that real users might ask. While this is a great way to kickstart your LLM development, collecting real user queries as soon as possible is invaluable.\\\\\\\\nYou can get your Langfuse API keys [here](https://cloud.langfuse.com/) and OpenAI API key [here](https://platform.openai.com/api-keys)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n%pip install langfuse openai deepeval --upgrade\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport os\\\\\\\\n \\\\\\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"pk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddea\\\\\\\\ud83c\\\\\\\\uddfa EU region\\\\\\\\n///# os.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://us.cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddfa\\\\\\\\ud83c\\\\\\\\uddf8 US region\\\\\\\\n \\\\\\\\n///# Your openai key\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"OPENAI_API_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-proj-...\\\\\\\\\\\\\\\"\\\\\\\\n```\\\\\\\\nLet\\\\\\\\u2019s go ahead and generate a list of topic suggestions that we can later query to our application.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport openai\\\\\\\\n \\\\\\\\ntopic_suggestion = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\" You're a world-class journalist, specialized\\\\\\\\nin figuring out which are the topics that excite people the most.\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nBATCH_SIZE = 10\\\\\\\\nTOTAL_TRACES = 50\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nnow = datetime.now()\\\\\\\\nfive_am_today = datetime(now.year, now.month, now.day, 5, 0)\\\\\\\\nfive_am_yesterday = five_am_today - timedelta(days=1)\\\\\\\\n \\\\\\\\ntraces_batch = langfuse.api.trace.list(page=1,\\\\\\\\n limit=BATCH_SIZE,\\\\\\\\n tags=\\\\\\\\\\\\\\\"ext_eval_pipelines\\\\\\\\\\\\\\\",\\\\\\\\n from_timestamp=five_am_yesterday,\\\\\\\\n to_timestamp=datetime.now()\\\\\\\\n ).data\\\\\\\\n \\\\\\\\nprint(f\\\\\\\\\\\\\\\"Traces in first batch: {len(traces_batch)}\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nTraces in first batch: 10\\\\\\\\n## 2\\\\\\\\\\\\\\\\. Run your evaluations\\\\\\\\nLangfuse can handle numerical, boolean and categorical (`string`) scores. Wrapping your custom evaluation logic in a function is often a good practice. Evaluation functions should take a `trace` as input and yield a valid score. Let\\\\\\\\u2019s begin with a simple example using a categorical score.\\\\\\\\n### 2.1. Categoric Evaluations\\\\\\\\nWhen analyzing the outputs of your LLM applications, you may want to evaluate traits that are best defined qualitatively, such as sentiment, tonality or text complexity (Grade level).\\\\\\\\nWe\\\\\\\\u2019re building a science educator LLM that should sound engaging and positive. To ensure it hits the right notes, we\\\\\\\\u2019ll evaluate the tone of its outputs to see if they match our intent. We\\\\\\\\u2019ll draft an evaluation prompt ourselves (no library) to identify the three main tones in each model output.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\ntemplate_tone_eval = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nYou're an expert in human emotional intelligence. You can identify with ease the\\\\\\\\n tone in human-written text. Your task is to identify the tones present in a\\\\\\\\n piece of <text/> with precission. Your output is a comma separated list of three\\\\\\\\n tones. PRINT THE LIST ALONE, NOTHING ELSE.\\\\\\\\n \\\\\\\\n<possible_tones>\\\\\\\\nneutral, confident, joyful, optimistic, friendly, urgent, analytical, respectful\\\\\\\\n</possible_tones>\\\\\\\\n \\\\\\\\n<example_1>\\\\\\\\nInput: Citizen science plays a crucial role in research by involving everyday\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\npeople in scientific projects. This collaboration allows researchers to collect\\\\\\\\nvast amounts of data that would be impossible to gather on their own. Citizen\\\\\\\\nscientists contribute valuable observations and insights that can lead to new\\\\\\\\ndiscoveries and advancements in various fields. By participating in citizen\\\\\\\\nscience projects, individuals can actively contribute to scientific research\\\\\\\\nand make a meaningful impact on our understanding of the world around us.\\\\\\\\n \\\\\\\\nOutput: respectful,optimistic,confident\\\\\\\\n</example_1>\\\\\\\\n \\\\\\\\n<example_2>\\\\\\\\nInput: Bionics is a field that combines biology and engineering to create\\\\\\\\ndevices that can enhance human abilities. By merging humans and machines,\\\\\\\\nbionics aims to improve quality of life for individuals with disabilities\\\\\\\\nor enhance performance for others. These technologies often mimic natural\\\\\\\\nprocesses in the body to create seamless integration. Overall, bionics holds\\\\\\\\ngreat potential for revolutionizing healthcare and technology in the future.\\\\\\\\n \\\\\\\\nOutput: optimistic,confident,analytical\\\\\\\\n</example_2>\\\\\\\\n \\\\\\\\n<example_3>\\\\\\\\nInput: Social media can have both positive and negative impacts on mental\\\\\\\\nhealth. On the positive side, it can help people connect, share experiences,\\\\\\\\nand find support. However, excessive use of social media can also lead to\\\\\\\\nfeelings of inadequacy, loneliness, and anxiety. It's important to find a\\\\\\\\nbalance and be mindful of how social media affects your mental well-being.\\\\\\\\nRemember, it's okay to take breaks and prioritize your mental health.\\\\\\\\n \\\\\\\\nOutput: friendly,neutral,respectful\\\\\\\\n</example_3>\\\\\\\\n \\\\\\\\n<text>\\\\\\\\n{text}\\\\\\\\n</text>\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n \\\\\\\\ntest_tone_score = openai.chat.completions.create(\\\\\\\\n messages=[\\\\\\\\n {\\\\\\\\n \\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": template_tone_eval.format(\\\\\\\\n text=traces_batch[1].output),\\\\\\\\n }\\\\\\\\n ],\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\n temperature=0\\\\\\\\n).choices[0].message.content\\\\\\\\nprint(f\\\\\\\\\\\\\\\"User query: {traces_batch[1].input['args'][0]}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Model answer: {traces_batch[1].output}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Dominant tones: {test_tone_score}\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nIdentifying human intents and tones can be tricky for language models. To handle this, we used a multi-shot prompt, which means giving the model several examples to learn from. Now let\\\\\\\\u2019s wrap our code in an evaluation function for convenience.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\ndef tone_score(trace):\\\\\\\\n return openai.chat.completions.create(\\\\\\\\n messages=[\\\\\\\\n {\\\\\\\\n \\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": template_tone_eval.format(text=trace.output),\\\\\\\\n }\\\\\\\\n ],\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\",\\\\\\\\n temperature=0\\\\\\\\n ).choices[0].message.content\\\\\\\\n \\\\\\\\ntone_score(traces_batch[1])\\\\\\\\n```\\\\\\\\nGreat! Now let\\\\\\\\u2019s go ahead and create a numeric evaluation score.\\\\\\\\n### 2.2. Numeric Evaluations\\\\\\\\nIn this cookbook, we\\\\\\\\u2019ll use the `Deepeval` framework ([docs](https://docs.confident-ai.com/docs/getting-started)) to handle our numeric evaluations. Deepeval provides scores ranging from zero to one for many common LLM metrics. Plus, you can create custom metrics by simply describing them in plain language. To ensure our app\\\\\\\\u2019s responses are joyful and engaging, we\\\\\\\\u2019ll define a custom \\\\\\\\u2018joyfulness\\\\\\\\u2019 score.\\\\\\\\nYou can use any evaluation library. These are popular ones:\\\\\\\\n  * OpenAI Evals ([GitHub](https://github.com/openai/evals))\\\\\\\\n  * Langchain Evaluators\\\\\\\\n  * [RAGAS](https://docs.ragas.io/en/latest/concepts/metrics/index.html) for RAG applications\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom deepeval.metrics import GEval\\\\\\\\nfrom deepeval.test_case import LLMTestCaseParams, LLMTestCase\\\\\\\\n \\\\\\\\ndef joyfulness_score(trace):\\\\\\\\n joyfulness_metric = GEval(\\\\\\\\n name=\\\\\\\\\\\\\\\"Correctness\\\\\\\\\\\\\\\",\\\\\\\\n criteria=\\\\\\\\\\\\\\\"Determine whether the output is engaging and fun.\\\\\\\\\\\\\\\",\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\\\\\\\\n )\\\\\\\\n test_case = LLMTestCase(\\\\\\\\n input=trace.input[\\\\\\\\\\\\\\\"args\\\\\\\\\\\\\\\"],\\\\\\\\n actual_output=trace.output)\\\\\\\\n \\\\\\\\n joyfulness_metric.measure(test_case)\\\\\\\\n \\\\\\\\n print(f\\\\\\\\\\\\\\\"Score: {joyfulness_metric.score}\\\\\\\\\\\\\\\")\\\\\\\\n print(f\\\\\\\\\\\\\\\"Reason: {joyfulness_metric.reason}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n return {\\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": joyfulness_metric.score, \\\\\\\\\\\\\\\"reason\\\\\\\\\\\\\\\": joyfulness_metric.reason}\\\\\\\\n \\\\\\\\njoyfulness_score(traces_batch[1])\\\\\\\\n```\\\\\\\\nUnder the hood, GEval uses chain of thought (CoT) prompting to formulate a set of criteria for scoring prompts. When developing your own metrics, it\\\\\\\\u2019s important to review the reasoning behind these scores. This helps ensure that the model evaluates the traces just as you intended when you wrote the evaluation prompt.\\\\\\\\nOur eval function returns a dictionary with both the score and the model\\\\\\\\u2019s reasoning. We do this as we\\\\\\\\u2019ll persist the reasoning with every langfuse score, ensuring interpretability.\\\\\\\\nNow we\\\\\\\\u2019re done with defining our evaluation functions. Let\\\\\\\\u2019s push those scores back to Langfuse!\\\\\\\\n## 3\\\\\\\\\\\\\\\\. Pushing Scores to Langfuse\\\\\\\\nNow that we have our evaluation functions ready, it\\\\\\\\u2019s time to put them to work. Use the Langfuse client to add scores to existing traces.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_score(\\\\\\\\n trace_id=traces_batch[1].id,\\\\\\\\n name=\\\\\\\\\\\\\\\"tone\\\\\\\\\\\\\\\",\\\\\\\\n value=joyfulness_score(traces_batch[1])[\\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\"],\\\\\\\\n comment=joyfulness_score(traces_batch[1])[\\\\\\\\\\\\\\\"reason\\\\\\\\\\\\\\\"]\\\\\\\\n)\\\\\\\\n```\\\\\\\\nAnd thus, you\\\\\\\\u2019ve added your first externally-evaluated score to Langfuse! Just 49 more to go \\\\\\\\ud83d\\\\\\\\ude01. But don\\\\\\\\u2019t worry \\\\\\\\u2014 our solutions are easy to scale.\\\\\\\\n## 4\\\\\\\\\\\\\\\\. Putting everything together\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nUntil now, we went through each of the necessary steps to build an external evaluation pipeline: Fetching traces, running the evaluations, and persisting the scores to Langfuse. Let\\\\\\\\u2019s sum it up into a compact script that you could run in your evaluation pipeline.\\\\\\\\nWe\\\\\\\\u2019ll fetch the data in batches of 10 traces and then iterate through each trace to score it and push the scores back to Langfuse. Note that this batch size is for demonstration purposes. In a production setup, you might want to process multiple batches in parallel to speed things up. Batching not only reduces the memory load on your system but also allows you to create checkpoints, so you can easily resume if something goes wrong.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport math\\\\\\\\n \\\\\\\\nfor page_number in range(1, math.ceil(TOTAL_TRACES/BATCH_SIZE)):\\\\\\\\n \\\\\\\\n traces_batch = langfuse.api.trace.list(\\\\\\\\n tags=\\\\\\\\\\\\\\\"ext_eval_pipelines\\\\\\\\\\\\\\\",\\\\\\\\n page=page_number,\\\\\\\\n from_timestamp=five_am_yesterday,\\\\\\\\n to_timestamp=five_am_today,\\\\\\\\n limit=BATCH_SIZE\\\\\\\\n ).data\\\\\\\\n \\\\\\\\n for trace in traces_batch:\\\\\\\\n print(f\\\\\\\\\\\\\\\"Processing {trace.name}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n if trace.output is None:\\\\\\\\n print(f\\\\\\\\\\\\\\\"Warning: \\\\\\\\\\\\\\\\n Trace {trace.name} had no generated output, \\\\\\\\\\\\\\\\\\\\\\\\n it was skipped\\\\\\\\\\\\\\\")\\\\\\\\n continue\\\\\\\\n \\\\\\\\n langfuse.create_score(\\\\\\\\n trace_id=trace.id,\\\\\\\\n name=\\\\\\\\\\\\\\\"tone\\\\\\\\\\\\\\\",\\\\\\\\n value=tone_score(trace)\\\\\\\\n )\\\\\\\\n \\\\\\\\n jscore = joyfulness_score(trace)\\\\\\\\n langfuse.create_score(\\\\\\\\n trace_id=trace.id,\\\\\\\\n name=\\\\\\\\\\\\\\\"joyfulness\\\\\\\\\\\\\\\",\\\\\\\\n value=jscore[\\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\"],\\\\\\\\n comment=jscore[\\\\\\\\\\\\\\\"reason\\\\\\\\\\\\\\\"]\\\\\\\\n )\\\\\\\\n \\\\\\\\n print(f\\\\\\\\\\\\\\\"Batch {page_number} processed \\\\\\\\ud83d\\\\\\\\ude80 \\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nIf your pipeline ran successfully, you should see your score in the Langfuse UI. \\\\\\\\nAnd that\\\\\\\\u2019s it! You\\\\\\\\u2019re now ready to integrate these lines into your preferred orchestration tool to ensure they run at the right times.\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nTo achieve our original goal of running the script every day at 5 am, simply schedule a Cron task in your chosen environment with the rule `cron(0 5 * * ? *)`.\\\\\\\\nThanks for coding along! I hope you enjoyed the tutorial and found it helpful.\\\\\\\\nLast updated on August 20, 2025\\\\\\\\n[Example - Tracing and Evaluation for the OpenAI-Agents SDK](/guides/cookbook/example_evaluating_openai_agents \\\\\\\\\\\\\\\"Example - Tracing and Evaluation for the OpenAI-Agents SDK\\\\\\\\\\\\\\\")\\\\\\\\n[Guide - Building an intent classification pipeline](/guides/cookbook/example_intent_classification_pipeline \\\\\\\\\\\\\\\"Guide - Building an intent classification pipeline\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"github_discussion\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://github.com/orgs/langfuse/discussions/7572\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"LLM-as-a-Judge Evaluators for Dataset doesn't work as expected.\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\\\\\n\\\\\\\\n---\\\\\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=hallucination) | [Report \\\\\\\\ud83d\\\\\\\\udc1b](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\\\\\n\\\\\\\\n        Reply #5 to comment #1 from dosubot on June 30, 2025:\\\\\\\\n\\\\\\\\n        I am using ui and variable mapping is as follows:\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n{{search_query}} is mapped to Object: Dataset item, Object Variable: Input, JsonPath: <empty>\\\\\\\\r\\\\\\\\n{{summary}} is mapped to Object: Dataset item, Object Variable: Metadata, JsonPath: <empty>\\\\\\\\r\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"{{expected}} is mapped to Object: Dataset item,  Object Variable: Expected Output, JsonPath: <empty>\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\nA single row format example from my dataset: \\\\\\\\r\\\\\\\\nInput column: \\\\\\\\\\\\\\\"Test input\\\\\\\\\\\\\\\", Expected Output column: \\\\\\\\\\\\\\\"Test expected output\\\\\\\\\\\\\\\", Metadata column: \\\\\\\\\\\\\\\"Test metadata\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n        Reply #6 to comment #1 from dosubot on June 30, 2025:\\\\\\\\n\\\\\\\\n        <!-- Answer -->\\\\\\\\nYour mapping setup looks correct, but the backend eligibility logic is strict: if any mapped variable can't be resolved (for example, if the field is empty or the type doesn't match expectations), the item is silently skipped and you get \\\\\\\\\\\\\\\"0 elements\\\\\\\\\\\\\\\" processed. \\\\\\\\n\\\\\\\\nOne likely issue is the Metadata field\\\\\\\\u2014if it's a plain string (like \\\\\\\\\\\\\\\"Test metadata\\\\\\\\\\\\\\\"), but the system expects a JSON object (since many Langfuse examples use objects for metadata), this can cause the mapping to fail even if JsonPath is empty. When JsonPath is empty, the backend uses the top-level value as-is, but if the field is a string and the mapping expects an object (or vice versa), the item is excluded from evaluation\\\\\\\\u00a0[see code logic](https://github.com/langfuse/langfuse/blob/98cc1bb7a1759e29de3dd818f6a500df48fb5e82/worker/src/features/evaluation/evalService.ts).\\\\\\\\n\\\\\\\\nTry this: map all variables to fields you know are plain strings (like Input or Expected Output) and see if the evaluation processes any items. If it works, the issue is with the Metadata field's type. To confirm, create a dataset item where Metadata is a JSON object (e.g., {\\\\\\\\\\\\\\\"meta\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"value\\\\\\\\\\\\\\\"}) and try mapping with and without a JsonPath. This will help isolate the problem.\\\\\\\\n\\\\\\\\nIf you still get 0 elements, export a sample of your dataset as JSON and share it\\\\\\\\u2014there may be a subtle data or mapping mismatch. \\\\\\\\n\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nMore details on dataset item structure and mapping: [Langfuse docs](https://langfuse.com/docs/datasets/get-started).\\\\\\\\n\\\\\\\\n\\\\\\\\n<!-- Dosu Comment Footer -->\\\\\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\\\\\n\\\\\\\\n---\\\\\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=hallucination) | [Report \\\\\\\\ud83d\\\\\\\\udc1b](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\\\\\n\\\\\\\\n        Reply #7 to comment #1 from dosubot on June 30, 2025:\\\\\\\\n\\\\\\\\n        I have created a new dataset with single row in json format, so columns Input, Expected Output and Metadata have following values respectively but still the same issue:\\\\\\\\r\\\\\\\\n{ \\\\\\\\\\\\\\\"search_query\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What's MLflow?\\\\\\\\\\\\\\\" }\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n{ \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"MLflow tracks ML experiments.\\\\\\\\\\\\\\\" } \\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2023-09-25-datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Datasets (beta)\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Datasets (beta) - Langfuse\\\\\\\\n[\\\\\\\\u2190 Back to changelog](/changelog#2023-09-25-datasets)\\\\\\\\nSeptember 25, 2023\\\\\\\\n# Datasets (beta)\\\\\\\\n[Marc Klingen](https://twitter.com/marcklingen)\\\\\\\\nCollect sets of inputs and expected outputs in Langfuse to evaluate your LLM app. Use evaluations to benchmark different experiments.\\\\\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\\\\\n_Run experiment on dataset_\\\\\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n \\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n # output also returned as it is used to evaluate the run\\\\\\\\n generation, output = my_llm_application.run(item.input)\\\\\\\\n \\\\\\\\n # link the execution trace to the dataset item and give it a run_name\\\\\\\\n item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # optionally, evaluate the output to compare different runs more easily\\\\\\\\n generation.score(\\\\\\\\n name=\\\\\\\\\\\\\\\"<example_eval>\\\\\\\\\\\\\\\",\\\\\\\\n # any float value\\\\\\\\n value=my_eval_fn(\\\\\\\\n item.input,\\\\\\\\n output,\\\\\\\\n item.expected_output\\\\\\\\n )\\\\\\\\n )\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst dataset = await langfuse.getDataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\nfor (const item of dataset.items) {\\\\\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n // output also returned as it is used to evaluate the run\\\\\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\\\\\n \\\\\\\\n // link the execution trace to the dataset item and give it a run_name\\\\\\\\n await item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\n // optionally, evaluate the output to compare different runs more easily\\\\\\\\n generation.score({\\\\\\\\n name: \\\\\\\\\\\\\\\"<score_name>\\\\\\\\\\\\\\\",\\\\\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\\\\\n });\\\\\\\\n}\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Troubleshooting and FAQ\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\\\\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\\\\\\\\\(\\\\\\\\\\\\\\\\)` method\\\\\\\\\\\\\\\")\\\\\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\\\\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\\\\\\\\\")\\\\\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\\\\\\\\\"Langfuse Support: \\\\\\\\\\\\\\\\[Experiment\\\\\\\\\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\\\\\\\\\")\\\\\\\\n[Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\\\\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing\\\\\\\\\\\\\\\")\\\\\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\\\\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\\\\\\\\\")\\\\\\\\n[Results for some data items not present when comparing experiments](https://github.com/orgs/langfuse/discussions/5928 \\\\\\\\\\\\\\\"Langfuse Support: Results for some data items not present when comparing experiments\\\\\\\\\\\\\\\")\\\\\\\\n[Deleting Metrics for Langfuse](https://github.com/orgs/langfuse/discussions/5849 \\\\\\\\\\\\\\\"Langfuse Support: Deleting Metrics for Langfuse\\\\\\\\\\\\\\\")\\\\\\\\n[Discrepancies between dataset items found in the UI vs retrieved from the SDK/API](https://github.com/orgs/langfuse/discussions/5822 \\\\\\\\\\\\\\\"Langfuse Support: Discrepancies between dataset items found in the UI vs retrieved from the SDK/API\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Request batch adding of multiple traces to datasets](https://github.com/orgs/langfuse/discussions/8526 \\\\\\\\\\\\\\\"Langfuse Ideas: Request batch adding of multiple traces to datasets\\\\\\\\\\\\\\\")\\\\\\\\n[How to categorize topics and create a pie chart?](https://github.com/orgs/langfuse/discussions/8512 \\\\\\\\\\\\\\\"Langfuse Ideas: How to categorize topics and create a pie chart?\\\\\\\\\\\\\\\")\\\\\\\\n[Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window](https://github.com/orgs/langfuse/discussions/8485 \\\\\\\\\\\\\\\"Langfuse Ideas: Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window\\\\\\\\\\\\\\\")\\\\\\\\n[feat(LLM-as-a-judge): support stratified sampling by trace property](https://github.com/orgs/langfuse/discussions/8480 \\\\\\\\\\\\\\\"Langfuse Ideas: feat\\\\\\\\\\\\\\\\(LLM-as-a-judge\\\\\\\\\\\\\\\\): support stratified sampling by trace property\\\\\\\\\\\\\\\")\\\\\\\\n[Bulk Evaluation Does not Displays the status of the dataset (Like for E.g. processing, evaluated, executed)](https://github.com/orgs/langfuse/discussions/8410 \\\\\\\\\\\\\\\"Langfuse Ideas: Bulk Evaluation Does not Displays the status of the dataset \\\\\\\\\\\\\\\\(Like for E.g. processing, evaluated, executed\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[Annotation Queue Creation API](https://github.com/orgs/langfuse/discussions/8372 \\\\\\\\\\\\\\\"Langfuse Ideas: Annotation Queue Creation API\\\\\\\\\\\\\\\")\\\\\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\\\\\\\\\"Langfuse Ideas: \\\\\\\\\\\\\\\\[Langfuse Cloud\\\\\\\\\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\\\\\\\\\")\\\\\\\\n[feat(evals): allow canceling a running evaluator with pending evaluation jobs](https://github.com/orgs/langfuse/discussions/8310 \\\\\\\\\\\\\\\"Langfuse Ideas: feat\\\\\\\\\\\\\\\\(evals\\\\\\\\\\\\\\\\): allow canceling a running evaluator with pending evaluation jobs\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Support Description on Dataset Items](https://github.com/orgs/langfuse/discussions/6011 \\\\\\\\\\\\\\\"Langfuse Ideas: Support Description on Dataset Items\\\\\\\\\\\\\\\")\\\\\\\\n[Alta Integration](https://github.com/orgs/langfuse/discussions/5957 \\\\\\\\\\\\\\\"Langfuse Ideas: Alta Integration\\\\\\\\\\\\\\\")\\\\\\\\n[Add run name to columns when looking at a specific dataset item](https://github.com/orgs/langfuse/discussions/5929 \\\\\\\\\\\\\\\"Langfuse Ideas: Add run name to columns when looking at a specific dataset item\\\\\\\\\\\\\\\")\\\\\\\\n[Delete multiple dataset runs](https://github.com/orgs/langfuse/discussions/5893 \\\\\\\\\\\\\\\"Langfuse Ideas: Delete multiple dataset runs\\\\\\\\\\\\\\\")\\\\\\\\n[Navigation between items in a dataset run is confusing - context of the selected dataset run is lost](https://github.com/orgs/langfuse/discussions/5892 \\\\\\\\\\\\\\\"Langfuse Ideas: Navigation between items in a dataset run is confusing - context of the selected dataset run is lost\\\\\\\\\\\\\\\")\\\\\\\\n[Enhanced score distribution visualization in experiment analysis](https://github.com/orgs/langfuse/discussions/5819 \\\\\\\\\\\\\\\"Langfuse Ideas: Enhanced score distribution visualization in experiment analysis\\\\\\\\\\\\\\\")\\\\\\\\n[Multi-step Prompt Experiments and Playground](https://github.com/orgs/langfuse/discussions/5812 \\\\\\\\\\\\\\\"Langfuse Ideas: Multi-step Prompt Experiments and Playground\\\\\\\\\\\\\\\")\\\\\\\\n[Option to add trace to new dataset](https://github.com/orgs/langfuse/discussions/5756 \\\\\\\\\\\\\\\"Langfuse Ideas: Option to add trace to new dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Simplified UI for Scoring](https://github.com/orgs/langfuse/discussions/5721 \\\\\\\\\\\\\\\"Langfuse Ideas: Simplified UI for Scoring\\\\\\\\\\\\\\\")\\\\\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\\\\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Datasets: Add selection of traces to a dataset](https://github.com/orgs/langfuse/discussions/4353 \\\\\\\\\\\\\\\"Langfuse Ideas: Datasets: Add selection of traces to a dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Multi-user annotation capability in Annotation Queues](https://github.com/orgs/langfuse/discussions/4348 \\\\\\\\\\\\\\\"Langfuse Ideas: Multi-user annotation capability in Annotation Queues\\\\\\\\\\\\\\\")\\\\\\\\n[Multi-turn / session experiments in datasets](https://github.com/orgs/langfuse/discussions/4208 \\\\\\\\\\\\\\\"Langfuse Ideas: Multi-turn / session experiments in datasets\\\\\\\\\\\\\\\")\\\\\\\\n[Enable to use variable of prompt on evaluator.](https://github.com/orgs/langfuse/discussions/4121 \\\\\\\\\\\\\\\"Langfuse Ideas: Enable to use variable of prompt on evaluator.\\\\\\\\\\\\\\\")\\\\\\\\n[Sessions Table: Scores Column](https://github.com/orgs/langfuse/discussions/4120 \\\\\\\\\\\\\\\"Langfuse Ideas: Sessions Table: Scores Column\\\\\\\\\\\\\\\")\\\\\\\\n[Add new filters for the LLM as a Judge Evaluation (other scores and cost)](https://github.com/orgs/langfuse/discussions/4106 \\\\\\\\\\\\\\\"Langfuse Ideas: Add new filters for the LLM as a Judge Evaluation \\\\\\\\\\\\\\\\(other scores and cost\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[Export dataset run table](https://github.com/orgs/langfuse/discussions/4077 \\\\\\\\\\\\\\\"Langfuse Ideas: Export dataset run table\\\\\\\\\\\\\\\")\\\\\\\\n[feat: support adding trace tags in annotation queue view](https://github.com/orgs/langfuse/discussions/4037 \\\\\\\\\\\\\\\"Langfuse Ideas: feat: support adding trace tags in annotation queue view\\\\\\\\\\\\\\\")\\\\\\\\n[Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times](https://github.com/orgs/langfuse/discussions/4035 \\\\\\\\\\\\\\\"Langfuse Ideas: Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times\\\\\\\\\\\\\\\")\\\\\\\\n[Diff support for dataset runs view](https://github.com/orgs/langfuse/discussions/4025 \\\\\\\\\\\\\\\"Langfuse Ideas: Diff support for dataset runs view\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Create Support for gemini models in playground](https://github.com/orgs/langfuse/discussions/4019 \\\\\\\\\\\\\\\"Langfuse Ideas: Create Support for gemini models in playground\\\\\\\\\\\\\\\")\\\\\\\\n[Change AWS access pattern for Bedrock LLM usage, assume role](https://github.com/orgs/langfuse/discussions/3988 \\\\\\\\\\\\\\\"Langfuse Ideas: Change AWS access pattern for Bedrock LLM usage, assume role\\\\\\\\\\\\\\\")\\\\\\\\n[Add ability to export and import evaluators between projects](https://github.com/orgs/langfuse/discussions/3970 \\\\\\\\\\\\\\\"Langfuse Ideas: Add ability to export and import evaluators between projects\\\\\\\\\\\\\\\")\\\\\\\\n[feat: Folder structure for dataset organisation](https://github.com/orgs/langfuse/discussions/3935 \\\\\\\\\\\\\\\"Langfuse Ideas: feat: Folder structure for dataset organisation\\\\\\\\\\\\\\\")\\\\\\\\n[Model-based evaluations triggered by observations](https://github.com/orgs/langfuse/discussions/3918 \\\\\\\\\\\\\\\"Langfuse Ideas: Model-based evaluations triggered by observations\\\\\\\\\\\\\\\")\\\\\\\\n[Scores: Conditional Annotation](https://github.com/orgs/langfuse/discussions/3842 \\\\\\\\\\\\\\\"Langfuse Ideas: Scores: Conditional Annotation\\\\\\\\\\\\\\\")\\\\\\\\n[Annotation Queues: define optional/mandatory score configs by queue](https://github.com/orgs/langfuse/discussions/3841 \\\\\\\\\\\\\\\"Langfuse Ideas: Annotation Queues: define optional/mandatory score configs by queue\\\\\\\\\\\\\\\")\\\\\\\\n[Scores: support for recording multiple choice selection as score value](https://github.com/orgs/langfuse/discussions/3840 \\\\\\\\\\\\\\\"Langfuse Ideas: Scores: support for recording multiple choice selection as score value\\\\\\\\\\\\\\\")\\\\\\\\n[Filter by status in dataset items table](https://github.com/orgs/langfuse/discussions/3818 \\\\\\\\\\\\\\\"Langfuse Ideas: Filter by status in dataset items table\\\\\\\\\\\\\\\")\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/overview\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Evaluation Overview\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Evaluation of LLM Applications - Langfuse\\\\\\\\nDocsEvaluationOverview\\\\\\\\nCopy as Markdown\\\\\\\\n# Evaluation Overview\\\\\\\\nEvaluation is a critical aspect of developing and deploying LLM applications. Usually, teams use a multitude of different evaluation methods to score the performance of their AI application depending on the use case and the stage of the development process.\\\\\\\\n\\\\\\\\ud83c\\\\\\\\udfa5\\\\\\\\n[ **Watch this walkthrough**](/watch-demo?tab=evaluation) of Langfuse Evaluation and how to use it to improve your LLM application.\\\\\\\\n## Why use LLM Evaluation?\\\\\\\\nLLM evaluation is crucial for improving the accuracy and robustness of language models, ultimately enhancing the user experience and trust in your AI application. Here are the key benefits:\\\\\\\\n  * **Quality Assurance** : Detect hallucinations, factual inaccuracies, and inconsistent outputs to ensure your AI app delivers reliable results\\\\\\\\n  * **Performance Monitoring** : Measure response quality, relevance, and user satisfaction across different scenarios and edge cases\\\\\\\\n  * **Continuous Improvement** : Identify areas for enhancement and track improvements over time through structured evaluation metrics\\\\\\\\n  * **User Trust** : Build confidence in your AI application by demonstrating consistent, high-quality outputs through systematic evaluation\\\\\\\\n  * **Risk Mitigation** : Catch potential issues before they reach production users, reducing the likelihood of poor user experiences or reputational damage\\\\\\\\n## Online & Offline Evaluation\\\\\\\\n**Offline Evaluation involves**\\\\\\\\n  * Evaluating the application in a controlled setting\\\\\\\\n  * Typically using curated test Datasets instead of live user queries\\\\\\\\n  * Heavily used during development (can be part of CI/CD pipelines) to measure improvements / regressions\\\\\\\\n  * Repeatable and you can get clear accuracy metrics since you have ground truth.\\\\\\\\n**Online Evaluation involves**\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Python SDK: Pass ScoreConfig instead of config_id when creating scores](https://github.com/orgs/langfuse/discussions/8623 \\\\\\\\\\\\\\\"Langfuse Ideas: Python SDK: Pass ScoreConfig instead of config_id when creating scores\\\\\\\\\\\\\\\")\\\\\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\\\\\\\\\"Langfuse Ideas: \\\\\\\\\\\\\\\\[Langfuse Cloud\\\\\\\\\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\\\\\\\\\")\\\\\\\\n[Score Configs: Allow editing the categories of a categorical score](https://github.com/orgs/langfuse/discussions/8259 \\\\\\\\\\\\\\\"Langfuse Ideas: Score Configs: Allow editing the categories of a categorical score\\\\\\\\\\\\\\\")\\\\\\\\n[UI-LLM as a Jury](https://github.com/orgs/langfuse/discussions/8195 \\\\\\\\\\\\\\\"Langfuse Ideas: UI-LLM as a Jury\\\\\\\\\\\\\\\")\\\\\\\\n[Enable Immediate Score Management for User Feedback](https://github.com/orgs/langfuse/discussions/7686 \\\\\\\\\\\\\\\"Langfuse Ideas: Enable Immediate Score Management for User Feedback\\\\\\\\\\\\\\\")\\\\\\\\n[Filter by scores in session view](https://github.com/orgs/langfuse/discussions/7528 \\\\\\\\\\\\\\\"Langfuse Ideas: Filter by scores in session view\\\\\\\\\\\\\\\")\\\\\\\\n[Support break lines on evaluation run tooltip hint](https://github.com/orgs/langfuse/discussions/7452 \\\\\\\\\\\\\\\"Langfuse Ideas: Support break lines on evaluation run tooltip hint\\\\\\\\\\\\\\\")\\\\\\\\n[Support new lines when storing / displaying score comments](https://github.com/orgs/langfuse/discussions/6473 \\\\\\\\\\\\\\\"Langfuse Ideas: Support new lines when storing / displaying score comments\\\\\\\\\\\\\\\")\\\\\\\\n[Evaluator: Filter for Scores](https://github.com/orgs/langfuse/discussions/6236 \\\\\\\\\\\\\\\"Langfuse Ideas: Evaluator: Filter for Scores\\\\\\\\\\\\\\\")\\\\\\\\n[Code-based custom evaluators](https://github.com/orgs/langfuse/discussions/6087 \\\\\\\\\\\\\\\"Langfuse Ideas: Code-based custom evaluators\\\\\\\\\\\\\\\")\\\\\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\\\\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Session-level scores](https://github.com/orgs/langfuse/discussions/2728 \\\\\\\\\\\\\\\"Langfuse Ideas: Session-level scores\\\\\\\\\\\\\\\")\\\\\\\\n[Scoring dataset runs, e.g. precision, recall, f-value](https://github.com/orgs/langfuse/discussions/2511 \\\\\\\\\\\\\\\"Langfuse Ideas: Scoring dataset runs, e.g. precision, recall, f-value\\\\\\\\\\\\\\\")\\\\\\\\n[Adding userId / author to score (custom metadata)](https://github.com/orgs/langfuse/discussions/2469 \\\\\\\\\\\\\\\"Langfuse Ideas: Adding userId / author to score \\\\\\\\\\\\\\\\(custom metadata\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[Add string data type in score config](https://github.com/orgs/langfuse/discussions/2402 \\\\\\\\\\\\\\\"Langfuse Ideas: Add string data type in score config\\\\\\\\\\\\\\\")\\\\\\\\n[API to delete scores](https://github.com/orgs/langfuse/discussions/1133 \\\\\\\\\\\\\\\"Langfuse Ideas: API to delete scores\\\\\\\\\\\\\\\")\\\\\\\\nGitHubSupportGitHubIdeas\\\\\\\\nUpvotes[GitHubNew](https://github.com/orgs/langfuse/discussions/new/choose)\\\\\\\\n  * 9votes\\\\\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486)\\\\\\\\nmsanand\\\\\\\\u20223/25/2024\\\\\\\\u2022\\\\\\\\n2Resolved\\\\\\\\n  * 4votes\\\\\\\\n[Filter Categorical Score Values](https://github.com/orgs/langfuse/discussions/3797)\\\\\\\\nalabrashJr\\\\\\\\u202210/17/2024\\\\\\\\u2022\\\\\\\\n3Resolved\\\\\\\\n  * 3votes\\\\\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590)\\\\\\\\nanuras\\\\\\\\u20228/18/2025\\\\\\\\u2022\\\\\\\\n1Resolved\\\\\\\\n  * 3votes\\\\\\\\n[Support for Metric Calculation (Precision@K, Recall@K) and Adding Custom Metrics Use Case Overview](https://github.com/orgs/langfuse/discussions/5215)\\\\\\\\nsrimantacse\\\\\\\\u20221/27/2025\\\\\\\\u2022\\\\\\\\n2Resolved\\\\\\\\n  * 2votes\\\\\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556)\\\\\\\\nflabbergastedbd\\\\\\\\u20228/15/2025\\\\\\\\u2022\\\\\\\\n1\\\\\\\\n  * 1votes\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}]}\\\",\\\"role\\\":\\\"assistant\\\",\\\"tool_calls\\\":null,\\\"function_call\\\":null}}],\\\"usage\\\":{\\\"prompt_tokens\\\":23,\\\"completion_tokens\\\":20274,\\\"total_tokens\\\":20297},\\\"system_fingerprint\\\":\\\"inkeep-fp-9a382b06-43e9-4c7a-b61b-7b35190c514a\\\",\\\"object\\\":\\\"chat.completion\\\",\\\"service_tier\\\":null,\\\"created\\\":1758264684},\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"{\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Troubleshooting and FAQ\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[API - /scores not respecting value when operator '='](https://github.com/orgs/langfuse/discussions/8770 \\\\\\\\\\\\\\\"Langfuse Support: API - /scores not respecting value when operator '='\\\\\\\\\\\\\\\")\\\\\\\\n[Does LangFuse support evaluations on an existing dataset (.csv)](https://github.com/orgs/langfuse/discussions/8665 \\\\\\\\\\\\\\\"Langfuse Support: Does LangFuse support evaluations on an existing dataset \\\\\\\\\\\\\\\\(.csv\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[deleting evaluators](https://github.com/orgs/langfuse/discussions/8640 \\\\\\\\\\\\\\\"Langfuse Support: deleting evaluators\\\\\\\\\\\\\\\")\\\\\\\\n[Not able to import my LLM-as-a-Judge evals](https://github.com/orgs/langfuse/discussions/8636 \\\\\\\\\\\\\\\"Langfuse Support: Not able to import my LLM-as-a-Judge evals\\\\\\\\\\\\\\\")\\\\\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590 \\\\\\\\\\\\\\\"Langfuse Support: How to get experiment run scores programmatically?\\\\\\\\\\\\\\\")\\\\\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556 \\\\\\\\\\\\\\\"Langfuse Support: Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset runs restore from backups](https://github.com/orgs/langfuse/discussions/8534 \\\\\\\\\\\\\\\"Langfuse Support: Dataset runs restore from backups\\\\\\\\\\\\\\\")\\\\\\\\n[Getting scores efficiently via API for analytics purposes](https://github.com/orgs/langfuse/discussions/8520 \\\\\\\\\\\\\\\"Langfuse Support: Getting scores efficiently via API for analytics purposes\\\\\\\\\\\\\\\")\\\\\\\\n[run experiment on dataset](https://github.com/orgs/langfuse/discussions/8433 \\\\\\\\\\\\\\\"Langfuse Support: run experiment on dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Experiments on Datasets with Human Annotated Labels?](https://github.com/orgs/langfuse/discussions/8414 \\\\\\\\\\\\\\\"Langfuse Support: Experiments on Datasets with Human Annotated Labels?\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[How to Recalculate Total Score on Dashboard After Updating User-Defined Model?](https://github.com/orgs/langfuse/discussions/8375 \\\\\\\\\\\\\\\"Langfuse Support: How to Recalculate Total Score on Dashboard After Updating User-Defined Model?\\\\\\\\\\\\\\\")\\\\\\\\n[How to filter by Categorical Scores in custom dashboard?](https://github.com/orgs/langfuse/discussions/8356 \\\\\\\\\\\\\\\"Langfuse Support: How to filter by Categorical Scores in custom dashboard?\\\\\\\\\\\\\\\")\\\\\\\\n[Running scheduled evals utilising LangFuse Datasets & Evaluators](https://github.com/orgs/langfuse/discussions/8355 \\\\\\\\\\\\\\\"Langfuse Support: Running scheduled evals utilising LangFuse Datasets & Evaluators\\\\\\\\\\\\\\\")\\\\\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\\\\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\\\\\\\\\(\\\\\\\\\\\\\\\\)` method\\\\\\\\\\\\\\\")\\\\\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\\\\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\\\\\\\\\")\\\\\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\\\\\\\\\"Langfuse Support: \\\\\\\\\\\\\\\\[Experiment\\\\\\\\\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\\\\\\\\\")\\\\\\\\n[Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\\\\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing\\\\\\\\\\\\\\\")\\\\\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\\\\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[2 traces generated instead of 1](https://github.com/orgs/langfuse/discussions/2244 \\\\\\\\\\\\\\\"Langfuse Support: 2 traces generated instead of 1\\\\\\\\\\\\\\\")\\\\\\\\n[Evaluations Not Available in Self-Hosted Version?](https://github.com/orgs/langfuse/discussions/2130 \\\\\\\\\\\\\\\"Langfuse Support: Evaluations Not Available in Self-Hosted Version?\\\\\\\\\\\\\\\")\\\\\\\\n[Deleting Duplicate Items in a Dataset](https://github.com/orgs/langfuse/discussions/2099 \\\\\\\\\\\\\\\"Langfuse Support: Deleting Duplicate Items in a Dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Availability of evals when self-hosting](https://github.com/orgs/langfuse/discussions/2042 \\\\\\\\\\\\\\\"Langfuse Support: Availability of evals when self-hosting\\\\\\\\\\\\\\\")\\\\\\\\n[How to utilize a dataset w/ typescript and langchain integration](https://github.com/orgs/langfuse/discussions/1969 \\\\\\\\\\\\\\\"Langfuse Support: How to utilize a dataset w/ typescript and langchain integration\\\\\\\\\\\\\\\")\\\\\\\\n[Scoring a trace after the LLM chain returns](https://github.com/orgs/langfuse/discussions/1610 \\\\\\\\\\\\\\\"Langfuse Support: Scoring a trace after the LLM chain returns\\\\\\\\\\\\\\\")\\\\\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486 \\\\\\\\\\\\\\\"Langfuse Support: Update/delete score using python sdk\\\\\\\\\\\\\\\")\\\\\\\\n[Linking dataset run items with existing callback handler](https://github.com/orgs/langfuse/discussions/1445 \\\\\\\\\\\\\\\"Langfuse Support: Linking dataset run items with existing callback handler\\\\\\\\\\\\\\\")\\\\\\\\n[Datasets list / by id](https://github.com/orgs/langfuse/discussions/1420 \\\\\\\\\\\\\\\"Langfuse Support: Datasets list / by id\\\\\\\\\\\\\\\")\\\\\\\\n[Run items not appearing when linking to a trace and not a span or a generation](https://github.com/orgs/langfuse/discussions/1357 \\\\\\\\\\\\\\\"Langfuse Support: Run items not appearing when linking to a trace and not a span or a generation\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Option to run Experiments without traces](https://github.com/orgs/langfuse/discussions/8133 \\\\\\\\\\\\\\\"Langfuse Ideas: Option to run Experiments without traces\\\\\\\\\\\\\\\")\\\\\\\\n[Auto-generate dataset items](https://github.com/orgs/langfuse/discussions/8126 \\\\\\\\\\\\\\\"Langfuse Ideas: Auto-generate dataset items\\\\\\\\\\\\\\\")\\\\\\\\n[feature: support for creation of custom) model adapters](https://github.com/orgs/langfuse/discussions/8123 \\\\\\\\\\\\\\\"Langfuse Ideas: feature: support for creation of custom\\\\\\\\\\\\\\\\) model adapters\\\\\\\\\\\\\\\")\\\\\\\\n[Prompt experiment result download.](https://github.com/orgs/langfuse/discussions/8120 \\\\\\\\\\\\\\\"Langfuse Ideas: Prompt experiment result download.\\\\\\\\\\\\\\\")\\\\\\\\n[Multiple predictions per-item in a single run](https://github.com/orgs/langfuse/discussions/8040 \\\\\\\\\\\\\\\"Langfuse Ideas: Multiple predictions per-item in a single run\\\\\\\\\\\\\\\")\\\\\\\\n[Add evaluator(s) to python SDK (FernLangfuse)](https://github.com/orgs/langfuse/discussions/8018 \\\\\\\\\\\\\\\"Langfuse Ideas: Add evaluator\\\\\\\\\\\\\\\\(s\\\\\\\\\\\\\\\\) to python SDK \\\\\\\\\\\\\\\\(FernLangfuse\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[feat(dataset-runs): return all dataset run item scores in a given run](https://github.com/orgs/langfuse/discussions/8011 \\\\\\\\\\\\\\\"Langfuse Ideas: feat\\\\\\\\\\\\\\\\(dataset-runs\\\\\\\\\\\\\\\\): return all dataset run item scores in a given run\\\\\\\\\\\\\\\")\\\\\\\\n[Delete an evaluator from the evaluator library](https://github.com/orgs/langfuse/discussions/7960 \\\\\\\\\\\\\\\"Langfuse Ideas: Delete an evaluator from the evaluator library\\\\\\\\\\\\\\\")\\\\\\\\n[Archive or Delete Evaluators from the Evaluator Library](https://github.com/orgs/langfuse/discussions/7868 \\\\\\\\\\\\\\\"Langfuse Ideas: Archive or Delete Evaluators from the Evaluator Library\\\\\\\\\\\\\\\")\\\\\\\\n[Edit dataset run name, description](https://github.com/orgs/langfuse/discussions/7814 \\\\\\\\\\\\\\\"Langfuse Ideas: Edit dataset run name, description\\\\\\\\\\\\\\\")\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Datasets\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Datasets - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")Dataset RunsDatasets\\\\\\\\nCopy as Markdown\\\\\\\\n# Datasets\\\\\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\\\\\n## Why use datasets?\\\\\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\\\\\n  * Create test cases for your application with real production traces\\\\\\\\n  * Collaboratively create and collect dataset items with your team\\\\\\\\n  * Have a single source of truth for your test data\\\\\\\\n## Get Started\\\\\\\\n### Creating a dataset\\\\\\\\nDatasets have a name which is unique within a project.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset(\\\\\\\\n name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # optional description\\\\\\\\n description=\\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n # optional metadata\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"author\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"date\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { LangfuseClient } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\nconst langfuse = new LangfuseClient()\\\\\\\\n \\\\\\\\nawait langfuse.api.datasets.create({\\\\\\\\n name: \\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n // optional description\\\\\\\\n description: \\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n // optional metadata\\\\\\\\n metadata: {\\\\\\\\n author: \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n date: \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n type: \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\",\\\\\\\\n },\\\\\\\\n});\\\\\\\\n```\\\\\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\\\\\n### Create new dataset items\\\\\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset_item(\\\\\\\\n dataset_name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # any python object or value, optional\\\\\\\\n input={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # any python object or value, optional\\\\\\\\n expected_output={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # metadata, optional\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"llama3\\\\\\\\\\\\\\\",\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/observability/sdk/python/evaluation\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Evaluations\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Evaluations with the Langfuse Python SDK - Langfuse\\\\\\\\nDocs[Observability](/docs/observability/overview \\\\\\\\\\\\\\\"Observability\\\\\\\\\\\\\\\")\\\\\\\\n[SDKs](/docs/observability/sdk/overview \\\\\\\\\\\\\\\"SDKs\\\\\\\\\\\\\\\")\\\\\\\\n[Python](/docs/observability/sdk/python/overview \\\\\\\\\\\\\\\"Python\\\\\\\\\\\\\\\")Evaluation\\\\\\\\nCopy as Markdown\\\\\\\\n# Evaluations\\\\\\\\nThe Python SDK provides ways to evaluate your application. You can add custom scores to your traces and observations, or use the SDK to execute Dataset Runs.\\\\\\\\nThis page shows the evaluation methods that are supported by the Python SDK. Please refer to the [Evaluation documentation](/docs/evaluation/overview) for more information on how to evaluate your application in Langfuse.\\\\\\\\n## Create Scores\\\\\\\\nLangfuseSpan / LangfuseGeneration object methodsContext-aware methodsLow-level method\\\\\\\\n  * `span_or_generation_obj.score()`: Scores the specific observation object.\\\\\\\\n  * `span_or_generation_obj.score_trace()`: Scores the entire trace to which the object belongs.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nwith langfuse.start_as_current_generation(name=\\\\\\\\\\\\\\\"summary_generation\\\\\\\\\\\\\\\") as gen:\\\\\\\\n # ... LLM call ...\\\\\\\\n gen.update(output=\\\\\\\\\\\\\\\"summary text...\\\\\\\\\\\\\\\")\\\\\\\\n # Score this specific generation\\\\\\\\n gen.score(name=\\\\\\\\\\\\\\\"conciseness\\\\\\\\\\\\\\\", value=0.8, data_type=\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\")\\\\\\\\n # Score the overall trace\\\\\\\\n gen.score_trace(name=\\\\\\\\\\\\\\\"user_feedback_rating\\\\\\\\\\\\\\\", value=\\\\\\\\\\\\\\\"positive\\\\\\\\\\\\\\\", data_type=\\\\\\\\\\\\\\\"CATEGORICAL\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\n  * `langfuse.score_current_span()`: Scores the currently active observation in the context.\\\\\\\\n  * `langfuse.score_current_trace()`: Scores the trace of the currently active observation.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nwith langfuse.start_as_current_span(name=\\\\\\\\\\\\\\\"complex_task\\\\\\\\\\\\\\\") as task_span:\\\\\\\\n # ... perform task ...\\\\\\\\n langfuse.score_current_span(name=\\\\\\\\\\\\\\\"task_component_quality\\\\\\\\\\\\\\\", value=True, data_type=\\\\\\\\\\\\\\\"BOOLEAN\\\\\\\\\\\\\\\")\\\\\\\\n # ...\\\\\\\\n if task_is_fully_successful:\\\\\\\\n langfuse.score_current_trace(name=\\\\\\\\\\\\\\\"overall_success\\\\\\\\\\\\\\\", value=1.0, data_type=\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n  * Creates a score for a specified `trace_id` and optionally `observation_id`.\\\\\\\\n  * Useful when IDs are known, or for scoring after the trace/observation has completed.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nlangfuse.create_score(\\\\\\\\n name=\\\\\\\\\\\\\\\"fact_check_accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value=0.95, ///# Can be float for NUMERIC/BOOLEAN, string for CATEGORICAL\\\\\\\\n trace_id=\\\\\\\\\\\\\\\"abcdef1234567890abcdef1234567890\\\\\\\\\\\\\\\",\\\\\\\\n observation_id=\\\\\\\\\\\\\\\"1234567890abcdef\\\\\\\\\\\\\\\", ///# Optional: if scoring a specific observation\\\\\\\\n session_id=\\\\\\\\\\\\\\\"session_123\\\\\\\\\\\\\\\", ///# Optional: if scoring a specific session\\\\\\\\n data_type=\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\", ///# \\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"BOOLEAN\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"CATEGORICAL\\\\\\\\\\\\\\\"\\\\\\\\n comment=\\\\\\\\\\\\\\\"Source verified for 95% of claims.\\\\\\\\\\\\\\\"\\\\\\\\n)\\\\\\\\n```\\\\\\\\n**Score Parameters:**\\\\\\\\nParameter| Type| Description  \\\\\\\\n---|---|---  \\\\\\\\n`name`| `str`| Name of the score (e.g., \\\\\\\\u201crelevance\\\\\\\\u201d, \\\\\\\\u201caccuracy\\\\\\\\u201d). **Required.**  \\\\\\\\n`value`| `Union[float, str]`| Score value. Float for `NUMERIC`/`BOOLEAN`, string for `CATEGORICAL`. **Required.**  \\\\\\\\n`trace_id`| `str`| ID of the trace to associate with (for `create_score`). **Required.**  \\\\\\\\n`observation_id`| `Optional[str]`| ID of the specific observation to score (for `create_score`).  \\\\\\\\n`session_id`| `Optional[str]`| ID of the specific session to score (for `create_score`).  \\\\\\\\n`score_id`| `Optional[str]`| Custom ID for the score (auto-generated if None).  \\\\\\\\n`data_type`| `Optional[ScoreDataType]`| `\\\\\\\\\\\\\\\"NUMERIC\\\\\\\\\\\\\\\"`, `\\\\\\\\\\\\\\\"BOOLEAN\\\\\\\\\\\\\\\"`, or `\\\\\\\\\\\\\\\"CATEGORICAL\\\\\\\\\\\\\\\"`. Inferred if not provided based on value type and score config on server.  \\\\\\\\n`comment`| `Optional[str]`| Optional comment or explanation for the score.  \\\\\\\\n`config_id`| `Optional[str]`| Optional ID of a pre-defined score configuration in Langfuse.  \\\\\\\\n  \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThe most powerful way to use datasets is by linking your application\\\\\\\\u2019s executions (traces) to specific dataset items when performing an evaluation run. The `DatasetItemClient.run()` method provides a context manager to streamline this process.\\\\\\\\n**How`item.run()` works:**\\\\\\\\nWhen you use `with item.run(run_name=\\\\\\\\\\\\\\\"your_eval_run_name\\\\\\\\\\\\\\\") as root_span:`:\\\\\\\\n  1. **Trace Creation** : A new Langfuse trace is initiated specifically for processing this dataset item within the context of the named run.\\\\\\\\n  2. **Trace Naming & Metadata**: \\\\\\\\n    * The trace is automatically named (e.g., \\\\\\\\u201cDataset run: your_eval_run_name\\\\\\\\u201d).\\\\\\\\n    * Essential metadata is added to this trace, including `dataset_item_id` (the ID of `item`), `run_name`, and `dataset_id`.\\\\\\\\n  3. **DatasetRunItem Linking** : The SDK makes an API call to Langfuse to create a `DatasetRunItem`. This backend object formally links: \\\\\\\\n    * The `dataset_item_id`\\\\\\\\n    * The `trace_id` of the newly created trace\\\\\\\\n    * The provided `run_name`\\\\\\\\n    * Any `run_metadata` or `run_description` you pass to `item.run()`. This linkage is what populates the \\\\\\\\u201cRuns\\\\\\\\u201d tab for your dataset in the Langfuse UI, allowing you to see all traces associated with a particular evaluation run.\\\\\\\\n  4. **Contextual Span** : The context manager yields `root_span`, which is a `LangfuseSpan` object representing the root span of this new trace.\\\\\\\\n  5. **Automatic Nesting** : Any Langfuse observations (spans or generations) created _inside_ the `with` block will automatically become children of `root_span` and thus part of the trace linked to this dataset item and run.\\\\\\\\n**Example:**\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\ndataset_name = \\\\\\\\\\\\\\\"qna-eval\\\\\\\\\\\\\\\"\\\\\\\\ncurrent_run_name = \\\\\\\\\\\\\\\"qna_model_v3_run_05_20\\\\\\\\\\\\\\\" # Identifies this specific evaluation run\\\\\\\\n \\\\\\\\n///# Assume 'my_qna_app' is your instrumented application function\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"def my_qna_app(question: str, context: str, item_id: str, run_name: str):\\\\\\\\n with langfuse.start_as_current_generation(\\\\\\\\n name=\\\\\\\\\\\\\\\"qna-llm-call\\\\\\\\\\\\\\\",\\\\\\\\n input={\\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\": question, \\\\\\\\\\\\\\\"context\\\\\\\\\\\\\\\": context},\\\\\\\\n metadata={\\\\\\\\\\\\\\\"item_id\\\\\\\\\\\\\\\": item_id, \\\\\\\\\\\\\\\"run\\\\\\\\\\\\\\\": run_name}, ///# Example metadata for the generation\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\"\\\\\\\\n ) as generation:\\\\\\\\n # Simulate LLM call\\\\\\\\n answer = f\\\\\\\\\\\\\\\"Answer to '{question}' using context.\\\\\\\\\\\\\\\" # Replace with actual LLM call\\\\\\\\n generation.update(output={\\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer})\\\\\\\\n \\\\\\\\n # Update the trace with the input and output\\\\\\\\n generation.update_trace(\\\\\\\\n input={\\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\": question, \\\\\\\\\\\\\\\"context\\\\\\\\\\\\\\\": context},\\\\\\\\n output={\\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\": answer},\\\\\\\\n )\\\\\\\\n \\\\\\\\n return answer\\\\\\\\n \\\\\\\\ndataset = langfuse.get_dataset(name=dataset_name) ///# Fetch your pre-populated dataset\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n print(f\\\\\\\\\\\\\\\"Running evaluation for item: {item.id} (Input: {item.input})\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # Use the item.run() context manager\\\\\\\\n with item.run(\\\\\\\\n run_name=current_run_name,\\\\\\\\n run_metadata={\\\\\\\\\\\\\\\"model_provider\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"OpenAI\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"temperature_setting\\\\\\\\\\\\\\\": 0.7},\\\\\\\\n run_description=\\\\\\\\\\\\\\\"Evaluation run for Q&A model v3 on May 20th\\\\\\\\\\\\\\\"\\\\\\\\n ) as root_span: ///# root_span is the root span of the new trace for this item and run.\\\\\\\\n # All subsequent langfuse operations within this block are part of this trace.\\\\\\\\n \\\\\\\\n # Call your application logic\\\\\\\\n generated_answer = my_qna_app(\\\\\\\\n question=item.input[\\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\"],\\\\\\\\n context=item.input[\\\\\\\\\\\\\\\"context\\\\\\\\\\\\\\\"],\\\\\\\\n item_id=item.id,\\\\\\\\n run_name=current_run_name\\\\\\\\n )\\\\\\\\n \\\\\\\\n print(f\\\\\\\\\\\\\\\" Item {item.id} processed. Trace ID: {root_span.trace_id}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # Optionally, score the result against the expected output\\\\\\\\n if item.expected_output and generated_answer == item.expected_output.get(\\\\\\\\\\\\\\\"answer\\\\\\\\\\\\\\\"):\\\\\\\\n root_span.score_trace(name=\\\\\\\\\\\\\\\"exact_match\\\\\\\\\\\\\\\", value=1.0)\\\\\\\\n else:\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n root_span.score_trace(name=\\\\\\\\\\\\\\\"exact_match\\\\\\\\\\\\\\\", value=0.0)\\\\\\\\n \\\\\\\\nprint(f\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\\nFinished processing dataset '{dataset_name}' for run '{current_run_name}'.\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nBy using `item.run()`, you ensure each dataset item\\\\\\\\u2019s processing is neatly encapsulated in its own trace, and these traces are aggregated under the specified `run_name` in the Langfuse UI. This allows for systematic review of results, comparison across runs, and deep dives into individual processing traces.\\\\\\\\nLast updated on August 20, 2025\\\\\\\\n[Instrumentation](/docs/observability/sdk/python/instrumentation \\\\\\\\\\\\\\\"Instrumentation\\\\\\\\\\\\\\\")\\\\\\\\n[Advanced usage](/docs/observability/sdk/python/advanced-usage \\\\\\\\\\\\\\\"Advanced usage\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"All new Datasets, Experimentation and Evaluation documentation\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"## New Datasets and Evaluation documentation\\\\\\\\nBased on this week\\\\\\\\u2019s changes, we have have restructured the documentation to:\\\\\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\\\\\n### Evaluation\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Evaluation documentation](/docs/scores)\\\\\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nPOST /api/public/scores\\\\\\\\n```\\\\\\\\nAdd scores via Python or JS SDK.\\\\\\\\nExample (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.score(\\\\\\\\n trace_id=\\\\\\\\\\\\\\\"123\\\\\\\\\\\\\\\",\\\\\\\\n name=\\\\\\\\\\\\\\\"my_custom_evaluator\\\\\\\\\\\\\\\",\\\\\\\\n value=0.5,\\\\\\\\n)\\\\\\\\n```\\\\\\\\n### Datasets\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Remote Dataset Runs\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Task function that answers geography questions\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n question = item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]\\\\\\\\n response = OpenAI().chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": question}]\\\\\\\\n )\\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n if expected_output and expected_output.lower() in output.lower():\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=1.0)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=0.0)\\\\\\\\n \\\\\\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n accuracies = [\\\\\\\\n eval.value for result in item_results\\\\\\\\n for eval in result.evaluations if eval.name == \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\"\\\\\\\\n ]\\\\\\\\n \\\\\\\\n if not accuracies:\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=None)\\\\\\\\n \\\\\\\\n avg = sum(accuracies) / len(accuracies)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=avg, comment=f\\\\\\\\\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n@pytest.fixture\\\\\\\\ndef langfuse_client():\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Initialize Langfuse client for testing\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n return get_client()\\\\\\\\n \\\\\\\\ndef test_geography_accuracy_passes(langfuse_client):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Test that passes when accuracy is above threshold\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n result = langfuse_client.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Geography Test - Should Pass\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=geography_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy_evaluator]\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Access the run evaluator result directly\\\\\\\\n avg_accuracy = next(\\\\\\\\n eval.value for eval in result.run_evaluations\\\\\\\\n if eval.name == \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\"\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Assert minimum accuracy threshold\\\\\\\\n assert avg_accuracy >= 0.8, f\\\\\\\\\\\\\\\"Average accuracy {avg_accuracy:.2f} below threshold 0.8\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\ndef test_geography_accuracy_fails(langfuse_client):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Example test that demonstrates failure conditions\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n # Use a weaker model or harder questions to demonstrate test failure\\\\\\\\n def failing_task(*, item, **kwargs):\\\\\\\\n # Simulate a task that gives wrong answers\\\\\\\\n return \\\\\\\\\\\\\\\"I don't know\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n result = langfuse_client.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Geography Test - Should Fail\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=failing_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy_evaluator]\\\\\\\\n )\\\\\\\\n \\\\\\\\n # Access the run evaluator result directly\\\\\\\\n avg_accuracy = next(\\\\\\\\n eval.value for eval in result.run_evaluations\\\\\\\\n if eval.name == \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\"\\\\\\\\n )\\\\\\\\n \\\\\\\\n # This test will fail because the task gives wrong answers\\\\\\\\n with pytest.raises(AssertionError):\\\\\\\\n assert avg_accuracy >= 0.8, f\\\\\\\\\\\\\\\"Expected test to fail with low accuracy: {avg_accuracy:.2f}\\\\\\\\\\\\\\\"\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n// test/geography-experiment.test.ts\\\\\\\\nimport { describe, it, expect, beforeAll, afterAll } from 'vitest';\\\\\\\\nimport { OpenAI } from \\\\\\\\\\\\\\\"openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { NodeSDK } from \\\\\\\\\\\\\\\"@opentelemetry/sdk-node\\\\\\\\\\\\\\\";\\\\\\\\nimport { LangfuseClient, ExperimentItem } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\nimport { observeOpenAI } from \\\\\\\\\\\\\\\"@langfuse/openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { LangfuseSpanProcessor } from \\\\\\\\\\\\\\\"@langfuse/otel\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Test data for European capitals\\\\\\\\nconst testData: ExperimentItem[] = [\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\", expected: \\\\\\\\\\\\\\\"Paris\\\\\\\\\\\\\\\" },\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\", expected: \\\\\\\\\\\\\\\"Berlin\\\\\\\\\\\\\\\" },\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of Spain?\\\\\\\\\\\\\\\", expected: \\\\\\\\\\\\\\\"Madrid\\\\\\\\\\\\\\\" },\\\\\\\\n];\\\\\\\\n \\\\\\\\nlet otelSdk: NodeSDK;\\\\\\\\nlet langfuse: LangfuseClient;\\\\\\\\n \\\\\\\\nbeforeAll(async () => {\\\\\\\\n // Initialize OpenTelemetry\\\\\\\\n otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\\\\\n otelSdk.start();\\\\\\\\n \\\\\\\\n // Initialize Langfuse client\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { Factuality, Levenshtein } from \\\\\\\\\\\\\\\"autoevals\\\\\\\\\\\\\\\";\\\\\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\\\\\n \\\\\\\\n// Use with additional parameters\\\\\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\\\\\n Factuality,\\\\\\\\n { model: \\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\" } // Additional AutoEvals parameters\\\\\\\\n);\\\\\\\\n \\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"AutoEvals Integration Test\\\\\\\\\\\\\\\",\\\\\\\\n data: testDataset,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [\\\\\\\\n factualityEvaluator,\\\\\\\\n levenshteinEvaluator,\\\\\\\\n customFactualityEvaluator\\\\\\\\n ]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Remote Dataset Runs\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Remote Dataset Runs via the SDK - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\\\\\\\\\"Dataset Runs\\\\\\\\\\\\\\\")Remote Run\\\\\\\\nCopy as Markdown\\\\\\\\n# Remote Dataset Runs\\\\\\\\nOnce you [created a dataset](/docs/evaluation/dataset-runs/datasets), you can use the dataset to test how your application performs on different inputs. Remote Dataset Runs are used to programmatically loop your applications or prompts through a dataset and optionally apply Evaluation Methods to the results.\\\\\\\\nThey are called \\\\\\\\u201cRemote Dataset Runs\\\\\\\\u201d because they can make use of \\\\\\\\u201cremote\\\\\\\\u201d or external logic and code.\\\\\\\\nOptionally, you can also trigger Remote Dataset Runs via the Langfuse UI which will call them via a webhook.\\\\\\\\n## Why use Remote Dataset Runs?\\\\\\\\n  * Full flexibility to use your own application logic\\\\\\\\n  * Use custom scoring functions to evaluate the outputs\\\\\\\\n  * Run multiple experiments on the same dataset in parallel\\\\\\\\n  * Easy to integrate with your existing evaluation infrastructure\\\\\\\\nSequence Diagram\\\\\\\\n## Experiment runner SDK\\\\\\\\nBoth the Python and JS/TS SDKs provide a high-level abstraction for running an experiment on a dataset. The dataset can be both local or hosted on Langfuse. Using the Experiment runner is the recommended way to run an experiment on a dataset with our SDK.\\\\\\\\nThe experiment runner automatically handles:\\\\\\\\n  * **Concurrent execution** of tasks with configurable limits\\\\\\\\n  * **Automatic tracing** of all executions for observability\\\\\\\\n  * **Flexible evaluation** with both item-level and run-level evaluators\\\\\\\\n  * **Error isolation** so individual failures don\\\\\\\\u2019t stop the experiment\\\\\\\\n  * **Dataset integration** for easy comparison and tracking\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThe experiment runner SDK supports both datasets hosted on Langfuse and datasets hosted locally. If you are using a dataset hosted on Langfuse for your experiment, the SDK will automatically create a dataset run for you that you can inspect and compare in the Langfuse UI. For locally hosted datasets not on Langfuse, only traces and scores (if evaluations are used) are tracked in Langfuse.\\\\\\\\n### Basic Usage\\\\\\\\nStart with the simplest possible experiment to test your task function on local data. If you already have a dataset in Langfuse, see here.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\nfrom langfuse.openai import OpenAI\\\\\\\\n \\\\\\\\n///# Initialize client\\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\n///# Define your task function\\\\\\\\ndef my_task(*, item, **kwargs):\\\\\\\\n question = item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]\\\\\\\\n response = OpenAI().chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4.1\\\\\\\\\\\\\\\", messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": question}]\\\\\\\\n )\\\\\\\\n \\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\n \\\\\\\\n///# Run experiment on local data\\\\\\\\nlocal_data = [\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\"},\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\"},\\\\\\\\n]\\\\\\\\n \\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Geography Quiz\\\\\\\\\\\\\\\",\\\\\\\\n description=\\\\\\\\\\\\\\\"Testing basic functionality\\\\\\\\\\\\\\\",\\\\\\\\n data=local_data,\\\\\\\\n task=my_task,\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# Use format method to display results\\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\nMake sure that OpenTelemetry is properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { OpenAI } from \\\\\\\\\\\\\\\"openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { NodeSDK } from \\\\\\\\\\\\\\\"@opentelemetry/sdk-node\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\nimport {\\\\\\\\n LangfuseClient,\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n ExperimentTask,\\\\\\\\n ExperimentItem,\\\\\\\\n} from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\nimport { observeOpenAI } from \\\\\\\\\\\\\\\"@langfuse/openai\\\\\\\\\\\\\\\";\\\\\\\\nimport { LangfuseSpanProcessor } from \\\\\\\\\\\\\\\"@langfuse/otel\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Initialize OpenTelemetry\\\\\\\\nconst otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\\\\\notelSdk.start();\\\\\\\\n \\\\\\\\n// Initialize client\\\\\\\\nconst langfuse = new LangfuseClient();\\\\\\\\n \\\\\\\\n// Run experiment on local data\\\\\\\\nconst localData: ExperimentItem[] = [\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\" },\\\\\\\\n { input: \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\" },\\\\\\\\n];\\\\\\\\n \\\\\\\\n// Define your task function\\\\\\\\nconst myTask: ExperimentTask = async (item) => {\\\\\\\\n const question = item.input;\\\\\\\\n \\\\\\\\n const response = await observeOpenAI(new OpenAI()).chat.completions.create({\\\\\\\\n model: \\\\\\\\\\\\\\\"gpt-4.1\\\\\\\\\\\\\\\",\\\\\\\\n messages: [\\\\\\\\n {\\\\\\\\n role: \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\",\\\\\\\\n content: question,\\\\\\\\n },\\\\\\\\n ],\\\\\\\\n });\\\\\\\\n \\\\\\\\n return response;\\\\\\\\n};\\\\\\\\n \\\\\\\\n// Run the experiment\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Geography Quiz\\\\\\\\\\\\\\\",\\\\\\\\n description: \\\\\\\\\\\\\\\"Testing basic functionality\\\\\\\\\\\\\\\",\\\\\\\\n data: localData,\\\\\\\\n task: myTask,\\\\\\\\n});\\\\\\\\n \\\\\\\\n// Print formatted result\\\\\\\\nconsole.log(await result.format());\\\\\\\\n \\\\\\\\n// Important: shut down OTEL SDK to deliver traces\\\\\\\\nawait otelSdk.shutdown();\\\\\\\\n```\\\\\\\\n **Note for JS/TS SDK** : OpenTelemetry must be properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\\\\\nWhen running experiments on local data, only traces are created in Langfuse - no dataset runs are generated. Each task execution creates an individual trace for observability and debugging.\\\\\\\\n### Usage with Langfuse Datasets\\\\\\\\nRun experiments directly on datasets stored in Langfuse for automatic tracing and comparison.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n///# Get dataset from Langfuse\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"my-evaluation-dataset\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n///# Run experiment directly on the dataset\\\\\\\\nresult = dataset.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Production Model Test\\\\\\\\\\\\\\\",\\\\\\\\n description=\\\\\\\\\\\\\\\"Monthly evaluation of our production model\\\\\\\\\\\\\\\",\\\\\\\\n task=my_task\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# Use format method to display results\\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n// Get dataset from Langfuse\\\\\\\\nconst dataset = await langfuse.dataset.get(\\\\\\\\\\\\\\\"my-evaluation-dataset\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\n// Run experiment directly on the dataset\\\\\\\\nconst result = await dataset.runExperiment({\\\\\\\\n name: \\\\\\\\\\\\\\\"Production Model Test\\\\\\\\\\\\\\\",\\\\\\\\n description: \\\\\\\\\\\\\\\"Monthly evaluation of our production model\\\\\\\\\\\\\\\",\\\\\\\\n task: myTask\\\\\\\\n});\\\\\\\\n \\\\\\\\n// Use format method to display results\\\\\\\\nconsole.log(await result.format());\\\\\\\\n \\\\\\\\n// Important: shut down OpenTelemetry to ensure traces are sent to Langfuse\\\\\\\\nawait otelSdk.shutdown()\\\\\\\\n```\\\\\\\\nWhen using Langfuse datasets, dataset runs are automatically created in Langfuse and are available for comparison in the UI. This enables tracking experiment performance over time and comparing different approaches on the same dataset.\\\\\\\\n### Advanced Features\\\\\\\\nEnhance your experiments with evaluators and advanced configuration options.\\\\\\\\n#### Evaluators\\\\\\\\nEvaluators assess the quality of task outputs at the item level. They receive the input, metadata, output, and expected output for each item and return evaluation metrics that are reported as scores on the traces in Langfuse.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import Evaluation\\\\\\\\n \\\\\\\\n///# Define evaluation functions\\\\\\\\ndef accuracy_evaluator(*, input, output, expected_output, metadata, **kwargs):\\\\\\\\n if expected_output and expected_output.lower() in output.lower():\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=1.0, comment=\\\\\\\\\\\\\\\"Correct answer found\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=0.0, comment=\\\\\\\\\\\\\\\"Incorrect answer\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\ndef length_evaluator(*, input, output, **kwargs):\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\", value=len(output), comment=f\\\\\\\\\\\\\\\"Response has {len(output)} characters\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n///# Use multiple evaluators\\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Multi-metric Evaluation\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[accuracy_evaluator, length_evaluator]\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n// Define evaluation functions\\\\\\\\nconst accuracyEvaluator = async ({ input, output, expectedOutput }) => {\\\\\\\\n if (expectedOutput && output.toLowerCase().includes(expectedOutput.toLowerCase())) {\\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value: 1.0,\\\\\\\\n comment: \\\\\\\\\\\\\\\"Correct answer found\\\\\\\\\\\\\\\"\\\\\\\\n };\\\\\\\\n }\\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value: 0.0,\\\\\\\\n comment: \\\\\\\\\\\\\\\"Incorrect answer\\\\\\\\\\\\\\\"\\\\\\\\n };\\\\\\\\n};\\\\\\\\n \\\\\\\\nconst lengthEvaluator = async ({ input, output }) => {\\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"response_length\\\\\\\\\\\\\\\",\\\\\\\\n value: output.length,\\\\\\\\n comment: `Response has ${output.length} characters`\\\\\\\\n };\\\\\\\\n};\\\\\\\\n \\\\\\\\n// Use multiple evaluators\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Multi-metric Evaluation\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [accuracyEvaluator, lengthEvaluator]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Run-level Evaluators\\\\\\\\nRun-level evaluators assess the full experiment results and compute aggregate metrics. When run on Langfuse datasets, these scores are attached to the full dataset run for tracking overall experiment performance.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import Evaluation\\\\\\\\n \\\\\\\\ndef average_accuracy(*, item_results, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Calculate average accuracy across all items\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n accuracies = [\\\\\\\\n eval.value for result in item_results\\\\\\\\n for eval in result.evaluations\\\\\\\\n if eval.name == \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\"\\\\\\\\n ]\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n if not accuracies:\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=None)\\\\\\\\n \\\\\\\\n avg = sum(accuracies) / len(accuracies)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=avg, comment=f\\\\\\\\\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Comprehensive Analysis\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy]\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst averageAccuracy = async ({ itemResults }) => {\\\\\\\\n // Calculate average accuracy across all items\\\\\\\\n const accuracies = itemResults\\\\\\\\n .flatMap(result => result.evaluations)\\\\\\\\n .filter(evaluation => evaluation.name === \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\")\\\\\\\\n .map(evaluation => evaluation.value as number);\\\\\\\\n \\\\\\\\n if (accuracies.length === 0) {\\\\\\\\n return { name: \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value: null };\\\\\\\\n }\\\\\\\\n \\\\\\\\n const avg = accuracies.reduce((sum, val) => sum + val, 0) / accuracies.length;\\\\\\\\n \\\\\\\\n return {\\\\\\\\n name: \\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\",\\\\\\\\n value: avg,\\\\\\\\n comment: `Average accuracy: ${(avg * 100).toFixed(1)}%`\\\\\\\\n };\\\\\\\\n};\\\\\\\\n \\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Comprehensive Analysis\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [accuracyEvaluator],\\\\\\\\n runEvaluators: [averageAccuracy]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Async Tasks and Evaluators\\\\\\\\nBoth task functions and evaluators can be asynchronous.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport asyncio\\\\\\\\nfrom langfuse.openai import AsyncOpenAI\\\\\\\\n \\\\\\\\nasync def async_llm_task(*, item, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Async task using OpenAI\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n client = AsyncOpenAI()\\\\\\\\n response = await client.chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]}]\\\\\\\\n )\\\\\\\\n \\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\n///# Works seamlessly with async functions\\\\\\\\nresult = langfuse.run_experiment(\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n name=\\\\\\\\\\\\\\\"Async Experiment\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=async_llm_task,\\\\\\\\n max_concurrency=5 # Control concurrent API calls\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport OpenAI from \\\\\\\\\\\\\\\"openai\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\nconst asyncLlmTask = async (item) => {\\\\\\\\n // Async task using OpenAI\\\\\\\\n const client = new OpenAI();\\\\\\\\n const response = await client.chat.completions.create({\\\\\\\\n model: \\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages: [{ role: \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", content: item.input }]\\\\\\\\n });\\\\\\\\n \\\\\\\\n return response.choices[0].message.content;\\\\\\\\n};\\\\\\\\n \\\\\\\\n// Works seamlessly with async functions\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Async Experiment\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: asyncLlmTask,\\\\\\\\n maxConcurrency: 5 // Control concurrent API calls\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Configuration Options\\\\\\\\nCustomize experiment behavior with various configuration options.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Configurable Experiment\\\\\\\\\\\\\\\",\\\\\\\\n run_name=\\\\\\\\\\\\\\\"Custom Run Name\\\\\\\\\\\\\\\", ///# will be dataset run name if dataset is used\\\\\\\\n description=\\\\\\\\\\\\\\\"Experiment with custom configuration\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[accuracy_evaluator],\\\\\\\\n run_evaluators=[average_accuracy],\\\\\\\\n max_concurrency=10, ///# Max concurrent executions\\\\\\\\n metadata={ ///# Attached to all traces\\\\\\\\n \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"temperature\\\\\\\\\\\\\\\": 0.7,\\\\\\\\n \\\\\\\\\\\\\\\"version\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"v1.2.0\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"Configurable Experiment\\\\\\\\\\\\\\\",\\\\\\\\n runName: \\\\\\\\\\\\\\\"Custom Run Name\\\\\\\\\\\\\\\", // will be dataset run name if dataset is used\\\\\\\\n description: \\\\\\\\\\\\\\\"Experiment with custom configuration\\\\\\\\\\\\\\\",\\\\\\\\n data: testData,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [accuracyEvaluator],\\\\\\\\n runEvaluators: [averageAccuracy],\\\\\\\\n maxConcurrency: 10, // Max concurrent executions\\\\\\\\n metadata: { // Attached to all traces\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n model: \\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n temperature: 0.7,\\\\\\\\n version: \\\\\\\\\\\\\\\"v1.2.0\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n#### Testing in CI Environments\\\\\\\\nIntegrate the experiment runner with testing frameworks like Pytest and Vitest to run automated evaluations in your CI pipeline. Use evaluators to create assertions that can fail tests based on evaluation results.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n///# test_geography_experiment.py\\\\\\\\nimport pytest\\\\\\\\nfrom langfuse import get_client, Evaluation\\\\\\\\nfrom langfuse.openai import OpenAI\\\\\\\\n \\\\\\\\n///# Test data for European capitals\\\\\\\\ntest_data = [\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of France?\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Paris\\\\\\\\\\\\\\\"},\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of Germany?\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Berlin\\\\\\\\\\\\\\\"},\\\\\\\\n {\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is the capital of Spain?\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Madrid\\\\\\\\\\\\\\\"},\\\\\\\\n]\\\\\\\\n \\\\\\\\ndef geography_task(*, item, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Task function that answers geography questions\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n question = item[\\\\\\\\\\\\\\\"input\\\\\\\\\\\\\\\"]\\\\\\\\n response = OpenAI().chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4\\\\\\\\\\\\\\\",\\\\\\\\n messages=[{\\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": question}]\\\\\\\\n )\\\\\\\\n return response.choices[0].message.content\\\\\\\\n \\\\\\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n if expected_output and expected_output.lower() in output.lower():\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=1.0)\\\\\\\\n \\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\", value=0.0)\\\\\\\\n \\\\\\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\\\\\n \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n accuracies = [\\\\\\\\n eval.value for result in item_results\\\\\\\\n for eval in result.evaluations if eval.name == \\\\\\\\\\\\\\\"accuracy\\\\\\\\\\\\\\\"\\\\\\\\n ]\\\\\\\\n \\\\\\\\n if not accuracies:\\\\\\\\n return Evaluation(name=\\\\\\\\\\\\\\\"avg_accuracy\\\\\\\\\\\\\\\", value=None)\\\\\\\\n \\\\\\\\n avg = sum(accuracies) / len(accuracies)\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nThese examples show how to use the experiment runner\\\\\\\\u2019s evaluation results to create meaningful test assertions in your CI pipeline. Tests can fail when accuracy drops below acceptable thresholds, ensuring model quality standards are maintained automatically.\\\\\\\\n### Autoevals Integration\\\\\\\\nAccess pre-built evaluation functions through the [autoevals library](https://github.com/braintrustdata/autoevals) integration.\\\\\\\\nPython SDKJS/TS SDK\\\\\\\\nThe Python SDK supports AutoEvals evaluators through direct integration:\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse.experiment import create_evaluator_from_autoevals\\\\\\\\nfrom autoevals.llm import Factuality\\\\\\\\n \\\\\\\\nevaluator = create_evaluator_from_autoevals(Factuality())\\\\\\\\n \\\\\\\\nresult = langfuse.run_experiment(\\\\\\\\n name=\\\\\\\\\\\\\\\"Autoevals Integration Test\\\\\\\\\\\\\\\",\\\\\\\\n data=test_data,\\\\\\\\n task=my_task,\\\\\\\\n evaluators=[evaluator]\\\\\\\\n)\\\\\\\\n \\\\\\\\nprint(result.format())\\\\\\\\n```\\\\\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { Factuality, Levenshtein } from \\\\\\\\\\\\\\\"autoevals\\\\\\\\\\\\\\\";\\\\\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\n \\\\\\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\\\\\n \\\\\\\\n// Use with additional parameters\\\\\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\\\\\n Factuality,\\\\\\\\n { model: \\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\" } // Additional AutoEvals parameters\\\\\\\\n);\\\\\\\\n \\\\\\\\nconst result = await langfuse.experiment.run({\\\\\\\\n name: \\\\\\\\\\\\\\\"AutoEvals Integration Test\\\\\\\\\\\\\\\",\\\\\\\\n data: testDataset,\\\\\\\\n task: myTask,\\\\\\\\n evaluators: [\\\\\\\\n factualityEvaluator,\\\\\\\\n levenshteinEvaluator,\\\\\\\\n customFactualityEvaluator\\\\\\\\n ]\\\\\\\\n});\\\\\\\\n \\\\\\\\nconsole.log(await result.format());\\\\\\\\n```\\\\\\\\n## Low-level SDK methods\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\\\\\n comment: \\\\\\\\\\\\\\\"This is a comment\\\\\\\\\\\\\\\", // optional, useful to add reasoning\\\\\\\\n });\\\\\\\\n}\\\\\\\\n \\\\\\\\n// Flush the langfuse client to ensure all score data is sent to the server at the end of the experiment run\\\\\\\\nawait langfuse.flush();\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\nfrom langfuse.langchain import CallbackHandler\\\\\\\\n///#from .app import my_llm_application\\\\\\\\n \\\\\\\\n///# Load the dataset\\\\\\\\ndataset = get_client().get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n///# Initialize the Langfuse handler\\\\\\\\nlangfuse_handler = CallbackHandler()\\\\\\\\n \\\\\\\\n///# Loop over the dataset items\\\\\\\\nfor item in dataset.items:\\\\\\\\n # Use the item.run() context manager for automatic trace linking\\\\\\\\n with item.run(\\\\\\\\n run_name=\\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\",\\\\\\\\n run_description=\\\\\\\\\\\\\\\"My first run\\\\\\\\\\\\\\\",\\\\\\\\n run_metadata={\\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"llama3\\\\\\\\\\\\\\\"},\\\\\\\\n ) as root_span:\\\\\\\\n # Execute your LLM-app against the dataset item input\\\\\\\\n output = my_langchain_chain(item.input, langfuse_handler)\\\\\\\\n \\\\\\\\n # Update top-level trace input and output\\\\\\\\n root_span.update_trace(input=item.input, output=output.content)\\\\\\\\n \\\\\\\\n # Optionally: Add scores computed in your experiment runner, e.g. json equality check\\\\\\\\n root_span.score_trace(\\\\\\\\n name=\\\\\\\\\\\\\\\"<example_eval>\\\\\\\\\\\\\\\",\\\\\\\\n value=my_eval_fn(item.input, output, item.expected_output),\\\\\\\\n comment=\\\\\\\\\\\\\\\"This is a comment\\\\\\\\\\\\\\\", ///# optional, useful to add reasoning\\\\\\\\n )\\\\\\\\n \\\\\\\\n///# Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run\\\\\\\\nget_client().flush()\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { LangfuseClient } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\";\\\\\\\\nimport { CallbackHandler } from \\\\\\\\\\\\\\\"@langfuse/langchain\\\\\\\\\\\\\\\";\\\\\\\\n...\\\\\\\\n \\\\\\\\nconst langfuse = new LangfuseClient()\\\\\\\\nconst runName = \\\\\\\\\\\\\\\"my-dataset-run\\\\\\\\\\\\\\\";\\\\\\\\nfor (const item of dataset.items) {\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/guides/cookbook/example_external_evaluation_pipelines\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline - Langfuse\\\\\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\\\\\\\\\"Cookbooks\\\\\\\\\\\\\\\")External Evaluation Pipelines\\\\\\\\nCopy as Markdown\\\\\\\\nThis is a Jupyter notebook\\\\\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\\\\\n# Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\\\\\\nThis cookbook explains how to build an external evaluation pipeline to measure the performance of your production LLM application using Langfuse.\\\\\\\\nAs a rule of thumb, we encourage you to check first if the [evaluations in the Langfuse UI](https://langfuse.com/docs/scores/model-based-evals) cover your use case. If your needs go beyond these, you can still implement in Langfuse custom evaluation templates without code.\\\\\\\\nConsider implementing an external evaluation pipeline if you need:\\\\\\\\n  * More control over **when** traces get evaluated. You could schedule the pipeline to run at specific times or responding to event-based triggers like Webhooks.\\\\\\\\n  * Greater flexibility with your custom evaluations, when your needs go beyond what\\\\\\\\u2019s possible with the Langfuse UI\\\\\\\\n  * Version control for your custom evaluations\\\\\\\\n  * The ability to evaluate data using existing evaluation frameworks\\\\\\\\nIf your use case meets any of this situations, let\\\\\\\\u2019s go ahead and implement your first external evaluation pipeline!\\\\\\\\n* * *\\\\\\\\nBy the end of this cookbook, you\\\\\\\\u2019ll be able to:\\\\\\\\n  * Create a synthetic dataset to test your models.\\\\\\\\n  * Use the Langfuse client to gather and filter traces of previous model runs\\\\\\\\n  * Evaluate these traces offline and incrementally\\\\\\\\n  * Add scores to existing Langfuse traces\\\\\\\\nConceptually, we will implement the following architecture:\\\\\\\\n* * *\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n**Note** : While we\\\\\\\\u2019re using a Jupyter notebook for this cookbook, in production you\\\\\\\\u2019d use your preferred orchestration tool. Just make sure to extract the code into a .py file and ensure all dependencies are available at runtime.\\\\\\\\n## (Prep-work) Loading synthetic traces to Langfuse\\\\\\\\nIn this demo, we\\\\\\\\u2019ll build a mock application: a science communicator LLM that explains any topic in an engaging and approachable way.\\\\\\\\nSince we don\\\\\\\\u2019t have real user data, our first step is to create a synthetic dataset. We\\\\\\\\u2019ll generate a variety of potential questions that real users might ask. While this is a great way to kickstart your LLM development, collecting real user queries as soon as possible is invaluable.\\\\\\\\nYou can get your Langfuse API keys [here](https://cloud.langfuse.com/) and OpenAI API key [here](https://platform.openai.com/api-keys)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n%pip install langfuse openai deepeval --upgrade\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport os\\\\\\\\n \\\\\\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"pk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddea\\\\\\\\ud83c\\\\\\\\uddfa EU region\\\\\\\\n///# os.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://us.cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddfa\\\\\\\\ud83c\\\\\\\\uddf8 US region\\\\\\\\n \\\\\\\\n///# Your openai key\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"OPENAI_API_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-proj-...\\\\\\\\\\\\\\\"\\\\\\\\n```\\\\\\\\nLet\\\\\\\\u2019s go ahead and generate a list of topic suggestions that we can later query to our application.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport openai\\\\\\\\n \\\\\\\\ntopic_suggestion = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\" You're a world-class journalist, specialized\\\\\\\\nin figuring out which are the topics that excite people the most.\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nBATCH_SIZE = 10\\\\\\\\nTOTAL_TRACES = 50\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\nnow = datetime.now()\\\\\\\\nfive_am_today = datetime(now.year, now.month, now.day, 5, 0)\\\\\\\\nfive_am_yesterday = five_am_today - timedelta(days=1)\\\\\\\\n \\\\\\\\ntraces_batch = langfuse.api.trace.list(page=1,\\\\\\\\n limit=BATCH_SIZE,\\\\\\\\n tags=\\\\\\\\\\\\\\\"ext_eval_pipelines\\\\\\\\\\\\\\\",\\\\\\\\n from_timestamp=five_am_yesterday,\\\\\\\\n to_timestamp=datetime.now()\\\\\\\\n ).data\\\\\\\\n \\\\\\\\nprint(f\\\\\\\\\\\\\\\"Traces in first batch: {len(traces_batch)}\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nTraces in first batch: 10\\\\\\\\n## 2\\\\\\\\\\\\\\\\. Run your evaluations\\\\\\\\nLangfuse can handle numerical, boolean and categorical (`string`) scores. Wrapping your custom evaluation logic in a function is often a good practice. Evaluation functions should take a `trace` as input and yield a valid score. Let\\\\\\\\u2019s begin with a simple example using a categorical score.\\\\\\\\n### 2.1. Categoric Evaluations\\\\\\\\nWhen analyzing the outputs of your LLM applications, you may want to evaluate traits that are best defined qualitatively, such as sentiment, tonality or text complexity (Grade level).\\\\\\\\nWe\\\\\\\\u2019re building a science educator LLM that should sound engaging and positive. To ensure it hits the right notes, we\\\\\\\\u2019ll evaluate the tone of its outputs to see if they match our intent. We\\\\\\\\u2019ll draft an evaluation prompt ourselves (no library) to identify the three main tones in each model output.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\ntemplate_tone_eval = \\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\nYou're an expert in human emotional intelligence. You can identify with ease the\\\\\\\\n tone in human-written text. Your task is to identify the tones present in a\\\\\\\\n piece of <text/> with precission. Your output is a comma separated list of three\\\\\\\\n tones. PRINT THE LIST ALONE, NOTHING ELSE.\\\\\\\\n \\\\\\\\n<possible_tones>\\\\\\\\nneutral, confident, joyful, optimistic, friendly, urgent, analytical, respectful\\\\\\\\n</possible_tones>\\\\\\\\n \\\\\\\\n<example_1>\\\\\\\\nInput: Citizen science plays a crucial role in research by involving everyday\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\npeople in scientific projects. This collaboration allows researchers to collect\\\\\\\\nvast amounts of data that would be impossible to gather on their own. Citizen\\\\\\\\nscientists contribute valuable observations and insights that can lead to new\\\\\\\\ndiscoveries and advancements in various fields. By participating in citizen\\\\\\\\nscience projects, individuals can actively contribute to scientific research\\\\\\\\nand make a meaningful impact on our understanding of the world around us.\\\\\\\\n \\\\\\\\nOutput: respectful,optimistic,confident\\\\\\\\n</example_1>\\\\\\\\n \\\\\\\\n<example_2>\\\\\\\\nInput: Bionics is a field that combines biology and engineering to create\\\\\\\\ndevices that can enhance human abilities. By merging humans and machines,\\\\\\\\nbionics aims to improve quality of life for individuals with disabilities\\\\\\\\nor enhance performance for others. These technologies often mimic natural\\\\\\\\nprocesses in the body to create seamless integration. Overall, bionics holds\\\\\\\\ngreat potential for revolutionizing healthcare and technology in the future.\\\\\\\\n \\\\\\\\nOutput: optimistic,confident,analytical\\\\\\\\n</example_2>\\\\\\\\n \\\\\\\\n<example_3>\\\\\\\\nInput: Social media can have both positive and negative impacts on mental\\\\\\\\nhealth. On the positive side, it can help people connect, share experiences,\\\\\\\\nand find support. However, excessive use of social media can also lead to\\\\\\\\nfeelings of inadequacy, loneliness, and anxiety. It's important to find a\\\\\\\\nbalance and be mindful of how social media affects your mental well-being.\\\\\\\\nRemember, it's okay to take breaks and prioritize your mental health.\\\\\\\\n \\\\\\\\nOutput: friendly,neutral,respectful\\\\\\\\n</example_3>\\\\\\\\n \\\\\\\\n<text>\\\\\\\\n{text}\\\\\\\\n</text>\\\\\\\\n\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\n \\\\\\\\ntest_tone_score = openai.chat.completions.create(\\\\\\\\n messages=[\\\\\\\\n {\\\\\\\\n \\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": template_tone_eval.format(\\\\\\\\n text=traces_batch[1].output),\\\\\\\\n }\\\\\\\\n ],\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\n temperature=0\\\\\\\\n).choices[0].message.content\\\\\\\\nprint(f\\\\\\\\\\\\\\\"User query: {traces_batch[1].input['args'][0]}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Model answer: {traces_batch[1].output}\\\\\\\\\\\\\\\")\\\\\\\\nprint(f\\\\\\\\\\\\\\\"Dominant tones: {test_tone_score}\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nIdentifying human intents and tones can be tricky for language models. To handle this, we used a multi-shot prompt, which means giving the model several examples to learn from. Now let\\\\\\\\u2019s wrap our code in an evaluation function for convenience.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\ndef tone_score(trace):\\\\\\\\n return openai.chat.completions.create(\\\\\\\\n messages=[\\\\\\\\n {\\\\\\\\n \\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"user\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": template_tone_eval.format(text=trace.output),\\\\\\\\n }\\\\\\\\n ],\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\",\\\\\\\\n temperature=0\\\\\\\\n ).choices[0].message.content\\\\\\\\n \\\\\\\\ntone_score(traces_batch[1])\\\\\\\\n```\\\\\\\\nGreat! Now let\\\\\\\\u2019s go ahead and create a numeric evaluation score.\\\\\\\\n### 2.2. Numeric Evaluations\\\\\\\\nIn this cookbook, we\\\\\\\\u2019ll use the `Deepeval` framework ([docs](https://docs.confident-ai.com/docs/getting-started)) to handle our numeric evaluations. Deepeval provides scores ranging from zero to one for many common LLM metrics. Plus, you can create custom metrics by simply describing them in plain language. To ensure our app\\\\\\\\u2019s responses are joyful and engaging, we\\\\\\\\u2019ll define a custom \\\\\\\\u2018joyfulness\\\\\\\\u2019 score.\\\\\\\\nYou can use any evaluation library. These are popular ones:\\\\\\\\n  * OpenAI Evals ([GitHub](https://github.com/openai/evals))\\\\\\\\n  * Langchain Evaluators\\\\\\\\n  * [RAGAS](https://docs.ragas.io/en/latest/concepts/metrics/index.html) for RAG applications\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom deepeval.metrics import GEval\\\\\\\\nfrom deepeval.test_case import LLMTestCaseParams, LLMTestCase\\\\\\\\n \\\\\\\\ndef joyfulness_score(trace):\\\\\\\\n joyfulness_metric = GEval(\\\\\\\\n name=\\\\\\\\\\\\\\\"Correctness\\\\\\\\\\\\\\\",\\\\\\\\n criteria=\\\\\\\\\\\\\\\"Determine whether the output is engaging and fun.\\\\\\\\\\\\\\\",\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\\\\\\\\n )\\\\\\\\n test_case = LLMTestCase(\\\\\\\\n input=trace.input[\\\\\\\\\\\\\\\"args\\\\\\\\\\\\\\\"],\\\\\\\\n actual_output=trace.output)\\\\\\\\n \\\\\\\\n joyfulness_metric.measure(test_case)\\\\\\\\n \\\\\\\\n print(f\\\\\\\\\\\\\\\"Score: {joyfulness_metric.score}\\\\\\\\\\\\\\\")\\\\\\\\n print(f\\\\\\\\\\\\\\\"Reason: {joyfulness_metric.reason}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n return {\\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\": joyfulness_metric.score, \\\\\\\\\\\\\\\"reason\\\\\\\\\\\\\\\": joyfulness_metric.reason}\\\\\\\\n \\\\\\\\njoyfulness_score(traces_batch[1])\\\\\\\\n```\\\\\\\\nUnder the hood, GEval uses chain of thought (CoT) prompting to formulate a set of criteria for scoring prompts. When developing your own metrics, it\\\\\\\\u2019s important to review the reasoning behind these scores. This helps ensure that the model evaluates the traces just as you intended when you wrote the evaluation prompt.\\\\\\\\nOur eval function returns a dictionary with both the score and the model\\\\\\\\u2019s reasoning. We do this as we\\\\\\\\u2019ll persist the reasoning with every langfuse score, ensuring interpretability.\\\\\\\\nNow we\\\\\\\\u2019re done with defining our evaluation functions. Let\\\\\\\\u2019s push those scores back to Langfuse!\\\\\\\\n## 3\\\\\\\\\\\\\\\\. Pushing Scores to Langfuse\\\\\\\\nNow that we have our evaluation functions ready, it\\\\\\\\u2019s time to put them to work. Use the Langfuse client to add scores to existing traces.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_score(\\\\\\\\n trace_id=traces_batch[1].id,\\\\\\\\n name=\\\\\\\\\\\\\\\"tone\\\\\\\\\\\\\\\",\\\\\\\\n value=joyfulness_score(traces_batch[1])[\\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\"],\\\\\\\\n comment=joyfulness_score(traces_batch[1])[\\\\\\\\\\\\\\\"reason\\\\\\\\\\\\\\\"]\\\\\\\\n)\\\\\\\\n```\\\\\\\\nAnd thus, you\\\\\\\\u2019ve added your first externally-evaluated score to Langfuse! Just 49 more to go \\\\\\\\ud83d\\\\\\\\ude01. But don\\\\\\\\u2019t worry \\\\\\\\u2014 our solutions are easy to scale.\\\\\\\\n## 4\\\\\\\\\\\\\\\\. Putting everything together\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nUntil now, we went through each of the necessary steps to build an external evaluation pipeline: Fetching traces, running the evaluations, and persisting the scores to Langfuse. Let\\\\\\\\u2019s sum it up into a compact script that you could run in your evaluation pipeline.\\\\\\\\nWe\\\\\\\\u2019ll fetch the data in batches of 10 traces and then iterate through each trace to score it and push the scores back to Langfuse. Note that this batch size is for demonstration purposes. In a production setup, you might want to process multiple batches in parallel to speed things up. Batching not only reduces the memory load on your system but also allows you to create checkpoints, so you can easily resume if something goes wrong.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport math\\\\\\\\n \\\\\\\\nfor page_number in range(1, math.ceil(TOTAL_TRACES/BATCH_SIZE)):\\\\\\\\n \\\\\\\\n traces_batch = langfuse.api.trace.list(\\\\\\\\n tags=\\\\\\\\\\\\\\\"ext_eval_pipelines\\\\\\\\\\\\\\\",\\\\\\\\n page=page_number,\\\\\\\\n from_timestamp=five_am_yesterday,\\\\\\\\n to_timestamp=five_am_today,\\\\\\\\n limit=BATCH_SIZE\\\\\\\\n ).data\\\\\\\\n \\\\\\\\n for trace in traces_batch:\\\\\\\\n print(f\\\\\\\\\\\\\\\"Processing {trace.name}\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n if trace.output is None:\\\\\\\\n print(f\\\\\\\\\\\\\\\"Warning: \\\\\\\\\\\\\\\\n Trace {trace.name} had no generated output, \\\\\\\\\\\\\\\\\\\\\\\\n it was skipped\\\\\\\\\\\\\\\")\\\\\\\\n continue\\\\\\\\n \\\\\\\\n langfuse.create_score(\\\\\\\\n trace_id=trace.id,\\\\\\\\n name=\\\\\\\\\\\\\\\"tone\\\\\\\\\\\\\\\",\\\\\\\\n value=tone_score(trace)\\\\\\\\n )\\\\\\\\n \\\\\\\\n jscore = joyfulness_score(trace)\\\\\\\\n langfuse.create_score(\\\\\\\\n trace_id=trace.id,\\\\\\\\n name=\\\\\\\\\\\\\\\"joyfulness\\\\\\\\\\\\\\\",\\\\\\\\n value=jscore[\\\\\\\\\\\\\\\"score\\\\\\\\\\\\\\\"],\\\\\\\\n comment=jscore[\\\\\\\\\\\\\\\"reason\\\\\\\\\\\\\\\"]\\\\\\\\n )\\\\\\\\n \\\\\\\\n print(f\\\\\\\\\\\\\\\"Batch {page_number} processed \\\\\\\\ud83d\\\\\\\\ude80 \\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nIf your pipeline ran successfully, you should see your score in the Langfuse UI. \\\\\\\\nAnd that\\\\\\\\u2019s it! You\\\\\\\\u2019re now ready to integrate these lines into your preferred orchestration tool to ensure they run at the right times.\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nTo achieve our original goal of running the script every day at 5 am, simply schedule a Cron task in your chosen environment with the rule `cron(0 5 * * ? *)`.\\\\\\\\nThanks for coding along! I hope you enjoyed the tutorial and found it helpful.\\\\\\\\nLast updated on August 20, 2025\\\\\\\\n[Example - Tracing and Evaluation for the OpenAI-Agents SDK](/guides/cookbook/example_evaluating_openai_agents \\\\\\\\\\\\\\\"Example - Tracing and Evaluation for the OpenAI-Agents SDK\\\\\\\\\\\\\\\")\\\\\\\\n[Guide - Building an intent classification pipeline](/guides/cookbook/example_intent_classification_pipeline \\\\\\\\\\\\\\\"Guide - Building an intent classification pipeline\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"github_discussion\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://github.com/orgs/langfuse/discussions/7572\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"LLM-as-a-Judge Evaluators for Dataset doesn't work as expected.\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\\\\\n\\\\\\\\n---\\\\\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=hallucination) | [Report \\\\\\\\ud83d\\\\\\\\udc1b](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\\\\\n\\\\\\\\n        Reply #5 to comment #1 from dosubot on June 30, 2025:\\\\\\\\n\\\\\\\\n        I am using ui and variable mapping is as follows:\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n{{search_query}} is mapped to Object: Dataset item, Object Variable: Input, JsonPath: <empty>\\\\\\\\r\\\\\\\\n{{summary}} is mapped to Object: Dataset item, Object Variable: Metadata, JsonPath: <empty>\\\\\\\\r\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"{{expected}} is mapped to Object: Dataset item,  Object Variable: Expected Output, JsonPath: <empty>\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\nA single row format example from my dataset: \\\\\\\\r\\\\\\\\nInput column: \\\\\\\\\\\\\\\"Test input\\\\\\\\\\\\\\\", Expected Output column: \\\\\\\\\\\\\\\"Test expected output\\\\\\\\\\\\\\\", Metadata column: \\\\\\\\\\\\\\\"Test metadata\\\\\\\\\\\\\\\"\\\\\\\\n\\\\\\\\n        Reply #6 to comment #1 from dosubot on June 30, 2025:\\\\\\\\n\\\\\\\\n        <!-- Answer -->\\\\\\\\nYour mapping setup looks correct, but the backend eligibility logic is strict: if any mapped variable can't be resolved (for example, if the field is empty or the type doesn't match expectations), the item is silently skipped and you get \\\\\\\\\\\\\\\"0 elements\\\\\\\\\\\\\\\" processed. \\\\\\\\n\\\\\\\\nOne likely issue is the Metadata field\\\\\\\\u2014if it's a plain string (like \\\\\\\\\\\\\\\"Test metadata\\\\\\\\\\\\\\\"), but the system expects a JSON object (since many Langfuse examples use objects for metadata), this can cause the mapping to fail even if JsonPath is empty. When JsonPath is empty, the backend uses the top-level value as-is, but if the field is a string and the mapping expects an object (or vice versa), the item is excluded from evaluation\\\\\\\\u00a0[see code logic](https://github.com/langfuse/langfuse/blob/98cc1bb7a1759e29de3dd818f6a500df48fb5e82/worker/src/features/evaluation/evalService.ts).\\\\\\\\n\\\\\\\\nTry this: map all variables to fields you know are plain strings (like Input or Expected Output) and see if the evaluation processes any items. If it works, the issue is with the Metadata field's type. To confirm, create a dataset item where Metadata is a JSON object (e.g., {\\\\\\\\\\\\\\\"meta\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"value\\\\\\\\\\\\\\\"}) and try mapping with and without a JsonPath. This will help isolate the problem.\\\\\\\\n\\\\\\\\nIf you still get 0 elements, export a sample of your dataset as JSON and share it\\\\\\\\u2014there may be a subtle data or mapping mismatch. \\\\\\\\n\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\nMore details on dataset item structure and mapping: [Langfuse docs](https://langfuse.com/docs/datasets/get-started).\\\\\\\\n\\\\\\\\n\\\\\\\\n<!-- Dosu Comment Footer -->\\\\\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\\\\\n\\\\\\\\n---\\\\\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=hallucination) | [Report \\\\\\\\ud83d\\\\\\\\udc1b](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\\\\\n\\\\\\\\n        Reply #7 to comment #1 from dosubot on June 30, 2025:\\\\\\\\n\\\\\\\\n        I have created a new dataset with single row in json format, so columns Input, Expected Output and Metadata have following values respectively but still the same issue:\\\\\\\\r\\\\\\\\n{ \\\\\\\\\\\\\\\"search_query\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What's MLflow?\\\\\\\\\\\\\\\" }\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n{ \\\\\\\\\\\\\\\"expected\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"MLflow tracks ML experiments.\\\\\\\\\\\\\\\" } \\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2023-09-25-datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Datasets (beta)\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Datasets (beta) - Langfuse\\\\\\\\n[\\\\\\\\u2190 Back to changelog](/changelog#2023-09-25-datasets)\\\\\\\\nSeptember 25, 2023\\\\\\\\n# Datasets (beta)\\\\\\\\n[Marc Klingen](https://twitter.com/marcklingen)\\\\\\\\nCollect sets of inputs and expected outputs in Langfuse to evaluate your LLM app. Use evaluations to benchmark different experiments.\\\\\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\\\\\n_Run experiment on dataset_\\\\\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n \\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n # output also returned as it is used to evaluate the run\\\\\\\\n generation, output = my_llm_application.run(item.input)\\\\\\\\n \\\\\\\\n # link the execution trace to the dataset item and give it a run_name\\\\\\\\n item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # optionally, evaluate the output to compare different runs more easily\\\\\\\\n generation.score(\\\\\\\\n name=\\\\\\\\\\\\\\\"<example_eval>\\\\\\\\\\\\\\\",\\\\\\\\n # any float value\\\\\\\\n value=my_eval_fn(\\\\\\\\n item.input,\\\\\\\\n output,\\\\\\\\n item.expected_output\\\\\\\\n )\\\\\\\\n )\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst dataset = await langfuse.getDataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\nfor (const item of dataset.items) {\\\\\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n // output also returned as it is used to evaluate the run\\\\\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\\\\\n \\\\\\\\n // link the execution trace to the dataset item and give it a run_name\\\\\\\\n await item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\n // optionally, evaluate the output to compare different runs more easily\\\\\\\\n generation.score({\\\\\\\\n name: \\\\\\\\\\\\\\\"<score_name>\\\\\\\\\\\\\\\",\\\\\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\\\\\n });\\\\\\\\n}\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Troubleshooting and FAQ\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\\\\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\\\\\\\\\(\\\\\\\\\\\\\\\\)` method\\\\\\\\\\\\\\\")\\\\\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\\\\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\\\\\\\\\")\\\\\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\\\\\\\\\"Langfuse Support: \\\\\\\\\\\\\\\\[Experiment\\\\\\\\\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\\\\\\\\\")\\\\\\\\n[Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\\\\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\\\\\u2014earlier generations are missing\\\\\\\\\\\\\\\")\\\\\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\\\\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\\\\\\\\\")\\\\\\\\n[Results for some data items not present when comparing experiments](https://github.com/orgs/langfuse/discussions/5928 \\\\\\\\\\\\\\\"Langfuse Support: Results for some data items not present when comparing experiments\\\\\\\\\\\\\\\")\\\\\\\\n[Deleting Metrics for Langfuse](https://github.com/orgs/langfuse/discussions/5849 \\\\\\\\\\\\\\\"Langfuse Support: Deleting Metrics for Langfuse\\\\\\\\\\\\\\\")\\\\\\\\n[Discrepancies between dataset items found in the UI vs retrieved from the SDK/API](https://github.com/orgs/langfuse/discussions/5822 \\\\\\\\\\\\\\\"Langfuse Support: Discrepancies between dataset items found in the UI vs retrieved from the SDK/API\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Request batch adding of multiple traces to datasets](https://github.com/orgs/langfuse/discussions/8526 \\\\\\\\\\\\\\\"Langfuse Ideas: Request batch adding of multiple traces to datasets\\\\\\\\\\\\\\\")\\\\\\\\n[How to categorize topics and create a pie chart?](https://github.com/orgs/langfuse/discussions/8512 \\\\\\\\\\\\\\\"Langfuse Ideas: How to categorize topics and create a pie chart?\\\\\\\\\\\\\\\")\\\\\\\\n[Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window](https://github.com/orgs/langfuse/discussions/8485 \\\\\\\\\\\\\\\"Langfuse Ideas: Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window\\\\\\\\\\\\\\\")\\\\\\\\n[feat(LLM-as-a-judge): support stratified sampling by trace property](https://github.com/orgs/langfuse/discussions/8480 \\\\\\\\\\\\\\\"Langfuse Ideas: feat\\\\\\\\\\\\\\\\(LLM-as-a-judge\\\\\\\\\\\\\\\\): support stratified sampling by trace property\\\\\\\\\\\\\\\")\\\\\\\\n[Bulk Evaluation Does not Displays the status of the dataset (Like for E.g. processing, evaluated, executed)](https://github.com/orgs/langfuse/discussions/8410 \\\\\\\\\\\\\\\"Langfuse Ideas: Bulk Evaluation Does not Displays the status of the dataset \\\\\\\\\\\\\\\\(Like for E.g. processing, evaluated, executed\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[Annotation Queue Creation API](https://github.com/orgs/langfuse/discussions/8372 \\\\\\\\\\\\\\\"Langfuse Ideas: Annotation Queue Creation API\\\\\\\\\\\\\\\")\\\\\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\\\\\\\\\"Langfuse Ideas: \\\\\\\\\\\\\\\\[Langfuse Cloud\\\\\\\\\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\\\\\\\\\")\\\\\\\\n[feat(evals): allow canceling a running evaluator with pending evaluation jobs](https://github.com/orgs/langfuse/discussions/8310 \\\\\\\\\\\\\\\"Langfuse Ideas: feat\\\\\\\\\\\\\\\\(evals\\\\\\\\\\\\\\\\): allow canceling a running evaluator with pending evaluation jobs\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Support Description on Dataset Items](https://github.com/orgs/langfuse/discussions/6011 \\\\\\\\\\\\\\\"Langfuse Ideas: Support Description on Dataset Items\\\\\\\\\\\\\\\")\\\\\\\\n[Alta Integration](https://github.com/orgs/langfuse/discussions/5957 \\\\\\\\\\\\\\\"Langfuse Ideas: Alta Integration\\\\\\\\\\\\\\\")\\\\\\\\n[Add run name to columns when looking at a specific dataset item](https://github.com/orgs/langfuse/discussions/5929 \\\\\\\\\\\\\\\"Langfuse Ideas: Add run name to columns when looking at a specific dataset item\\\\\\\\\\\\\\\")\\\\\\\\n[Delete multiple dataset runs](https://github.com/orgs/langfuse/discussions/5893 \\\\\\\\\\\\\\\"Langfuse Ideas: Delete multiple dataset runs\\\\\\\\\\\\\\\")\\\\\\\\n[Navigation between items in a dataset run is confusing - context of the selected dataset run is lost](https://github.com/orgs/langfuse/discussions/5892 \\\\\\\\\\\\\\\"Langfuse Ideas: Navigation between items in a dataset run is confusing - context of the selected dataset run is lost\\\\\\\\\\\\\\\")\\\\\\\\n[Enhanced score distribution visualization in experiment analysis](https://github.com/orgs/langfuse/discussions/5819 \\\\\\\\\\\\\\\"Langfuse Ideas: Enhanced score distribution visualization in experiment analysis\\\\\\\\\\\\\\\")\\\\\\\\n[Multi-step Prompt Experiments and Playground](https://github.com/orgs/langfuse/discussions/5812 \\\\\\\\\\\\\\\"Langfuse Ideas: Multi-step Prompt Experiments and Playground\\\\\\\\\\\\\\\")\\\\\\\\n[Option to add trace to new dataset](https://github.com/orgs/langfuse/discussions/5756 \\\\\\\\\\\\\\\"Langfuse Ideas: Option to add trace to new dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Simplified UI for Scoring](https://github.com/orgs/langfuse/discussions/5721 \\\\\\\\\\\\\\\"Langfuse Ideas: Simplified UI for Scoring\\\\\\\\\\\\\\\")\\\\\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\\\\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Datasets: Add selection of traces to a dataset](https://github.com/orgs/langfuse/discussions/4353 \\\\\\\\\\\\\\\"Langfuse Ideas: Datasets: Add selection of traces to a dataset\\\\\\\\\\\\\\\")\\\\\\\\n[Multi-user annotation capability in Annotation Queues](https://github.com/orgs/langfuse/discussions/4348 \\\\\\\\\\\\\\\"Langfuse Ideas: Multi-user annotation capability in Annotation Queues\\\\\\\\\\\\\\\")\\\\\\\\n[Multi-turn / session experiments in datasets](https://github.com/orgs/langfuse/discussions/4208 \\\\\\\\\\\\\\\"Langfuse Ideas: Multi-turn / session experiments in datasets\\\\\\\\\\\\\\\")\\\\\\\\n[Enable to use variable of prompt on evaluator.](https://github.com/orgs/langfuse/discussions/4121 \\\\\\\\\\\\\\\"Langfuse Ideas: Enable to use variable of prompt on evaluator.\\\\\\\\\\\\\\\")\\\\\\\\n[Sessions Table: Scores Column](https://github.com/orgs/langfuse/discussions/4120 \\\\\\\\\\\\\\\"Langfuse Ideas: Sessions Table: Scores Column\\\\\\\\\\\\\\\")\\\\\\\\n[Add new filters for the LLM as a Judge Evaluation (other scores and cost)](https://github.com/orgs/langfuse/discussions/4106 \\\\\\\\\\\\\\\"Langfuse Ideas: Add new filters for the LLM as a Judge Evaluation \\\\\\\\\\\\\\\\(other scores and cost\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[Export dataset run table](https://github.com/orgs/langfuse/discussions/4077 \\\\\\\\\\\\\\\"Langfuse Ideas: Export dataset run table\\\\\\\\\\\\\\\")\\\\\\\\n[feat: support adding trace tags in annotation queue view](https://github.com/orgs/langfuse/discussions/4037 \\\\\\\\\\\\\\\"Langfuse Ideas: feat: support adding trace tags in annotation queue view\\\\\\\\\\\\\\\")\\\\\\\\n[Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times](https://github.com/orgs/langfuse/discussions/4035 \\\\\\\\\\\\\\\"Langfuse Ideas: Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times\\\\\\\\\\\\\\\")\\\\\\\\n[Diff support for dataset runs view](https://github.com/orgs/langfuse/discussions/4025 \\\\\\\\\\\\\\\"Langfuse Ideas: Diff support for dataset runs view\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Create Support for gemini models in playground](https://github.com/orgs/langfuse/discussions/4019 \\\\\\\\\\\\\\\"Langfuse Ideas: Create Support for gemini models in playground\\\\\\\\\\\\\\\")\\\\\\\\n[Change AWS access pattern for Bedrock LLM usage, assume role](https://github.com/orgs/langfuse/discussions/3988 \\\\\\\\\\\\\\\"Langfuse Ideas: Change AWS access pattern for Bedrock LLM usage, assume role\\\\\\\\\\\\\\\")\\\\\\\\n[Add ability to export and import evaluators between projects](https://github.com/orgs/langfuse/discussions/3970 \\\\\\\\\\\\\\\"Langfuse Ideas: Add ability to export and import evaluators between projects\\\\\\\\\\\\\\\")\\\\\\\\n[feat: Folder structure for dataset organisation](https://github.com/orgs/langfuse/discussions/3935 \\\\\\\\\\\\\\\"Langfuse Ideas: feat: Folder structure for dataset organisation\\\\\\\\\\\\\\\")\\\\\\\\n[Model-based evaluations triggered by observations](https://github.com/orgs/langfuse/discussions/3918 \\\\\\\\\\\\\\\"Langfuse Ideas: Model-based evaluations triggered by observations\\\\\\\\\\\\\\\")\\\\\\\\n[Scores: Conditional Annotation](https://github.com/orgs/langfuse/discussions/3842 \\\\\\\\\\\\\\\"Langfuse Ideas: Scores: Conditional Annotation\\\\\\\\\\\\\\\")\\\\\\\\n[Annotation Queues: define optional/mandatory score configs by queue](https://github.com/orgs/langfuse/discussions/3841 \\\\\\\\\\\\\\\"Langfuse Ideas: Annotation Queues: define optional/mandatory score configs by queue\\\\\\\\\\\\\\\")\\\\\\\\n[Scores: support for recording multiple choice selection as score value](https://github.com/orgs/langfuse/discussions/3840 \\\\\\\\\\\\\\\"Langfuse Ideas: Scores: support for recording multiple choice selection as score value\\\\\\\\\\\\\\\")\\\\\\\\n[Filter by status in dataset items table](https://github.com/orgs/langfuse/discussions/3818 \\\\\\\\\\\\\\\"Langfuse Ideas: Filter by status in dataset items table\\\\\\\\\\\\\\\")\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/overview\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Evaluation Overview\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Evaluation of LLM Applications - Langfuse\\\\\\\\nDocsEvaluationOverview\\\\\\\\nCopy as Markdown\\\\\\\\n# Evaluation Overview\\\\\\\\nEvaluation is a critical aspect of developing and deploying LLM applications. Usually, teams use a multitude of different evaluation methods to score the performance of their AI application depending on the use case and the stage of the development process.\\\\\\\\n\\\\\\\\ud83c\\\\\\\\udfa5\\\\\\\\n[ **Watch this walkthrough**](/watch-demo?tab=evaluation) of Langfuse Evaluation and how to use it to improve your LLM application.\\\\\\\\n## Why use LLM Evaluation?\\\\\\\\nLLM evaluation is crucial for improving the accuracy and robustness of language models, ultimately enhancing the user experience and trust in your AI application. Here are the key benefits:\\\\\\\\n  * **Quality Assurance** : Detect hallucinations, factual inaccuracies, and inconsistent outputs to ensure your AI app delivers reliable results\\\\\\\\n  * **Performance Monitoring** : Measure response quality, relevance, and user satisfaction across different scenarios and edge cases\\\\\\\\n  * **Continuous Improvement** : Identify areas for enhancement and track improvements over time through structured evaluation metrics\\\\\\\\n  * **User Trust** : Build confidence in your AI application by demonstrating consistent, high-quality outputs through systematic evaluation\\\\\\\\n  * **Risk Mitigation** : Catch potential issues before they reach production users, reducing the likelihood of poor user experiences or reputational damage\\\\\\\\n## Online & Offline Evaluation\\\\\\\\n**Offline Evaluation involves**\\\\\\\\n  * Evaluating the application in a controlled setting\\\\\\\\n  * Typically using curated test Datasets instead of live user queries\\\\\\\\n  * Heavily used during development (can be part of CI/CD pipelines) to measure improvements / regressions\\\\\\\\n  * Repeatable and you can get clear accuracy metrics since you have ground truth.\\\\\\\\n**Online Evaluation involves**\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Python SDK: Pass ScoreConfig instead of config_id when creating scores](https://github.com/orgs/langfuse/discussions/8623 \\\\\\\\\\\\\\\"Langfuse Ideas: Python SDK: Pass ScoreConfig instead of config_id when creating scores\\\\\\\\\\\\\\\")\\\\\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\\\\\\\\\"Langfuse Ideas: \\\\\\\\\\\\\\\\[Langfuse Cloud\\\\\\\\\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\\\\\\\\\")\\\\\\\\n[Score Configs: Allow editing the categories of a categorical score](https://github.com/orgs/langfuse/discussions/8259 \\\\\\\\\\\\\\\"Langfuse Ideas: Score Configs: Allow editing the categories of a categorical score\\\\\\\\\\\\\\\")\\\\\\\\n[UI-LLM as a Jury](https://github.com/orgs/langfuse/discussions/8195 \\\\\\\\\\\\\\\"Langfuse Ideas: UI-LLM as a Jury\\\\\\\\\\\\\\\")\\\\\\\\n[Enable Immediate Score Management for User Feedback](https://github.com/orgs/langfuse/discussions/7686 \\\\\\\\\\\\\\\"Langfuse Ideas: Enable Immediate Score Management for User Feedback\\\\\\\\\\\\\\\")\\\\\\\\n[Filter by scores in session view](https://github.com/orgs/langfuse/discussions/7528 \\\\\\\\\\\\\\\"Langfuse Ideas: Filter by scores in session view\\\\\\\\\\\\\\\")\\\\\\\\n[Support break lines on evaluation run tooltip hint](https://github.com/orgs/langfuse/discussions/7452 \\\\\\\\\\\\\\\"Langfuse Ideas: Support break lines on evaluation run tooltip hint\\\\\\\\\\\\\\\")\\\\\\\\n[Support new lines when storing / displaying score comments](https://github.com/orgs/langfuse/discussions/6473 \\\\\\\\\\\\\\\"Langfuse Ideas: Support new lines when storing / displaying score comments\\\\\\\\\\\\\\\")\\\\\\\\n[Evaluator: Filter for Scores](https://github.com/orgs/langfuse/discussions/6236 \\\\\\\\\\\\\\\"Langfuse Ideas: Evaluator: Filter for Scores\\\\\\\\\\\\\\\")\\\\\\\\n[Code-based custom evaluators](https://github.com/orgs/langfuse/discussions/6087 \\\\\\\\\\\\\\\"Langfuse Ideas: Code-based custom evaluators\\\\\\\\\\\\\\\")\\\\\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\\\\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\\\\\\\\\")\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Session-level scores](https://github.com/orgs/langfuse/discussions/2728 \\\\\\\\\\\\\\\"Langfuse Ideas: Session-level scores\\\\\\\\\\\\\\\")\\\\\\\\n[Scoring dataset runs, e.g. precision, recall, f-value](https://github.com/orgs/langfuse/discussions/2511 \\\\\\\\\\\\\\\"Langfuse Ideas: Scoring dataset runs, e.g. precision, recall, f-value\\\\\\\\\\\\\\\")\\\\\\\\n[Adding userId / author to score (custom metadata)](https://github.com/orgs/langfuse/discussions/2469 \\\\\\\\\\\\\\\"Langfuse Ideas: Adding userId / author to score \\\\\\\\\\\\\\\\(custom metadata\\\\\\\\\\\\\\\\)\\\\\\\\\\\\\\\")\\\\\\\\n[Add string data type in score config](https://github.com/orgs/langfuse/discussions/2402 \\\\\\\\\\\\\\\"Langfuse Ideas: Add string data type in score config\\\\\\\\\\\\\\\")\\\\\\\\n[API to delete scores](https://github.com/orgs/langfuse/discussions/1133 \\\\\\\\\\\\\\\"Langfuse Ideas: API to delete scores\\\\\\\\\\\\\\\")\\\\\\\\nGitHubSupportGitHubIdeas\\\\\\\\nUpvotes[GitHubNew](https://github.com/orgs/langfuse/discussions/new/choose)\\\\\\\\n  * 9votes\\\\\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486)\\\\\\\\nmsanand\\\\\\\\u20223/25/2024\\\\\\\\u2022\\\\\\\\n2Resolved\\\\\\\\n  * 4votes\\\\\\\\n[Filter Categorical Score Values](https://github.com/orgs/langfuse/discussions/3797)\\\\\\\\nalabrashJr\\\\\\\\u202210/17/2024\\\\\\\\u2022\\\\\\\\n3Resolved\\\\\\\\n  * 3votes\\\\\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590)\\\\\\\\nanuras\\\\\\\\u20228/18/2025\\\\\\\\u2022\\\\\\\\n1Resolved\\\\\\\\n  * 3votes\\\\\\\\n[Support for Metric Calculation (Precision@K, Recall@K) and Adding Custom Metrics Use Case Overview](https://github.com/orgs/langfuse/discussions/5215)\\\\\\\\nsrimantacse\\\\\\\\u20221/27/2025\\\\\\\\u2022\\\\\\\\n2Resolved\\\\\\\\n  * 2votes\\\\\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556)\\\\\\\\nflabbergastedbd\\\\\\\\u20228/15/2025\\\\\\\\u2022\\\\\\\\n1\\\\\\\\n  * 1votes\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}]}\\\"}],\\\"isError\\\":false}\"},\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"ai\",\"attributes\":{}}}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "input": "{\"query\":\"N+1 evaluations Langfuse how to implement n+1 evaluation pattern batch eval per prediction vs per dataset Python JS\"}",
      "output": "{\"_meta\":{\"id\":\"inkeep-chatcmpl-40fc3a6d-a341-4906-ba8f-eecf1570e0e3\",\"model\":\"inkeep-rag\",\"choices\":[{\"finish_reason\":\"stop\",\"log_probs\":null,\"index\":0,\"message\":{\"content\":\"{\\\"content\\\": [{\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\", \\\"title\\\": \\\"Troubleshooting and FAQ\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[API - /scores not respecting value when operator '='](https://github.com/orgs/langfuse/discussions/8770 \\\\\\\"Langfuse Support: API - /scores not respecting value when operator '='\\\\\\\")\\\\n[Does LangFuse support evaluations on an existing dataset (.csv)](https://github.com/orgs/langfuse/discussions/8665 \\\\\\\"Langfuse Support: Does LangFuse support evaluations on an existing dataset \\\\\\\\(.csv\\\\\\\\)\\\\\\\")\\\\n[deleting evaluators](https://github.com/orgs/langfuse/discussions/8640 \\\\\\\"Langfuse Support: deleting evaluators\\\\\\\")\\\\n[Not able to import my LLM-as-a-Judge evals](https://github.com/orgs/langfuse/discussions/8636 \\\\\\\"Langfuse Support: Not able to import my LLM-as-a-Judge evals\\\\\\\")\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590 \\\\\\\"Langfuse Support: How to get experiment run scores programmatically?\\\\\\\")\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556 \\\\\\\"Langfuse Support: Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations\\\\\\\")\\\\n[Dataset runs restore from backups](https://github.com/orgs/langfuse/discussions/8534 \\\\\\\"Langfuse Support: Dataset runs restore from backups\\\\\\\")\\\\n[Getting scores efficiently via API for analytics purposes](https://github.com/orgs/langfuse/discussions/8520 \\\\\\\"Langfuse Support: Getting scores efficiently via API for analytics purposes\\\\\\\")\\\\n[run experiment on dataset](https://github.com/orgs/langfuse/discussions/8433 \\\\\\\"Langfuse Support: run experiment on dataset\\\\\\\")\\\\n[Experiments on Datasets with Human Annotated Labels?](https://github.com/orgs/langfuse/discussions/8414 \\\\\\\"Langfuse Support: Experiments on Datasets with Human Annotated Labels?\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[How to Recalculate Total Score on Dashboard After Updating User-Defined Model?](https://github.com/orgs/langfuse/discussions/8375 \\\\\\\"Langfuse Support: How to Recalculate Total Score on Dashboard After Updating User-Defined Model?\\\\\\\")\\\\n[How to filter by Categorical Scores in custom dashboard?](https://github.com/orgs/langfuse/discussions/8356 \\\\\\\"Langfuse Support: How to filter by Categorical Scores in custom dashboard?\\\\\\\")\\\\n[Running scheduled evals utilising LangFuse Datasets & Evaluators](https://github.com/orgs/langfuse/discussions/8355 \\\\\\\"Langfuse Support: Running scheduled evals utilising LangFuse Datasets & Evaluators\\\\\\\")\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\(\\\\\\\\)` method\\\\\\\")\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\")\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\"Langfuse Support: \\\\\\\\[Experiment\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\")\\\\n[Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing\\\\\\\")\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[2 traces generated instead of 1](https://github.com/orgs/langfuse/discussions/2244 \\\\\\\"Langfuse Support: 2 traces generated instead of 1\\\\\\\")\\\\n[Evaluations Not Available in Self-Hosted Version?](https://github.com/orgs/langfuse/discussions/2130 \\\\\\\"Langfuse Support: Evaluations Not Available in Self-Hosted Version?\\\\\\\")\\\\n[Deleting Duplicate Items in a Dataset](https://github.com/orgs/langfuse/discussions/2099 \\\\\\\"Langfuse Support: Deleting Duplicate Items in a Dataset\\\\\\\")\\\\n[Availability of evals when self-hosting](https://github.com/orgs/langfuse/discussions/2042 \\\\\\\"Langfuse Support: Availability of evals when self-hosting\\\\\\\")\\\\n[How to utilize a dataset w/ typescript and langchain integration](https://github.com/orgs/langfuse/discussions/1969 \\\\\\\"Langfuse Support: How to utilize a dataset w/ typescript and langchain integration\\\\\\\")\\\\n[Scoring a trace after the LLM chain returns](https://github.com/orgs/langfuse/discussions/1610 \\\\\\\"Langfuse Support: Scoring a trace after the LLM chain returns\\\\\\\")\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486 \\\\\\\"Langfuse Support: Update/delete score using python sdk\\\\\\\")\\\\n[Linking dataset run items with existing callback handler](https://github.com/orgs/langfuse/discussions/1445 \\\\\\\"Langfuse Support: Linking dataset run items with existing callback handler\\\\\\\")\\\\n[Datasets list / by id](https://github.com/orgs/langfuse/discussions/1420 \\\\\\\"Langfuse Support: Datasets list / by id\\\\\\\")\\\\n[Run items not appearing when linking to a trace and not a span or a generation](https://github.com/orgs/langfuse/discussions/1357 \\\\\\\"Langfuse Support: Run items not appearing when linking to a trace and not a span or a generation\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Option to run Experiments without traces](https://github.com/orgs/langfuse/discussions/8133 \\\\\\\"Langfuse Ideas: Option to run Experiments without traces\\\\\\\")\\\\n[Auto-generate dataset items](https://github.com/orgs/langfuse/discussions/8126 \\\\\\\"Langfuse Ideas: Auto-generate dataset items\\\\\\\")\\\\n[feature: support for creation of custom) model adapters](https://github.com/orgs/langfuse/discussions/8123 \\\\\\\"Langfuse Ideas: feature: support for creation of custom\\\\\\\\) model adapters\\\\\\\")\\\\n[Prompt experiment result download.](https://github.com/orgs/langfuse/discussions/8120 \\\\\\\"Langfuse Ideas: Prompt experiment result download.\\\\\\\")\\\\n[Multiple predictions per-item in a single run](https://github.com/orgs/langfuse/discussions/8040 \\\\\\\"Langfuse Ideas: Multiple predictions per-item in a single run\\\\\\\")\\\\n[Add evaluator(s) to python SDK (FernLangfuse)](https://github.com/orgs/langfuse/discussions/8018 \\\\\\\"Langfuse Ideas: Add evaluator\\\\\\\\(s\\\\\\\\) to python SDK \\\\\\\\(FernLangfuse\\\\\\\\)\\\\\\\")\\\\n[feat(dataset-runs): return all dataset run item scores in a given run](https://github.com/orgs/langfuse/discussions/8011 \\\\\\\"Langfuse Ideas: feat\\\\\\\\(dataset-runs\\\\\\\\): return all dataset run item scores in a given run\\\\\\\")\\\\n[Delete an evaluator from the evaluator library](https://github.com/orgs/langfuse/discussions/7960 \\\\\\\"Langfuse Ideas: Delete an evaluator from the evaluator library\\\\\\\")\\\\n[Archive or Delete Evaluators from the Evaluator Library](https://github.com/orgs/langfuse/discussions/7868 \\\\\\\"Langfuse Ideas: Archive or Delete Evaluators from the Evaluator Library\\\\\\\")\\\\n[Edit dataset run name, description](https://github.com/orgs/langfuse/discussions/7814 \\\\\\\"Langfuse Ideas: Edit dataset run name, description\\\\\\\")\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\", \\\"title\\\": \\\"Datasets\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Datasets - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")Dataset RunsDatasets\\\\nCopy as Markdown\\\\n# Datasets\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\n## Why use datasets?\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\n  * Create test cases for your application with real production traces\\\\n  * Collaboratively create and collect dataset items with your team\\\\n  * Have a single source of truth for your test data\\\\n## Get Started\\\\n### Creating a dataset\\\\nDatasets have a name which is unique within a project.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset(\\\\n name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # optional description\\\\n description=\\\\\\\"My first dataset\\\\\\\",\\\\n # optional metadata\\\\n metadata={\\\\n \\\\\\\"author\\\\\\\": \\\\\\\"Alice\\\\\\\",\\\\n \\\\\\\"date\\\\\\\": \\\\\\\"2022-01-01\\\\\\\",\\\\n \\\\\\\"type\\\\\\\": \\\\\\\"benchmark\\\\\\\"\\\\n }\\\\n)\\\\n```\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\n    \\\\n    \\\\n```\\\\nimport { LangfuseClient } from \\\\\\\"@langfuse/client\\\\\\\"\\\\n \\\\nconst langfuse = new LangfuseClient()\\\\n \\\\nawait langfuse.api.datasets.create({\\\\n name: \\\\\\\"<dataset_name>\\\\\\\",\\\\n // optional description\\\\n description: \\\\\\\"My first dataset\\\\\\\",\\\\n // optional metadata\\\\n metadata: {\\\\n author: \\\\\\\"Alice\\\\\\\",\\\\n date: \\\\\\\"2022-01-01\\\\\\\",\\\\n type: \\\\\\\"benchmark\\\\\\\",\\\\n },\\\\n});\\\\n```\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\n### Create new dataset items\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset_item(\\\\n dataset_name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # any python object or value, optional\\\\n input={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # any python object or value, optional\\\\n expected_output={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # metadata, optional\\\\n metadata={\\\\n \\\\\\\"model\\\\\\\": \\\\\\\"llama3\\\\\\\",\\\\n }\\\\n)\\\\n```\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/observability/sdk/python/evaluation\\\", \\\"title\\\": \\\"Evaluations\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Evaluations with the Langfuse Python SDK - Langfuse\\\\nDocs[Observability](/docs/observability/overview \\\\\\\"Observability\\\\\\\")\\\\n[SDKs](/docs/observability/sdk/overview \\\\\\\"SDKs\\\\\\\")\\\\n[Python](/docs/observability/sdk/python/overview \\\\\\\"Python\\\\\\\")Evaluation\\\\nCopy as Markdown\\\\n# Evaluations\\\\nThe Python SDK provides ways to evaluate your application. You can add custom scores to your traces and observations, or use the SDK to execute Dataset Runs.\\\\nThis page shows the evaluation methods that are supported by the Python SDK. Please refer to the [Evaluation documentation](/docs/evaluation/overview) for more information on how to evaluate your application in Langfuse.\\\\n## Create Scores\\\\nLangfuseSpan / LangfuseGeneration object methodsContext-aware methodsLow-level method\\\\n  * `span_or_generation_obj.score()`: Scores the specific observation object.\\\\n  * `span_or_generation_obj.score_trace()`: Scores the entire trace to which the object belongs.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nwith langfuse.start_as_current_generation(name=\\\\\\\"summary_generation\\\\\\\") as gen:\\\\n # ... LLM call ...\\\\n gen.update(output=\\\\\\\"summary text...\\\\\\\")\\\\n # Score this specific generation\\\\n gen.score(name=\\\\\\\"conciseness\\\\\\\", value=0.8, data_type=\\\\\\\"NUMERIC\\\\\\\")\\\\n # Score the overall trace\\\\n gen.score_trace(name=\\\\\\\"user_feedback_rating\\\\\\\", value=\\\\\\\"positive\\\\\\\", data_type=\\\\\\\"CATEGORICAL\\\\\\\")\\\\n```\\\\n  * `langfuse.score_current_span()`: Scores the currently active observation in the context.\\\\n  * `langfuse.score_current_trace()`: Scores the trace of the currently active observation.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nwith langfuse.start_as_current_span(name=\\\\\\\"complex_task\\\\\\\") as task_span:\\\\n # ... perform task ...\\\\n langfuse.score_current_span(name=\\\\\\\"task_component_quality\\\\\\\", value=True, data_type=\\\\\\\"BOOLEAN\\\\\\\")\\\\n # ...\\\\n if task_is_fully_successful:\\\\n langfuse.score_current_trace(name=\\\\\\\"overall_success\\\\\\\", value=1.0, data_type=\\\\\\\"NUMERIC\\\\\\\")\\\\n```\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n  * Creates a score for a specified `trace_id` and optionally `observation_id`.\\\\n  * Useful when IDs are known, or for scoring after the trace/observation has completed.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nlangfuse.create_score(\\\\n name=\\\\\\\"fact_check_accuracy\\\\\\\",\\\\n value=0.95, ///# Can be float for NUMERIC/BOOLEAN, string for CATEGORICAL\\\\n trace_id=\\\\\\\"abcdef1234567890abcdef1234567890\\\\\\\",\\\\n observation_id=\\\\\\\"1234567890abcdef\\\\\\\", ///# Optional: if scoring a specific observation\\\\n session_id=\\\\\\\"session_123\\\\\\\", ///# Optional: if scoring a specific session\\\\n data_type=\\\\\\\"NUMERIC\\\\\\\", ///# \\\\\\\"NUMERIC\\\\\\\", \\\\\\\"BOOLEAN\\\\\\\", \\\\\\\"CATEGORICAL\\\\\\\"\\\\n comment=\\\\\\\"Source verified for 95% of claims.\\\\\\\"\\\\n)\\\\n```\\\\n**Score Parameters:**\\\\nParameter| Type| Description  \\\\n---|---|---  \\\\n`name`| `str`| Name of the score (e.g., \\\\u201crelevance\\\\u201d, \\\\u201caccuracy\\\\u201d). **Required.**  \\\\n`value`| `Union[float, str]`| Score value. Float for `NUMERIC`/`BOOLEAN`, string for `CATEGORICAL`. **Required.**  \\\\n`trace_id`| `str`| ID of the trace to associate with (for `create_score`). **Required.**  \\\\n`observation_id`| `Optional[str]`| ID of the specific observation to score (for `create_score`).  \\\\n`session_id`| `Optional[str]`| ID of the specific session to score (for `create_score`).  \\\\n`score_id`| `Optional[str]`| Custom ID for the score (auto-generated if None).  \\\\n`data_type`| `Optional[ScoreDataType]`| `\\\\\\\"NUMERIC\\\\\\\"`, `\\\\\\\"BOOLEAN\\\\\\\"`, or `\\\\\\\"CATEGORICAL\\\\\\\"`. Inferred if not provided based on value type and score config on server.  \\\\n`comment`| `Optional[str]`| Optional comment or explanation for the score.  \\\\n`config_id`| `Optional[str]`| Optional ID of a pre-defined score configuration in Langfuse.  \\\\n  \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThe most powerful way to use datasets is by linking your application\\\\u2019s executions (traces) to specific dataset items when performing an evaluation run. The `DatasetItemClient.run()` method provides a context manager to streamline this process.\\\\n**How`item.run()` works:**\\\\nWhen you use `with item.run(run_name=\\\\\\\"your_eval_run_name\\\\\\\") as root_span:`:\\\\n  1. **Trace Creation** : A new Langfuse trace is initiated specifically for processing this dataset item within the context of the named run.\\\\n  2. **Trace Naming & Metadata**: \\\\n    * The trace is automatically named (e.g., \\\\u201cDataset run: your_eval_run_name\\\\u201d).\\\\n    * Essential metadata is added to this trace, including `dataset_item_id` (the ID of `item`), `run_name`, and `dataset_id`.\\\\n  3. **DatasetRunItem Linking** : The SDK makes an API call to Langfuse to create a `DatasetRunItem`. This backend object formally links: \\\\n    * The `dataset_item_id`\\\\n    * The `trace_id` of the newly created trace\\\\n    * The provided `run_name`\\\\n    * Any `run_metadata` or `run_description` you pass to `item.run()`. This linkage is what populates the \\\\u201cRuns\\\\u201d tab for your dataset in the Langfuse UI, allowing you to see all traces associated with a particular evaluation run.\\\\n  4. **Contextual Span** : The context manager yields `root_span`, which is a `LangfuseSpan` object representing the root span of this new trace.\\\\n  5. **Automatic Nesting** : Any Langfuse observations (spans or generations) created _inside_ the `with` block will automatically become children of `root_span` and thus part of the trace linked to this dataset item and run.\\\\n**Example:**\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\ndataset_name = \\\\\\\"qna-eval\\\\\\\"\\\\ncurrent_run_name = \\\\\\\"qna_model_v3_run_05_20\\\\\\\" # Identifies this specific evaluation run\\\\n \\\\n///# Assume 'my_qna_app' is your instrumented application function\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"def my_qna_app(question: str, context: str, item_id: str, run_name: str):\\\\n with langfuse.start_as_current_generation(\\\\n name=\\\\\\\"qna-llm-call\\\\\\\",\\\\n input={\\\\\\\"question\\\\\\\": question, \\\\\\\"context\\\\\\\": context},\\\\n metadata={\\\\\\\"item_id\\\\\\\": item_id, \\\\\\\"run\\\\\\\": run_name}, ///# Example metadata for the generation\\\\n model=\\\\\\\"gpt-4o\\\\\\\"\\\\n ) as generation:\\\\n # Simulate LLM call\\\\n answer = f\\\\\\\"Answer to '{question}' using context.\\\\\\\" # Replace with actual LLM call\\\\n generation.update(output={\\\\\\\"answer\\\\\\\": answer})\\\\n \\\\n # Update the trace with the input and output\\\\n generation.update_trace(\\\\n input={\\\\\\\"question\\\\\\\": question, \\\\\\\"context\\\\\\\": context},\\\\n output={\\\\\\\"answer\\\\\\\": answer},\\\\n )\\\\n \\\\n return answer\\\\n \\\\ndataset = langfuse.get_dataset(name=dataset_name) ///# Fetch your pre-populated dataset\\\\n \\\\nfor item in dataset.items:\\\\n print(f\\\\\\\"Running evaluation for item: {item.id} (Input: {item.input})\\\\\\\")\\\\n \\\\n # Use the item.run() context manager\\\\n with item.run(\\\\n run_name=current_run_name,\\\\n run_metadata={\\\\\\\"model_provider\\\\\\\": \\\\\\\"OpenAI\\\\\\\", \\\\\\\"temperature_setting\\\\\\\": 0.7},\\\\n run_description=\\\\\\\"Evaluation run for Q&A model v3 on May 20th\\\\\\\"\\\\n ) as root_span: ///# root_span is the root span of the new trace for this item and run.\\\\n # All subsequent langfuse operations within this block are part of this trace.\\\\n \\\\n # Call your application logic\\\\n generated_answer = my_qna_app(\\\\n question=item.input[\\\\\\\"question\\\\\\\"],\\\\n context=item.input[\\\\\\\"context\\\\\\\"],\\\\n item_id=item.id,\\\\n run_name=current_run_name\\\\n )\\\\n \\\\n print(f\\\\\\\" Item {item.id} processed. Trace ID: {root_span.trace_id}\\\\\\\")\\\\n \\\\n # Optionally, score the result against the expected output\\\\n if item.expected_output and generated_answer == item.expected_output.get(\\\\\\\"answer\\\\\\\"):\\\\n root_span.score_trace(name=\\\\\\\"exact_match\\\\\\\", value=1.0)\\\\n else:\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n root_span.score_trace(name=\\\\\\\"exact_match\\\\\\\", value=0.0)\\\\n \\\\nprint(f\\\\\\\"\\\\\\\\nFinished processing dataset '{dataset_name}' for run '{current_run_name}'.\\\\\\\")\\\\n```\\\\nBy using `item.run()`, you ensure each dataset item\\\\u2019s processing is neatly encapsulated in its own trace, and these traces are aggregated under the specified `run_name` in the Langfuse UI. This allows for systematic review of results, comparison across runs, and deep dives into individual processing traces.\\\\nLast updated on August 20, 2025\\\\n[Instrumentation](/docs/observability/sdk/python/instrumentation \\\\\\\"Instrumentation\\\\\\\")\\\\n[Advanced usage](/docs/observability/sdk/python/advanced-usage \\\\\\\"Advanced usage\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\", \\\"title\\\": \\\"All new Datasets, Experimentation and Evaluation documentation\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"## New Datasets and Evaluation documentation\\\\nBased on this week\\\\u2019s changes, we have have restructured the documentation to:\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\n### Evaluation\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Evaluation documentation](/docs/scores)\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\n    \\\\n    \\\\n```\\\\nPOST /api/public/scores\\\\n```\\\\nAdd scores via Python or JS SDK.\\\\nExample (Python)\\\\n    \\\\n    \\\\n```\\\\nlangfuse.score(\\\\n trace_id=\\\\\\\"123\\\\\\\",\\\\n name=\\\\\\\"my_custom_evaluator\\\\\\\",\\\\n value=0.5,\\\\n)\\\\n```\\\\n### Datasets\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\", \\\"title\\\": \\\"Remote Dataset Runs\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Task function that answers geography questions\\\\\\\"\\\\\\\"\\\\\\\"\\\\n question = item[\\\\\\\"input\\\\\\\"]\\\\n response = OpenAI().chat.completions.create(\\\\n model=\\\\\\\"gpt-4\\\\\\\",\\\\n messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": question}]\\\\n )\\\\n return response.choices[0].message.content\\\\n \\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\"\\\\\\\"\\\\\\\"\\\\n if expected_output and expected_output.lower() in output.lower():\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=1.0)\\\\n \\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=0.0)\\\\n \\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\"\\\\\\\"\\\\\\\"\\\\n accuracies = [\\\\n eval.value for result in item_results\\\\n for eval in result.evaluations if eval.name == \\\\\\\"accuracy\\\\\\\"\\\\n ]\\\\n \\\\n if not accuracies:\\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=None)\\\\n \\\\n avg = sum(accuracies) / len(accuracies)\\\\n \\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=avg, comment=f\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\")\\\\n \\\\n@pytest.fixture\\\\ndef langfuse_client():\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Initialize Langfuse client for testing\\\\\\\"\\\\\\\"\\\\\\\"\\\\n return get_client()\\\\n \\\\ndef test_geography_accuracy_passes(langfuse_client):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Test that passes when accuracy is above threshold\\\\\\\"\\\\\\\"\\\\\\\"\\\\n result = langfuse_client.run_experiment(\\\\n name=\\\\\\\"Geography Test - Should Pass\\\\\\\",\\\\n data=test_data,\\\\n task=geography_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy_evaluator]\\\\n )\\\\n \\\\n # Access the run evaluator result directly\\\\n avg_accuracy = next(\\\\n eval.value for eval in result.run_evaluations\\\\n if eval.name == \\\\\\\"avg_accuracy\\\\\\\"\\\\n )\\\\n \\\\n # Assert minimum accuracy threshold\\\\n assert avg_accuracy >= 0.8, f\\\\\\\"Average accuracy {avg_accuracy:.2f} below threshold 0.8\\\\\\\"\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\ndef test_geography_accuracy_fails(langfuse_client):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Example test that demonstrates failure conditions\\\\\\\"\\\\\\\"\\\\\\\"\\\\n # Use a weaker model or harder questions to demonstrate test failure\\\\n def failing_task(*, item, **kwargs):\\\\n # Simulate a task that gives wrong answers\\\\n return \\\\\\\"I don't know\\\\\\\"\\\\n \\\\n result = langfuse_client.run_experiment(\\\\n name=\\\\\\\"Geography Test - Should Fail\\\\\\\",\\\\n data=test_data,\\\\n task=failing_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy_evaluator]\\\\n )\\\\n \\\\n # Access the run evaluator result directly\\\\n avg_accuracy = next(\\\\n eval.value for eval in result.run_evaluations\\\\n if eval.name == \\\\\\\"avg_accuracy\\\\\\\"\\\\n )\\\\n \\\\n # This test will fail because the task gives wrong answers\\\\n with pytest.raises(AssertionError):\\\\n assert avg_accuracy >= 0.8, f\\\\\\\"Expected test to fail with low accuracy: {avg_accuracy:.2f}\\\\\\\"\\\\n```\\\\n    \\\\n    \\\\n```\\\\n// test/geography-experiment.test.ts\\\\nimport { describe, it, expect, beforeAll, afterAll } from 'vitest';\\\\nimport { OpenAI } from \\\\\\\"openai\\\\\\\";\\\\nimport { NodeSDK } from \\\\\\\"@opentelemetry/sdk-node\\\\\\\";\\\\nimport { LangfuseClient, ExperimentItem } from \\\\\\\"@langfuse/client\\\\\\\";\\\\nimport { observeOpenAI } from \\\\\\\"@langfuse/openai\\\\\\\";\\\\nimport { LangfuseSpanProcessor } from \\\\\\\"@langfuse/otel\\\\\\\";\\\\n \\\\n// Test data for European capitals\\\\nconst testData: ExperimentItem[] = [\\\\n { input: \\\\\\\"What is the capital of France?\\\\\\\", expected: \\\\\\\"Paris\\\\\\\" },\\\\n { input: \\\\\\\"What is the capital of Germany?\\\\\\\", expected: \\\\\\\"Berlin\\\\\\\" },\\\\n { input: \\\\\\\"What is the capital of Spain?\\\\\\\", expected: \\\\\\\"Madrid\\\\\\\" },\\\\n];\\\\n \\\\nlet otelSdk: NodeSDK;\\\\nlet langfuse: LangfuseClient;\\\\n \\\\nbeforeAll(async () => {\\\\n // Initialize OpenTelemetry\\\\n otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\n otelSdk.start();\\\\n \\\\n // Initialize Langfuse client\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\n    \\\\n    \\\\n```\\\\nimport { Factuality, Levenshtein } from \\\\\\\"autoevals\\\\\\\";\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\"@langfuse/client\\\\\\\";\\\\n \\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\n \\\\n// Use with additional parameters\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\n Factuality,\\\\n { model: \\\\\\\"gpt-4o\\\\\\\" } // Additional AutoEvals parameters\\\\n);\\\\n \\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"AutoEvals Integration Test\\\\\\\",\\\\n data: testDataset,\\\\n task: myTask,\\\\n evaluators: [\\\\n factualityEvaluator,\\\\n levenshteinEvaluator,\\\\n customFactualityEvaluator\\\\n ]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\", \\\"title\\\": \\\"Remote Dataset Runs\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Remote Dataset Runs via the SDK - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\"Dataset Runs\\\\\\\")Remote Run\\\\nCopy as Markdown\\\\n# Remote Dataset Runs\\\\nOnce you [created a dataset](/docs/evaluation/dataset-runs/datasets), you can use the dataset to test how your application performs on different inputs. Remote Dataset Runs are used to programmatically loop your applications or prompts through a dataset and optionally apply Evaluation Methods to the results.\\\\nThey are called \\\\u201cRemote Dataset Runs\\\\u201d because they can make use of \\\\u201cremote\\\\u201d or external logic and code.\\\\nOptionally, you can also trigger Remote Dataset Runs via the Langfuse UI which will call them via a webhook.\\\\n## Why use Remote Dataset Runs?\\\\n  * Full flexibility to use your own application logic\\\\n  * Use custom scoring functions to evaluate the outputs\\\\n  * Run multiple experiments on the same dataset in parallel\\\\n  * Easy to integrate with your existing evaluation infrastructure\\\\nSequence Diagram\\\\n## Experiment runner SDK\\\\nBoth the Python and JS/TS SDKs provide a high-level abstraction for running an experiment on a dataset. The dataset can be both local or hosted on Langfuse. Using the Experiment runner is the recommended way to run an experiment on a dataset with our SDK.\\\\nThe experiment runner automatically handles:\\\\n  * **Concurrent execution** of tasks with configurable limits\\\\n  * **Automatic tracing** of all executions for observability\\\\n  * **Flexible evaluation** with both item-level and run-level evaluators\\\\n  * **Error isolation** so individual failures don\\\\u2019t stop the experiment\\\\n  * **Dataset integration** for easy comparison and tracking\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThe experiment runner SDK supports both datasets hosted on Langfuse and datasets hosted locally. If you are using a dataset hosted on Langfuse for your experiment, the SDK will automatically create a dataset run for you that you can inspect and compare in the Langfuse UI. For locally hosted datasets not on Langfuse, only traces and scores (if evaluations are used) are tracked in Langfuse.\\\\n### Basic Usage\\\\nStart with the simplest possible experiment to test your task function on local data. If you already have a dataset in Langfuse, see here.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\nfrom langfuse.openai import OpenAI\\\\n \\\\n///# Initialize client\\\\nlangfuse = get_client()\\\\n \\\\n///# Define your task function\\\\ndef my_task(*, item, **kwargs):\\\\n question = item[\\\\\\\"input\\\\\\\"]\\\\n response = OpenAI().chat.completions.create(\\\\n model=\\\\\\\"gpt-4.1\\\\\\\", messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": question}]\\\\n )\\\\n \\\\n return response.choices[0].message.content\\\\n \\\\n \\\\n///# Run experiment on local data\\\\nlocal_data = [\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of France?\\\\\\\"},\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of Germany?\\\\\\\"},\\\\n]\\\\n \\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Geography Quiz\\\\\\\",\\\\n description=\\\\\\\"Testing basic functionality\\\\\\\",\\\\n data=local_data,\\\\n task=my_task,\\\\n)\\\\n \\\\n///# Use format method to display results\\\\nprint(result.format())\\\\n```\\\\nMake sure that OpenTelemetry is properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\n    \\\\n    \\\\n```\\\\nimport { OpenAI } from \\\\\\\"openai\\\\\\\";\\\\nimport { NodeSDK } from \\\\\\\"@opentelemetry/sdk-node\\\\\\\";\\\\n \\\\nimport {\\\\n LangfuseClient,\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n ExperimentTask,\\\\n ExperimentItem,\\\\n} from \\\\\\\"@langfuse/client\\\\\\\";\\\\nimport { observeOpenAI } from \\\\\\\"@langfuse/openai\\\\\\\";\\\\nimport { LangfuseSpanProcessor } from \\\\\\\"@langfuse/otel\\\\\\\";\\\\n \\\\n// Initialize OpenTelemetry\\\\nconst otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\notelSdk.start();\\\\n \\\\n// Initialize client\\\\nconst langfuse = new LangfuseClient();\\\\n \\\\n// Run experiment on local data\\\\nconst localData: ExperimentItem[] = [\\\\n { input: \\\\\\\"What is the capital of France?\\\\\\\" },\\\\n { input: \\\\\\\"What is the capital of Germany?\\\\\\\" },\\\\n];\\\\n \\\\n// Define your task function\\\\nconst myTask: ExperimentTask = async (item) => {\\\\n const question = item.input;\\\\n \\\\n const response = await observeOpenAI(new OpenAI()).chat.completions.create({\\\\n model: \\\\\\\"gpt-4.1\\\\\\\",\\\\n messages: [\\\\n {\\\\n role: \\\\\\\"user\\\\\\\",\\\\n content: question,\\\\n },\\\\n ],\\\\n });\\\\n \\\\n return response;\\\\n};\\\\n \\\\n// Run the experiment\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Geography Quiz\\\\\\\",\\\\n description: \\\\\\\"Testing basic functionality\\\\\\\",\\\\n data: localData,\\\\n task: myTask,\\\\n});\\\\n \\\\n// Print formatted result\\\\nconsole.log(await result.format());\\\\n \\\\n// Important: shut down OTEL SDK to deliver traces\\\\nawait otelSdk.shutdown();\\\\n```\\\\n **Note for JS/TS SDK** : OpenTelemetry must be properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\nWhen running experiments on local data, only traces are created in Langfuse - no dataset runs are generated. Each task execution creates an individual trace for observability and debugging.\\\\n### Usage with Langfuse Datasets\\\\nRun experiments directly on datasets stored in Langfuse for automatic tracing and comparison.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\n///# Get dataset from Langfuse\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\ndataset = langfuse.get_dataset(\\\\\\\"my-evaluation-dataset\\\\\\\")\\\\n \\\\n///# Run experiment directly on the dataset\\\\nresult = dataset.run_experiment(\\\\n name=\\\\\\\"Production Model Test\\\\\\\",\\\\n description=\\\\\\\"Monthly evaluation of our production model\\\\\\\",\\\\n task=my_task\\\\n)\\\\n \\\\n///# Use format method to display results\\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\n// Get dataset from Langfuse\\\\nconst dataset = await langfuse.dataset.get(\\\\\\\"my-evaluation-dataset\\\\\\\");\\\\n \\\\n// Run experiment directly on the dataset\\\\nconst result = await dataset.runExperiment({\\\\n name: \\\\\\\"Production Model Test\\\\\\\",\\\\n description: \\\\\\\"Monthly evaluation of our production model\\\\\\\",\\\\n task: myTask\\\\n});\\\\n \\\\n// Use format method to display results\\\\nconsole.log(await result.format());\\\\n \\\\n// Important: shut down OpenTelemetry to ensure traces are sent to Langfuse\\\\nawait otelSdk.shutdown()\\\\n```\\\\nWhen using Langfuse datasets, dataset runs are automatically created in Langfuse and are available for comparison in the UI. This enables tracking experiment performance over time and comparing different approaches on the same dataset.\\\\n### Advanced Features\\\\nEnhance your experiments with evaluators and advanced configuration options.\\\\n#### Evaluators\\\\nEvaluators assess the quality of task outputs at the item level. They receive the input, metadata, output, and expected output for each item and return evaluation metrics that are reported as scores on the traces in Langfuse.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import Evaluation\\\\n \\\\n///# Define evaluation functions\\\\ndef accuracy_evaluator(*, input, output, expected_output, metadata, **kwargs):\\\\n if expected_output and expected_output.lower() in output.lower():\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=1.0, comment=\\\\\\\"Correct answer found\\\\\\\")\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=0.0, comment=\\\\\\\"Incorrect answer\\\\\\\")\\\\n \\\\ndef length_evaluator(*, input, output, **kwargs):\\\\n return Evaluation(name=\\\\\\\"response_length\\\\\\\", value=len(output), comment=f\\\\\\\"Response has {len(output)} characters\\\\\\\")\\\\n \\\\n///# Use multiple evaluators\\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Multi-metric Evaluation\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[accuracy_evaluator, length_evaluator]\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\n// Define evaluation functions\\\\nconst accuracyEvaluator = async ({ input, output, expectedOutput }) => {\\\\n if (expectedOutput && output.toLowerCase().includes(expectedOutput.toLowerCase())) {\\\\n return {\\\\n name: \\\\\\\"accuracy\\\\\\\",\\\\n value: 1.0,\\\\n comment: \\\\\\\"Correct answer found\\\\\\\"\\\\n };\\\\n }\\\\n return {\\\\n name: \\\\\\\"accuracy\\\\\\\",\\\\n value: 0.0,\\\\n comment: \\\\\\\"Incorrect answer\\\\\\\"\\\\n };\\\\n};\\\\n \\\\nconst lengthEvaluator = async ({ input, output }) => {\\\\n return {\\\\n name: \\\\\\\"response_length\\\\\\\",\\\\n value: output.length,\\\\n comment: `Response has ${output.length} characters`\\\\n };\\\\n};\\\\n \\\\n// Use multiple evaluators\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Multi-metric Evaluation\\\\\\\",\\\\n data: testData,\\\\n task: myTask,\\\\n evaluators: [accuracyEvaluator, lengthEvaluator]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Run-level Evaluators\\\\nRun-level evaluators assess the full experiment results and compute aggregate metrics. When run on Langfuse datasets, these scores are attached to the full dataset run for tracking overall experiment performance.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import Evaluation\\\\n \\\\ndef average_accuracy(*, item_results, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Calculate average accuracy across all items\\\\\\\"\\\\\\\"\\\\\\\"\\\\n accuracies = [\\\\n eval.value for result in item_results\\\\n for eval in result.evaluations\\\\n if eval.name == \\\\\\\"accuracy\\\\\\\"\\\\n ]\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n if not accuracies:\\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=None)\\\\n \\\\n avg = sum(accuracies) / len(accuracies)\\\\n \\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=avg, comment=f\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\")\\\\n \\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Comprehensive Analysis\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy]\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst averageAccuracy = async ({ itemResults }) => {\\\\n // Calculate average accuracy across all items\\\\n const accuracies = itemResults\\\\n .flatMap(result => result.evaluations)\\\\n .filter(evaluation => evaluation.name === \\\\\\\"accuracy\\\\\\\")\\\\n .map(evaluation => evaluation.value as number);\\\\n \\\\n if (accuracies.length === 0) {\\\\n return { name: \\\\\\\"avg_accuracy\\\\\\\", value: null };\\\\n }\\\\n \\\\n const avg = accuracies.reduce((sum, val) => sum + val, 0) / accuracies.length;\\\\n \\\\n return {\\\\n name: \\\\\\\"avg_accuracy\\\\\\\",\\\\n value: avg,\\\\n comment: `Average accuracy: ${(avg * 100).toFixed(1)}%`\\\\n };\\\\n};\\\\n \\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Comprehensive Analysis\\\\\\\",\\\\n data: testData,\\\\n task: myTask,\\\\n evaluators: [accuracyEvaluator],\\\\n runEvaluators: [averageAccuracy]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Async Tasks and Evaluators\\\\nBoth task functions and evaluators can be asynchronous.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nimport asyncio\\\\nfrom langfuse.openai import AsyncOpenAI\\\\n \\\\nasync def async_llm_task(*, item, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Async task using OpenAI\\\\\\\"\\\\\\\"\\\\\\\"\\\\n client = AsyncOpenAI()\\\\n response = await client.chat.completions.create(\\\\n model=\\\\\\\"gpt-4\\\\\\\",\\\\n messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": item[\\\\\\\"input\\\\\\\"]}]\\\\n )\\\\n \\\\n return response.choices[0].message.content\\\\n \\\\n///# Works seamlessly with async functions\\\\nresult = langfuse.run_experiment(\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n name=\\\\\\\"Async Experiment\\\\\\\",\\\\n data=test_data,\\\\n task=async_llm_task,\\\\n max_concurrency=5 # Control concurrent API calls\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\nimport OpenAI from \\\\\\\"openai\\\\\\\";\\\\n \\\\nconst asyncLlmTask = async (item) => {\\\\n // Async task using OpenAI\\\\n const client = new OpenAI();\\\\n const response = await client.chat.completions.create({\\\\n model: \\\\\\\"gpt-4\\\\\\\",\\\\n messages: [{ role: \\\\\\\"user\\\\\\\", content: item.input }]\\\\n });\\\\n \\\\n return response.choices[0].message.content;\\\\n};\\\\n \\\\n// Works seamlessly with async functions\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Async Experiment\\\\\\\",\\\\n data: testData,\\\\n task: asyncLlmTask,\\\\n maxConcurrency: 5 // Control concurrent API calls\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Configuration Options\\\\nCustomize experiment behavior with various configuration options.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Configurable Experiment\\\\\\\",\\\\n run_name=\\\\\\\"Custom Run Name\\\\\\\", ///# will be dataset run name if dataset is used\\\\n description=\\\\\\\"Experiment with custom configuration\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy],\\\\n max_concurrency=10, ///# Max concurrent executions\\\\n metadata={ ///# Attached to all traces\\\\n \\\\\\\"model\\\\\\\": \\\\\\\"gpt-4\\\\\\\",\\\\n \\\\\\\"temperature\\\\\\\": 0.7,\\\\n \\\\\\\"version\\\\\\\": \\\\\\\"v1.2.0\\\\\\\"\\\\n }\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Configurable Experiment\\\\\\\",\\\\n runName: \\\\\\\"Custom Run Name\\\\\\\", // will be dataset run name if dataset is used\\\\n description: \\\\\\\"Experiment with custom configuration\\\\\\\",\\\\n data: testData,\\\\n task: myTask,\\\\n evaluators: [accuracyEvaluator],\\\\n runEvaluators: [averageAccuracy],\\\\n maxConcurrency: 10, // Max concurrent executions\\\\n metadata: { // Attached to all traces\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n model: \\\\\\\"gpt-4\\\\\\\",\\\\n temperature: 0.7,\\\\n version: \\\\\\\"v1.2.0\\\\\\\"\\\\n }\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Testing in CI Environments\\\\nIntegrate the experiment runner with testing frameworks like Pytest and Vitest to run automated evaluations in your CI pipeline. Use evaluators to create assertions that can fail tests based on evaluation results.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\n///# test_geography_experiment.py\\\\nimport pytest\\\\nfrom langfuse import get_client, Evaluation\\\\nfrom langfuse.openai import OpenAI\\\\n \\\\n///# Test data for European capitals\\\\ntest_data = [\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of France?\\\\\\\", \\\\\\\"expected\\\\\\\": \\\\\\\"Paris\\\\\\\"},\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of Germany?\\\\\\\", \\\\\\\"expected\\\\\\\": \\\\\\\"Berlin\\\\\\\"},\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of Spain?\\\\\\\", \\\\\\\"expected\\\\\\\": \\\\\\\"Madrid\\\\\\\"},\\\\n]\\\\n \\\\ndef geography_task(*, item, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Task function that answers geography questions\\\\\\\"\\\\\\\"\\\\\\\"\\\\n question = item[\\\\\\\"input\\\\\\\"]\\\\n response = OpenAI().chat.completions.create(\\\\n model=\\\\\\\"gpt-4\\\\\\\",\\\\n messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": question}]\\\\n )\\\\n return response.choices[0].message.content\\\\n \\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\"\\\\\\\"\\\\\\\"\\\\n if expected_output and expected_output.lower() in output.lower():\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=1.0)\\\\n \\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=0.0)\\\\n \\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\"\\\\\\\"\\\\\\\"\\\\n accuracies = [\\\\n eval.value for result in item_results\\\\n for eval in result.evaluations if eval.name == \\\\\\\"accuracy\\\\\\\"\\\\n ]\\\\n \\\\n if not accuracies:\\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=None)\\\\n \\\\n avg = sum(accuracies) / len(accuracies)\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThese examples show how to use the experiment runner\\\\u2019s evaluation results to create meaningful test assertions in your CI pipeline. Tests can fail when accuracy drops below acceptable thresholds, ensuring model quality standards are maintained automatically.\\\\n### Autoevals Integration\\\\nAccess pre-built evaluation functions through the [autoevals library](https://github.com/braintrustdata/autoevals) integration.\\\\nPython SDKJS/TS SDK\\\\nThe Python SDK supports AutoEvals evaluators through direct integration:\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse.experiment import create_evaluator_from_autoevals\\\\nfrom autoevals.llm import Factuality\\\\n \\\\nevaluator = create_evaluator_from_autoevals(Factuality())\\\\n \\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Autoevals Integration Test\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[evaluator]\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\n    \\\\n    \\\\n```\\\\nimport { Factuality, Levenshtein } from \\\\\\\"autoevals\\\\\\\";\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\"@langfuse/client\\\\\\\";\\\\n \\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\n \\\\n// Use with additional parameters\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\n Factuality,\\\\n { model: \\\\\\\"gpt-4o\\\\\\\" } // Additional AutoEvals parameters\\\\n);\\\\n \\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"AutoEvals Integration Test\\\\\\\",\\\\n data: testDataset,\\\\n task: myTask,\\\\n evaluators: [\\\\n factualityEvaluator,\\\\n levenshteinEvaluator,\\\\n customFactualityEvaluator\\\\n ]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n## Low-level SDK methods\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\n comment: \\\\\\\"This is a comment\\\\\\\", // optional, useful to add reasoning\\\\n });\\\\n}\\\\n \\\\n// Flush the langfuse client to ensure all score data is sent to the server at the end of the experiment run\\\\nawait langfuse.flush();\\\\n```\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\nfrom langfuse.langchain import CallbackHandler\\\\n///#from .app import my_llm_application\\\\n \\\\n///# Load the dataset\\\\ndataset = get_client().get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\n///# Initialize the Langfuse handler\\\\nlangfuse_handler = CallbackHandler()\\\\n \\\\n///# Loop over the dataset items\\\\nfor item in dataset.items:\\\\n # Use the item.run() context manager for automatic trace linking\\\\n with item.run(\\\\n run_name=\\\\\\\"<run_name>\\\\\\\",\\\\n run_description=\\\\\\\"My first run\\\\\\\",\\\\n run_metadata={\\\\\\\"model\\\\\\\": \\\\\\\"llama3\\\\\\\"},\\\\n ) as root_span:\\\\n # Execute your LLM-app against the dataset item input\\\\n output = my_langchain_chain(item.input, langfuse_handler)\\\\n \\\\n # Update top-level trace input and output\\\\n root_span.update_trace(input=item.input, output=output.content)\\\\n \\\\n # Optionally: Add scores computed in your experiment runner, e.g. json equality check\\\\n root_span.score_trace(\\\\n name=\\\\\\\"<example_eval>\\\\\\\",\\\\n value=my_eval_fn(item.input, output, item.expected_output),\\\\n comment=\\\\\\\"This is a comment\\\\\\\", ///# optional, useful to add reasoning\\\\n )\\\\n \\\\n///# Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run\\\\nget_client().flush()\\\\n```\\\\n    \\\\n    \\\\n```\\\\nimport { LangfuseClient } from \\\\\\\"@langfuse/client\\\\\\\";\\\\nimport { CallbackHandler } from \\\\\\\"@langfuse/langchain\\\\\\\";\\\\n...\\\\n \\\\nconst langfuse = new LangfuseClient()\\\\nconst runName = \\\\\\\"my-dataset-run\\\\\\\";\\\\nfor (const item of dataset.items) {\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/guides/cookbook/example_external_evaluation_pipelines\\\", \\\"title\\\": \\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline - Langfuse\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\"Cookbooks\\\\\\\")External Evaluation Pipelines\\\\nCopy as Markdown\\\\nThis is a Jupyter notebook\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\n# Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\\nThis cookbook explains how to build an external evaluation pipeline to measure the performance of your production LLM application using Langfuse.\\\\nAs a rule of thumb, we encourage you to check first if the [evaluations in the Langfuse UI](https://langfuse.com/docs/scores/model-based-evals) cover your use case. If your needs go beyond these, you can still implement in Langfuse custom evaluation templates without code.\\\\nConsider implementing an external evaluation pipeline if you need:\\\\n  * More control over **when** traces get evaluated. You could schedule the pipeline to run at specific times or responding to event-based triggers like Webhooks.\\\\n  * Greater flexibility with your custom evaluations, when your needs go beyond what\\\\u2019s possible with the Langfuse UI\\\\n  * Version control for your custom evaluations\\\\n  * The ability to evaluate data using existing evaluation frameworks\\\\nIf your use case meets any of this situations, let\\\\u2019s go ahead and implement your first external evaluation pipeline!\\\\n* * *\\\\nBy the end of this cookbook, you\\\\u2019ll be able to:\\\\n  * Create a synthetic dataset to test your models.\\\\n  * Use the Langfuse client to gather and filter traces of previous model runs\\\\n  * Evaluate these traces offline and incrementally\\\\n  * Add scores to existing Langfuse traces\\\\nConceptually, we will implement the following architecture:\\\\n* * *\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n**Note** : While we\\\\u2019re using a Jupyter notebook for this cookbook, in production you\\\\u2019d use your preferred orchestration tool. Just make sure to extract the code into a .py file and ensure all dependencies are available at runtime.\\\\n## (Prep-work) Loading synthetic traces to Langfuse\\\\nIn this demo, we\\\\u2019ll build a mock application: a science communicator LLM that explains any topic in an engaging and approachable way.\\\\nSince we don\\\\u2019t have real user data, our first step is to create a synthetic dataset. We\\\\u2019ll generate a variety of potential questions that real users might ask. While this is a great way to kickstart your LLM development, collecting real user queries as soon as possible is invaluable.\\\\nYou can get your Langfuse API keys [here](https://cloud.langfuse.com/) and OpenAI API key [here](https://platform.openai.com/api-keys)\\\\n    \\\\n    \\\\n```\\\\n%pip install langfuse openai deepeval --upgrade\\\\n```\\\\n    \\\\n    \\\\n```\\\\nimport os\\\\n \\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\nos.environ[\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\"] = \\\\\\\"pk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\"] = \\\\\\\"sk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddea\\\\ud83c\\\\uddfa EU region\\\\n///# os.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://us.cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddfa\\\\ud83c\\\\uddf8 US region\\\\n \\\\n///# Your openai key\\\\nos.environ[\\\\\\\"OPENAI_API_KEY\\\\\\\"] = \\\\\\\"sk-proj-...\\\\\\\"\\\\n```\\\\nLet\\\\u2019s go ahead and generate a list of topic suggestions that we can later query to our application.\\\\n    \\\\n    \\\\n```\\\\nimport openai\\\\n \\\\ntopic_suggestion = \\\\\\\"\\\\\\\"\\\\\\\" You're a world-class journalist, specialized\\\\nin figuring out which are the topics that excite people the most.\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nBATCH_SIZE = 10\\\\nTOTAL_TRACES = 50\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nnow = datetime.now()\\\\nfive_am_today = datetime(now.year, now.month, now.day, 5, 0)\\\\nfive_am_yesterday = five_am_today - timedelta(days=1)\\\\n \\\\ntraces_batch = langfuse.api.trace.list(page=1,\\\\n limit=BATCH_SIZE,\\\\n tags=\\\\\\\"ext_eval_pipelines\\\\\\\",\\\\n from_timestamp=five_am_yesterday,\\\\n to_timestamp=datetime.now()\\\\n ).data\\\\n \\\\nprint(f\\\\\\\"Traces in first batch: {len(traces_batch)}\\\\\\\")\\\\n```\\\\nTraces in first batch: 10\\\\n## 2\\\\\\\\. Run your evaluations\\\\nLangfuse can handle numerical, boolean and categorical (`string`) scores. Wrapping your custom evaluation logic in a function is often a good practice. Evaluation functions should take a `trace` as input and yield a valid score. Let\\\\u2019s begin with a simple example using a categorical score.\\\\n### 2.1. Categoric Evaluations\\\\nWhen analyzing the outputs of your LLM applications, you may want to evaluate traits that are best defined qualitatively, such as sentiment, tonality or text complexity (Grade level).\\\\nWe\\\\u2019re building a science educator LLM that should sound engaging and positive. To ensure it hits the right notes, we\\\\u2019ll evaluate the tone of its outputs to see if they match our intent. We\\\\u2019ll draft an evaluation prompt ourselves (no library) to identify the three main tones in each model output.\\\\n    \\\\n    \\\\n```\\\\ntemplate_tone_eval = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nYou're an expert in human emotional intelligence. You can identify with ease the\\\\n tone in human-written text. Your task is to identify the tones present in a\\\\n piece of <text/> with precission. Your output is a comma separated list of three\\\\n tones. PRINT THE LIST ALONE, NOTHING ELSE.\\\\n \\\\n<possible_tones>\\\\nneutral, confident, joyful, optimistic, friendly, urgent, analytical, respectful\\\\n</possible_tones>\\\\n \\\\n<example_1>\\\\nInput: Citizen science plays a crucial role in research by involving everyday\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\npeople in scientific projects. This collaboration allows researchers to collect\\\\nvast amounts of data that would be impossible to gather on their own. Citizen\\\\nscientists contribute valuable observations and insights that can lead to new\\\\ndiscoveries and advancements in various fields. By participating in citizen\\\\nscience projects, individuals can actively contribute to scientific research\\\\nand make a meaningful impact on our understanding of the world around us.\\\\n \\\\nOutput: respectful,optimistic,confident\\\\n</example_1>\\\\n \\\\n<example_2>\\\\nInput: Bionics is a field that combines biology and engineering to create\\\\ndevices that can enhance human abilities. By merging humans and machines,\\\\nbionics aims to improve quality of life for individuals with disabilities\\\\nor enhance performance for others. These technologies often mimic natural\\\\nprocesses in the body to create seamless integration. Overall, bionics holds\\\\ngreat potential for revolutionizing healthcare and technology in the future.\\\\n \\\\nOutput: optimistic,confident,analytical\\\\n</example_2>\\\\n \\\\n<example_3>\\\\nInput: Social media can have both positive and negative impacts on mental\\\\nhealth. On the positive side, it can help people connect, share experiences,\\\\nand find support. However, excessive use of social media can also lead to\\\\nfeelings of inadequacy, loneliness, and anxiety. It's important to find a\\\\nbalance and be mindful of how social media affects your mental well-being.\\\\nRemember, it's okay to take breaks and prioritize your mental health.\\\\n \\\\nOutput: friendly,neutral,respectful\\\\n</example_3>\\\\n \\\\n<text>\\\\n{text}\\\\n</text>\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n \\\\n \\\\ntest_tone_score = openai.chat.completions.create(\\\\n messages=[\\\\n {\\\\n \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\",\\\\n \\\\\\\"content\\\\\\\": template_tone_eval.format(\\\\n text=traces_batch[1].output),\\\\n }\\\\n ],\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n model=\\\\\\\"gpt-4o\\\\\\\",\\\\n \\\\n temperature=0\\\\n).choices[0].message.content\\\\nprint(f\\\\\\\"User query: {traces_batch[1].input['args'][0]}\\\\\\\")\\\\nprint(f\\\\\\\"Model answer: {traces_batch[1].output}\\\\\\\")\\\\nprint(f\\\\\\\"Dominant tones: {test_tone_score}\\\\\\\")\\\\n```\\\\nIdentifying human intents and tones can be tricky for language models. To handle this, we used a multi-shot prompt, which means giving the model several examples to learn from. Now let\\\\u2019s wrap our code in an evaluation function for convenience.\\\\n    \\\\n    \\\\n```\\\\ndef tone_score(trace):\\\\n return openai.chat.completions.create(\\\\n messages=[\\\\n {\\\\n \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\",\\\\n \\\\\\\"content\\\\\\\": template_tone_eval.format(text=trace.output),\\\\n }\\\\n ],\\\\n model=\\\\\\\"gpt-4o\\\\\\\",\\\\n temperature=0\\\\n ).choices[0].message.content\\\\n \\\\ntone_score(traces_batch[1])\\\\n```\\\\nGreat! Now let\\\\u2019s go ahead and create a numeric evaluation score.\\\\n### 2.2. Numeric Evaluations\\\\nIn this cookbook, we\\\\u2019ll use the `Deepeval` framework ([docs](https://docs.confident-ai.com/docs/getting-started)) to handle our numeric evaluations. Deepeval provides scores ranging from zero to one for many common LLM metrics. Plus, you can create custom metrics by simply describing them in plain language. To ensure our app\\\\u2019s responses are joyful and engaging, we\\\\u2019ll define a custom \\\\u2018joyfulness\\\\u2019 score.\\\\nYou can use any evaluation library. These are popular ones:\\\\n  * OpenAI Evals ([GitHub](https://github.com/openai/evals))\\\\n  * Langchain Evaluators\\\\n  * [RAGAS](https://docs.ragas.io/en/latest/concepts/metrics/index.html) for RAG applications\\\\n    \\\\n    \\\\n```\\\\nfrom deepeval.metrics import GEval\\\\nfrom deepeval.test_case import LLMTestCaseParams, LLMTestCase\\\\n \\\\ndef joyfulness_score(trace):\\\\n joyfulness_metric = GEval(\\\\n name=\\\\\\\"Correctness\\\\\\\",\\\\n criteria=\\\\\\\"Determine whether the output is engaging and fun.\\\\\\\",\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\\\\n )\\\\n test_case = LLMTestCase(\\\\n input=trace.input[\\\\\\\"args\\\\\\\"],\\\\n actual_output=trace.output)\\\\n \\\\n joyfulness_metric.measure(test_case)\\\\n \\\\n print(f\\\\\\\"Score: {joyfulness_metric.score}\\\\\\\")\\\\n print(f\\\\\\\"Reason: {joyfulness_metric.reason}\\\\\\\")\\\\n \\\\n return {\\\\\\\"score\\\\\\\": joyfulness_metric.score, \\\\\\\"reason\\\\\\\": joyfulness_metric.reason}\\\\n \\\\njoyfulness_score(traces_batch[1])\\\\n```\\\\nUnder the hood, GEval uses chain of thought (CoT) prompting to formulate a set of criteria for scoring prompts. When developing your own metrics, it\\\\u2019s important to review the reasoning behind these scores. This helps ensure that the model evaluates the traces just as you intended when you wrote the evaluation prompt.\\\\nOur eval function returns a dictionary with both the score and the model\\\\u2019s reasoning. We do this as we\\\\u2019ll persist the reasoning with every langfuse score, ensuring interpretability.\\\\nNow we\\\\u2019re done with defining our evaluation functions. Let\\\\u2019s push those scores back to Langfuse!\\\\n## 3\\\\\\\\. Pushing Scores to Langfuse\\\\nNow that we have our evaluation functions ready, it\\\\u2019s time to put them to work. Use the Langfuse client to add scores to existing traces.\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_score(\\\\n trace_id=traces_batch[1].id,\\\\n name=\\\\\\\"tone\\\\\\\",\\\\n value=joyfulness_score(traces_batch[1])[\\\\\\\"score\\\\\\\"],\\\\n comment=joyfulness_score(traces_batch[1])[\\\\\\\"reason\\\\\\\"]\\\\n)\\\\n```\\\\nAnd thus, you\\\\u2019ve added your first externally-evaluated score to Langfuse! Just 49 more to go \\\\ud83d\\\\ude01. But don\\\\u2019t worry \\\\u2014 our solutions are easy to scale.\\\\n## 4\\\\\\\\. Putting everything together\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nUntil now, we went through each of the necessary steps to build an external evaluation pipeline: Fetching traces, running the evaluations, and persisting the scores to Langfuse. Let\\\\u2019s sum it up into a compact script that you could run in your evaluation pipeline.\\\\nWe\\\\u2019ll fetch the data in batches of 10 traces and then iterate through each trace to score it and push the scores back to Langfuse. Note that this batch size is for demonstration purposes. In a production setup, you might want to process multiple batches in parallel to speed things up. Batching not only reduces the memory load on your system but also allows you to create checkpoints, so you can easily resume if something goes wrong.\\\\n    \\\\n    \\\\n```\\\\nimport math\\\\n \\\\nfor page_number in range(1, math.ceil(TOTAL_TRACES/BATCH_SIZE)):\\\\n \\\\n traces_batch = langfuse.api.trace.list(\\\\n tags=\\\\\\\"ext_eval_pipelines\\\\\\\",\\\\n page=page_number,\\\\n from_timestamp=five_am_yesterday,\\\\n to_timestamp=five_am_today,\\\\n limit=BATCH_SIZE\\\\n ).data\\\\n \\\\n for trace in traces_batch:\\\\n print(f\\\\\\\"Processing {trace.name}\\\\\\\")\\\\n \\\\n if trace.output is None:\\\\n print(f\\\\\\\"Warning: \\\\\\\\n Trace {trace.name} had no generated output, \\\\\\\\\\\\n it was skipped\\\\\\\")\\\\n continue\\\\n \\\\n langfuse.create_score(\\\\n trace_id=trace.id,\\\\n name=\\\\\\\"tone\\\\\\\",\\\\n value=tone_score(trace)\\\\n )\\\\n \\\\n jscore = joyfulness_score(trace)\\\\n langfuse.create_score(\\\\n trace_id=trace.id,\\\\n name=\\\\\\\"joyfulness\\\\\\\",\\\\n value=jscore[\\\\\\\"score\\\\\\\"],\\\\n comment=jscore[\\\\\\\"reason\\\\\\\"]\\\\n )\\\\n \\\\n print(f\\\\\\\"Batch {page_number} processed \\\\ud83d\\\\ude80 \\\\\\\\n\\\\\\\")\\\\n```\\\\nIf your pipeline ran successfully, you should see your score in the Langfuse UI. \\\\nAnd that\\\\u2019s it! You\\\\u2019re now ready to integrate these lines into your preferred orchestration tool to ensure they run at the right times.\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nTo achieve our original goal of running the script every day at 5 am, simply schedule a Cron task in your chosen environment with the rule `cron(0 5 * * ? *)`.\\\\nThanks for coding along! I hope you enjoyed the tutorial and found it helpful.\\\\nLast updated on August 20, 2025\\\\n[Example - Tracing and Evaluation for the OpenAI-Agents SDK](/guides/cookbook/example_evaluating_openai_agents \\\\\\\"Example - Tracing and Evaluation for the OpenAI-Agents SDK\\\\\\\")\\\\n[Guide - Building an intent classification pipeline](/guides/cookbook/example_intent_classification_pipeline \\\\\\\"Guide - Building an intent classification pipeline\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"github_discussion\\\", \\\"url\\\": \\\"https://github.com/orgs/langfuse/discussions/7572\\\", \\\"title\\\": \\\"LLM-as-a-Judge Evaluators for Dataset doesn't work as expected.\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\n\\\\n---\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=hallucination) | [Report \\\\ud83d\\\\udc1b](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\n\\\\n        Reply #5 to comment #1 from dosubot on June 30, 2025:\\\\n\\\\n        I am using ui and variable mapping is as follows:\\\\r\\\\n\\\\r\\\\n{{search_query}} is mapped to Object: Dataset item, Object Variable: Input, JsonPath: <empty>\\\\r\\\\n{{summary}} is mapped to Object: Dataset item, Object Variable: Metadata, JsonPath: <empty>\\\\r\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{{expected}} is mapped to Object: Dataset item,  Object Variable: Expected Output, JsonPath: <empty>\\\\r\\\\n\\\\r\\\\nA single row format example from my dataset: \\\\r\\\\nInput column: \\\\\\\"Test input\\\\\\\", Expected Output column: \\\\\\\"Test expected output\\\\\\\", Metadata column: \\\\\\\"Test metadata\\\\\\\"\\\\n\\\\n        Reply #6 to comment #1 from dosubot on June 30, 2025:\\\\n\\\\n        <!-- Answer -->\\\\nYour mapping setup looks correct, but the backend eligibility logic is strict: if any mapped variable can't be resolved (for example, if the field is empty or the type doesn't match expectations), the item is silently skipped and you get \\\\\\\"0 elements\\\\\\\" processed. \\\\n\\\\nOne likely issue is the Metadata field\\\\u2014if it's a plain string (like \\\\\\\"Test metadata\\\\\\\"), but the system expects a JSON object (since many Langfuse examples use objects for metadata), this can cause the mapping to fail even if JsonPath is empty. When JsonPath is empty, the backend uses the top-level value as-is, but if the field is a string and the mapping expects an object (or vice versa), the item is excluded from evaluation\\\\u00a0[see code logic](https://github.com/langfuse/langfuse/blob/98cc1bb7a1759e29de3dd818f6a500df48fb5e82/worker/src/features/evaluation/evalService.ts).\\\\n\\\\nTry this: map all variables to fields you know are plain strings (like Input or Expected Output) and see if the evaluation processes any items. If it works, the issue is with the Metadata field's type. To confirm, create a dataset item where Metadata is a JSON object (e.g., {\\\\\\\"meta\\\\\\\": \\\\\\\"value\\\\\\\"}) and try mapping with and without a JsonPath. This will help isolate the problem.\\\\n\\\\nIf you still get 0 elements, export a sample of your dataset as JSON and share it\\\\u2014there may be a subtle data or mapping mismatch. \\\\n\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nMore details on dataset item structure and mapping: [Langfuse docs](https://langfuse.com/docs/datasets/get-started).\\\\n\\\\n\\\\n<!-- Dosu Comment Footer -->\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\n\\\\n---\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=hallucination) | [Report \\\\ud83d\\\\udc1b](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\n\\\\n        Reply #7 to comment #1 from dosubot on June 30, 2025:\\\\n\\\\n        I have created a new dataset with single row in json format, so columns Input, Expected Output and Metadata have following values respectively but still the same issue:\\\\r\\\\n{ \\\\\\\"search_query\\\\\\\": \\\\\\\"What's MLflow?\\\\\\\" }\\\\r\\\\n\\\\r\\\\n{ \\\\\\\"expected\\\\\\\": \\\\\\\"MLflow tracks ML experiments.\\\\\\\" } \\\\r\\\\n\\\\r\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2023-09-25-datasets\\\", \\\"title\\\": \\\"Datasets (beta)\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Datasets (beta) - Langfuse\\\\n[\\\\u2190 Back to changelog](/changelog#2023-09-25-datasets)\\\\nSeptember 25, 2023\\\\n# Datasets (beta)\\\\n[Marc Klingen](https://twitter.com/marcklingen)\\\\nCollect sets of inputs and expected outputs in Langfuse to evaluate your LLM app. Use evaluations to benchmark different experiments.\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\n_Run experiment on dataset_\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\n    \\\\n    \\\\n```\\\\n \\\\ndataset = langfuse.get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\nfor item in dataset.items:\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\n # output also returned as it is used to evaluate the run\\\\n generation, output = my_llm_application.run(item.input)\\\\n \\\\n # link the execution trace to the dataset item and give it a run_name\\\\n item.link(generation, \\\\\\\"<run_name>\\\\\\\")\\\\n \\\\n # optionally, evaluate the output to compare different runs more easily\\\\n generation.score(\\\\n name=\\\\\\\"<example_eval>\\\\\\\",\\\\n # any float value\\\\n value=my_eval_fn(\\\\n item.input,\\\\n output,\\\\n item.expected_output\\\\n )\\\\n )\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst dataset = await langfuse.getDataset(\\\\\\\"<dataset_name>\\\\\\\");\\\\n \\\\nfor (const item of dataset.items) {\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\n // output also returned as it is used to evaluate the run\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\n \\\\n // link the execution trace to the dataset item and give it a run_name\\\\n await item.link(generation, \\\\\\\"<run_name>\\\\\\\");\\\\n \\\\n // optionally, evaluate the output to compare different runs more easily\\\\n generation.score({\\\\n name: \\\\\\\"<score_name>\\\\\\\",\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\n });\\\\n}\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\", \\\"title\\\": \\\"Troubleshooting and FAQ\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\(\\\\\\\\)` method\\\\\\\")\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\")\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\"Langfuse Support: \\\\\\\\[Experiment\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\")\\\\n[Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing\\\\\\\")\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\")\\\\n[Results for some data items not present when comparing experiments](https://github.com/orgs/langfuse/discussions/5928 \\\\\\\"Langfuse Support: Results for some data items not present when comparing experiments\\\\\\\")\\\\n[Deleting Metrics for Langfuse](https://github.com/orgs/langfuse/discussions/5849 \\\\\\\"Langfuse Support: Deleting Metrics for Langfuse\\\\\\\")\\\\n[Discrepancies between dataset items found in the UI vs retrieved from the SDK/API](https://github.com/orgs/langfuse/discussions/5822 \\\\\\\"Langfuse Support: Discrepancies between dataset items found in the UI vs retrieved from the SDK/API\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Request batch adding of multiple traces to datasets](https://github.com/orgs/langfuse/discussions/8526 \\\\\\\"Langfuse Ideas: Request batch adding of multiple traces to datasets\\\\\\\")\\\\n[How to categorize topics and create a pie chart?](https://github.com/orgs/langfuse/discussions/8512 \\\\\\\"Langfuse Ideas: How to categorize topics and create a pie chart?\\\\\\\")\\\\n[Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window](https://github.com/orgs/langfuse/discussions/8485 \\\\\\\"Langfuse Ideas: Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window\\\\\\\")\\\\n[feat(LLM-as-a-judge): support stratified sampling by trace property](https://github.com/orgs/langfuse/discussions/8480 \\\\\\\"Langfuse Ideas: feat\\\\\\\\(LLM-as-a-judge\\\\\\\\): support stratified sampling by trace property\\\\\\\")\\\\n[Bulk Evaluation Does not Displays the status of the dataset (Like for E.g. processing, evaluated, executed)](https://github.com/orgs/langfuse/discussions/8410 \\\\\\\"Langfuse Ideas: Bulk Evaluation Does not Displays the status of the dataset \\\\\\\\(Like for E.g. processing, evaluated, executed\\\\\\\\)\\\\\\\")\\\\n[Annotation Queue Creation API](https://github.com/orgs/langfuse/discussions/8372 \\\\\\\"Langfuse Ideas: Annotation Queue Creation API\\\\\\\")\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\"Langfuse Ideas: \\\\\\\\[Langfuse Cloud\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\")\\\\n[feat(evals): allow canceling a running evaluator with pending evaluation jobs](https://github.com/orgs/langfuse/discussions/8310 \\\\\\\"Langfuse Ideas: feat\\\\\\\\(evals\\\\\\\\): allow canceling a running evaluator with pending evaluation jobs\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Support Description on Dataset Items](https://github.com/orgs/langfuse/discussions/6011 \\\\\\\"Langfuse Ideas: Support Description on Dataset Items\\\\\\\")\\\\n[Alta Integration](https://github.com/orgs/langfuse/discussions/5957 \\\\\\\"Langfuse Ideas: Alta Integration\\\\\\\")\\\\n[Add run name to columns when looking at a specific dataset item](https://github.com/orgs/langfuse/discussions/5929 \\\\\\\"Langfuse Ideas: Add run name to columns when looking at a specific dataset item\\\\\\\")\\\\n[Delete multiple dataset runs](https://github.com/orgs/langfuse/discussions/5893 \\\\\\\"Langfuse Ideas: Delete multiple dataset runs\\\\\\\")\\\\n[Navigation between items in a dataset run is confusing - context of the selected dataset run is lost](https://github.com/orgs/langfuse/discussions/5892 \\\\\\\"Langfuse Ideas: Navigation between items in a dataset run is confusing - context of the selected dataset run is lost\\\\\\\")\\\\n[Enhanced score distribution visualization in experiment analysis](https://github.com/orgs/langfuse/discussions/5819 \\\\\\\"Langfuse Ideas: Enhanced score distribution visualization in experiment analysis\\\\\\\")\\\\n[Multi-step Prompt Experiments and Playground](https://github.com/orgs/langfuse/discussions/5812 \\\\\\\"Langfuse Ideas: Multi-step Prompt Experiments and Playground\\\\\\\")\\\\n[Option to add trace to new dataset](https://github.com/orgs/langfuse/discussions/5756 \\\\\\\"Langfuse Ideas: Option to add trace to new dataset\\\\\\\")\\\\n[Simplified UI for Scoring](https://github.com/orgs/langfuse/discussions/5721 \\\\\\\"Langfuse Ideas: Simplified UI for Scoring\\\\\\\")\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Datasets: Add selection of traces to a dataset](https://github.com/orgs/langfuse/discussions/4353 \\\\\\\"Langfuse Ideas: Datasets: Add selection of traces to a dataset\\\\\\\")\\\\n[Multi-user annotation capability in Annotation Queues](https://github.com/orgs/langfuse/discussions/4348 \\\\\\\"Langfuse Ideas: Multi-user annotation capability in Annotation Queues\\\\\\\")\\\\n[Multi-turn / session experiments in datasets](https://github.com/orgs/langfuse/discussions/4208 \\\\\\\"Langfuse Ideas: Multi-turn / session experiments in datasets\\\\\\\")\\\\n[Enable to use variable of prompt on evaluator.](https://github.com/orgs/langfuse/discussions/4121 \\\\\\\"Langfuse Ideas: Enable to use variable of prompt on evaluator.\\\\\\\")\\\\n[Sessions Table: Scores Column](https://github.com/orgs/langfuse/discussions/4120 \\\\\\\"Langfuse Ideas: Sessions Table: Scores Column\\\\\\\")\\\\n[Add new filters for the LLM as a Judge Evaluation (other scores and cost)](https://github.com/orgs/langfuse/discussions/4106 \\\\\\\"Langfuse Ideas: Add new filters for the LLM as a Judge Evaluation \\\\\\\\(other scores and cost\\\\\\\\)\\\\\\\")\\\\n[Export dataset run table](https://github.com/orgs/langfuse/discussions/4077 \\\\\\\"Langfuse Ideas: Export dataset run table\\\\\\\")\\\\n[feat: support adding trace tags in annotation queue view](https://github.com/orgs/langfuse/discussions/4037 \\\\\\\"Langfuse Ideas: feat: support adding trace tags in annotation queue view\\\\\\\")\\\\n[Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times](https://github.com/orgs/langfuse/discussions/4035 \\\\\\\"Langfuse Ideas: Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times\\\\\\\")\\\\n[Diff support for dataset runs view](https://github.com/orgs/langfuse/discussions/4025 \\\\\\\"Langfuse Ideas: Diff support for dataset runs view\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Create Support for gemini models in playground](https://github.com/orgs/langfuse/discussions/4019 \\\\\\\"Langfuse Ideas: Create Support for gemini models in playground\\\\\\\")\\\\n[Change AWS access pattern for Bedrock LLM usage, assume role](https://github.com/orgs/langfuse/discussions/3988 \\\\\\\"Langfuse Ideas: Change AWS access pattern for Bedrock LLM usage, assume role\\\\\\\")\\\\n[Add ability to export and import evaluators between projects](https://github.com/orgs/langfuse/discussions/3970 \\\\\\\"Langfuse Ideas: Add ability to export and import evaluators between projects\\\\\\\")\\\\n[feat: Folder structure for dataset organisation](https://github.com/orgs/langfuse/discussions/3935 \\\\\\\"Langfuse Ideas: feat: Folder structure for dataset organisation\\\\\\\")\\\\n[Model-based evaluations triggered by observations](https://github.com/orgs/langfuse/discussions/3918 \\\\\\\"Langfuse Ideas: Model-based evaluations triggered by observations\\\\\\\")\\\\n[Scores: Conditional Annotation](https://github.com/orgs/langfuse/discussions/3842 \\\\\\\"Langfuse Ideas: Scores: Conditional Annotation\\\\\\\")\\\\n[Annotation Queues: define optional/mandatory score configs by queue](https://github.com/orgs/langfuse/discussions/3841 \\\\\\\"Langfuse Ideas: Annotation Queues: define optional/mandatory score configs by queue\\\\\\\")\\\\n[Scores: support for recording multiple choice selection as score value](https://github.com/orgs/langfuse/discussions/3840 \\\\\\\"Langfuse Ideas: Scores: support for recording multiple choice selection as score value\\\\\\\")\\\\n[Filter by status in dataset items table](https://github.com/orgs/langfuse/discussions/3818 \\\\\\\"Langfuse Ideas: Filter by status in dataset items table\\\\\\\")\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/overview\\\", \\\"title\\\": \\\"Evaluation Overview\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Evaluation of LLM Applications - Langfuse\\\\nDocsEvaluationOverview\\\\nCopy as Markdown\\\\n# Evaluation Overview\\\\nEvaluation is a critical aspect of developing and deploying LLM applications. Usually, teams use a multitude of different evaluation methods to score the performance of their AI application depending on the use case and the stage of the development process.\\\\n\\\\ud83c\\\\udfa5\\\\n[ **Watch this walkthrough**](/watch-demo?tab=evaluation) of Langfuse Evaluation and how to use it to improve your LLM application.\\\\n## Why use LLM Evaluation?\\\\nLLM evaluation is crucial for improving the accuracy and robustness of language models, ultimately enhancing the user experience and trust in your AI application. Here are the key benefits:\\\\n  * **Quality Assurance** : Detect hallucinations, factual inaccuracies, and inconsistent outputs to ensure your AI app delivers reliable results\\\\n  * **Performance Monitoring** : Measure response quality, relevance, and user satisfaction across different scenarios and edge cases\\\\n  * **Continuous Improvement** : Identify areas for enhancement and track improvements over time through structured evaluation metrics\\\\n  * **User Trust** : Build confidence in your AI application by demonstrating consistent, high-quality outputs through systematic evaluation\\\\n  * **Risk Mitigation** : Catch potential issues before they reach production users, reducing the likelihood of poor user experiences or reputational damage\\\\n## Online & Offline Evaluation\\\\n**Offline Evaluation involves**\\\\n  * Evaluating the application in a controlled setting\\\\n  * Typically using curated test Datasets instead of live user queries\\\\n  * Heavily used during development (can be part of CI/CD pipelines) to measure improvements / regressions\\\\n  * Repeatable and you can get clear accuracy metrics since you have ground truth.\\\\n**Online Evaluation involves**\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Python SDK: Pass ScoreConfig instead of config_id when creating scores](https://github.com/orgs/langfuse/discussions/8623 \\\\\\\"Langfuse Ideas: Python SDK: Pass ScoreConfig instead of config_id when creating scores\\\\\\\")\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\"Langfuse Ideas: \\\\\\\\[Langfuse Cloud\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\")\\\\n[Score Configs: Allow editing the categories of a categorical score](https://github.com/orgs/langfuse/discussions/8259 \\\\\\\"Langfuse Ideas: Score Configs: Allow editing the categories of a categorical score\\\\\\\")\\\\n[UI-LLM as a Jury](https://github.com/orgs/langfuse/discussions/8195 \\\\\\\"Langfuse Ideas: UI-LLM as a Jury\\\\\\\")\\\\n[Enable Immediate Score Management for User Feedback](https://github.com/orgs/langfuse/discussions/7686 \\\\\\\"Langfuse Ideas: Enable Immediate Score Management for User Feedback\\\\\\\")\\\\n[Filter by scores in session view](https://github.com/orgs/langfuse/discussions/7528 \\\\\\\"Langfuse Ideas: Filter by scores in session view\\\\\\\")\\\\n[Support break lines on evaluation run tooltip hint](https://github.com/orgs/langfuse/discussions/7452 \\\\\\\"Langfuse Ideas: Support break lines on evaluation run tooltip hint\\\\\\\")\\\\n[Support new lines when storing / displaying score comments](https://github.com/orgs/langfuse/discussions/6473 \\\\\\\"Langfuse Ideas: Support new lines when storing / displaying score comments\\\\\\\")\\\\n[Evaluator: Filter for Scores](https://github.com/orgs/langfuse/discussions/6236 \\\\\\\"Langfuse Ideas: Evaluator: Filter for Scores\\\\\\\")\\\\n[Code-based custom evaluators](https://github.com/orgs/langfuse/discussions/6087 \\\\\\\"Langfuse Ideas: Code-based custom evaluators\\\\\\\")\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Session-level scores](https://github.com/orgs/langfuse/discussions/2728 \\\\\\\"Langfuse Ideas: Session-level scores\\\\\\\")\\\\n[Scoring dataset runs, e.g. precision, recall, f-value](https://github.com/orgs/langfuse/discussions/2511 \\\\\\\"Langfuse Ideas: Scoring dataset runs, e.g. precision, recall, f-value\\\\\\\")\\\\n[Adding userId / author to score (custom metadata)](https://github.com/orgs/langfuse/discussions/2469 \\\\\\\"Langfuse Ideas: Adding userId / author to score \\\\\\\\(custom metadata\\\\\\\\)\\\\\\\")\\\\n[Add string data type in score config](https://github.com/orgs/langfuse/discussions/2402 \\\\\\\"Langfuse Ideas: Add string data type in score config\\\\\\\")\\\\n[API to delete scores](https://github.com/orgs/langfuse/discussions/1133 \\\\\\\"Langfuse Ideas: API to delete scores\\\\\\\")\\\\nGitHubSupportGitHubIdeas\\\\nUpvotes[GitHubNew](https://github.com/orgs/langfuse/discussions/new/choose)\\\\n  * 9votes\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486)\\\\nmsanand\\\\u20223/25/2024\\\\u2022\\\\n2Resolved\\\\n  * 4votes\\\\n[Filter Categorical Score Values](https://github.com/orgs/langfuse/discussions/3797)\\\\nalabrashJr\\\\u202210/17/2024\\\\u2022\\\\n3Resolved\\\\n  * 3votes\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590)\\\\nanuras\\\\u20228/18/2025\\\\u2022\\\\n1Resolved\\\\n  * 3votes\\\\n[Support for Metric Calculation (Precision@K, Recall@K) and Adding Custom Metrics Use Case Overview](https://github.com/orgs/langfuse/discussions/5215)\\\\nsrimantacse\\\\u20221/27/2025\\\\u2022\\\\n2Resolved\\\\n  * 2votes\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556)\\\\nflabbergastedbd\\\\u20228/15/2025\\\\u2022\\\\n1\\\\n  * 1votes\\\"}], \\\"type\\\": \\\"content\\\"}}]}\",\"role\":\"assistant\",\"tool_calls\":null,\"function_call\":null}}],\"usage\":{\"prompt_tokens\":23,\"completion_tokens\":20274,\"total_tokens\":20297},\"system_fingerprint\":\"inkeep-fp-9a382b06-43e9-4c7a-b61b-7b35190c514a\",\"object\":\"chat.completion\",\"service_tier\":null,\"created\":1758264684},\"content\":[{\"type\":\"text\",\"text\":\"{\\\"content\\\": [{\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\", \\\"title\\\": \\\"Troubleshooting and FAQ\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[API - /scores not respecting value when operator '='](https://github.com/orgs/langfuse/discussions/8770 \\\\\\\"Langfuse Support: API - /scores not respecting value when operator '='\\\\\\\")\\\\n[Does LangFuse support evaluations on an existing dataset (.csv)](https://github.com/orgs/langfuse/discussions/8665 \\\\\\\"Langfuse Support: Does LangFuse support evaluations on an existing dataset \\\\\\\\(.csv\\\\\\\\)\\\\\\\")\\\\n[deleting evaluators](https://github.com/orgs/langfuse/discussions/8640 \\\\\\\"Langfuse Support: deleting evaluators\\\\\\\")\\\\n[Not able to import my LLM-as-a-Judge evals](https://github.com/orgs/langfuse/discussions/8636 \\\\\\\"Langfuse Support: Not able to import my LLM-as-a-Judge evals\\\\\\\")\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590 \\\\\\\"Langfuse Support: How to get experiment run scores programmatically?\\\\\\\")\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556 \\\\\\\"Langfuse Support: Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations\\\\\\\")\\\\n[Dataset runs restore from backups](https://github.com/orgs/langfuse/discussions/8534 \\\\\\\"Langfuse Support: Dataset runs restore from backups\\\\\\\")\\\\n[Getting scores efficiently via API for analytics purposes](https://github.com/orgs/langfuse/discussions/8520 \\\\\\\"Langfuse Support: Getting scores efficiently via API for analytics purposes\\\\\\\")\\\\n[run experiment on dataset](https://github.com/orgs/langfuse/discussions/8433 \\\\\\\"Langfuse Support: run experiment on dataset\\\\\\\")\\\\n[Experiments on Datasets with Human Annotated Labels?](https://github.com/orgs/langfuse/discussions/8414 \\\\\\\"Langfuse Support: Experiments on Datasets with Human Annotated Labels?\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[How to Recalculate Total Score on Dashboard After Updating User-Defined Model?](https://github.com/orgs/langfuse/discussions/8375 \\\\\\\"Langfuse Support: How to Recalculate Total Score on Dashboard After Updating User-Defined Model?\\\\\\\")\\\\n[How to filter by Categorical Scores in custom dashboard?](https://github.com/orgs/langfuse/discussions/8356 \\\\\\\"Langfuse Support: How to filter by Categorical Scores in custom dashboard?\\\\\\\")\\\\n[Running scheduled evals utilising LangFuse Datasets & Evaluators](https://github.com/orgs/langfuse/discussions/8355 \\\\\\\"Langfuse Support: Running scheduled evals utilising LangFuse Datasets & Evaluators\\\\\\\")\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\(\\\\\\\\)` method\\\\\\\")\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\")\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\"Langfuse Support: \\\\\\\\[Experiment\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\")\\\\n[Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing\\\\\\\")\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[2 traces generated instead of 1](https://github.com/orgs/langfuse/discussions/2244 \\\\\\\"Langfuse Support: 2 traces generated instead of 1\\\\\\\")\\\\n[Evaluations Not Available in Self-Hosted Version?](https://github.com/orgs/langfuse/discussions/2130 \\\\\\\"Langfuse Support: Evaluations Not Available in Self-Hosted Version?\\\\\\\")\\\\n[Deleting Duplicate Items in a Dataset](https://github.com/orgs/langfuse/discussions/2099 \\\\\\\"Langfuse Support: Deleting Duplicate Items in a Dataset\\\\\\\")\\\\n[Availability of evals when self-hosting](https://github.com/orgs/langfuse/discussions/2042 \\\\\\\"Langfuse Support: Availability of evals when self-hosting\\\\\\\")\\\\n[How to utilize a dataset w/ typescript and langchain integration](https://github.com/orgs/langfuse/discussions/1969 \\\\\\\"Langfuse Support: How to utilize a dataset w/ typescript and langchain integration\\\\\\\")\\\\n[Scoring a trace after the LLM chain returns](https://github.com/orgs/langfuse/discussions/1610 \\\\\\\"Langfuse Support: Scoring a trace after the LLM chain returns\\\\\\\")\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486 \\\\\\\"Langfuse Support: Update/delete score using python sdk\\\\\\\")\\\\n[Linking dataset run items with existing callback handler](https://github.com/orgs/langfuse/discussions/1445 \\\\\\\"Langfuse Support: Linking dataset run items with existing callback handler\\\\\\\")\\\\n[Datasets list / by id](https://github.com/orgs/langfuse/discussions/1420 \\\\\\\"Langfuse Support: Datasets list / by id\\\\\\\")\\\\n[Run items not appearing when linking to a trace and not a span or a generation](https://github.com/orgs/langfuse/discussions/1357 \\\\\\\"Langfuse Support: Run items not appearing when linking to a trace and not a span or a generation\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Option to run Experiments without traces](https://github.com/orgs/langfuse/discussions/8133 \\\\\\\"Langfuse Ideas: Option to run Experiments without traces\\\\\\\")\\\\n[Auto-generate dataset items](https://github.com/orgs/langfuse/discussions/8126 \\\\\\\"Langfuse Ideas: Auto-generate dataset items\\\\\\\")\\\\n[feature: support for creation of custom) model adapters](https://github.com/orgs/langfuse/discussions/8123 \\\\\\\"Langfuse Ideas: feature: support for creation of custom\\\\\\\\) model adapters\\\\\\\")\\\\n[Prompt experiment result download.](https://github.com/orgs/langfuse/discussions/8120 \\\\\\\"Langfuse Ideas: Prompt experiment result download.\\\\\\\")\\\\n[Multiple predictions per-item in a single run](https://github.com/orgs/langfuse/discussions/8040 \\\\\\\"Langfuse Ideas: Multiple predictions per-item in a single run\\\\\\\")\\\\n[Add evaluator(s) to python SDK (FernLangfuse)](https://github.com/orgs/langfuse/discussions/8018 \\\\\\\"Langfuse Ideas: Add evaluator\\\\\\\\(s\\\\\\\\) to python SDK \\\\\\\\(FernLangfuse\\\\\\\\)\\\\\\\")\\\\n[feat(dataset-runs): return all dataset run item scores in a given run](https://github.com/orgs/langfuse/discussions/8011 \\\\\\\"Langfuse Ideas: feat\\\\\\\\(dataset-runs\\\\\\\\): return all dataset run item scores in a given run\\\\\\\")\\\\n[Delete an evaluator from the evaluator library](https://github.com/orgs/langfuse/discussions/7960 \\\\\\\"Langfuse Ideas: Delete an evaluator from the evaluator library\\\\\\\")\\\\n[Archive or Delete Evaluators from the Evaluator Library](https://github.com/orgs/langfuse/discussions/7868 \\\\\\\"Langfuse Ideas: Archive or Delete Evaluators from the Evaluator Library\\\\\\\")\\\\n[Edit dataset run name, description](https://github.com/orgs/langfuse/discussions/7814 \\\\\\\"Langfuse Ideas: Edit dataset run name, description\\\\\\\")\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\", \\\"title\\\": \\\"Datasets\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Datasets - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")Dataset RunsDatasets\\\\nCopy as Markdown\\\\n# Datasets\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\n## Why use datasets?\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\n  * Create test cases for your application with real production traces\\\\n  * Collaboratively create and collect dataset items with your team\\\\n  * Have a single source of truth for your test data\\\\n## Get Started\\\\n### Creating a dataset\\\\nDatasets have a name which is unique within a project.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset(\\\\n name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # optional description\\\\n description=\\\\\\\"My first dataset\\\\\\\",\\\\n # optional metadata\\\\n metadata={\\\\n \\\\\\\"author\\\\\\\": \\\\\\\"Alice\\\\\\\",\\\\n \\\\\\\"date\\\\\\\": \\\\\\\"2022-01-01\\\\\\\",\\\\n \\\\\\\"type\\\\\\\": \\\\\\\"benchmark\\\\\\\"\\\\n }\\\\n)\\\\n```\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\n    \\\\n    \\\\n```\\\\nimport { LangfuseClient } from \\\\\\\"@langfuse/client\\\\\\\"\\\\n \\\\nconst langfuse = new LangfuseClient()\\\\n \\\\nawait langfuse.api.datasets.create({\\\\n name: \\\\\\\"<dataset_name>\\\\\\\",\\\\n // optional description\\\\n description: \\\\\\\"My first dataset\\\\\\\",\\\\n // optional metadata\\\\n metadata: {\\\\n author: \\\\\\\"Alice\\\\\\\",\\\\n date: \\\\\\\"2022-01-01\\\\\\\",\\\\n type: \\\\\\\"benchmark\\\\\\\",\\\\n },\\\\n});\\\\n```\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\n### Create new dataset items\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset_item(\\\\n dataset_name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # any python object or value, optional\\\\n input={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # any python object or value, optional\\\\n expected_output={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # metadata, optional\\\\n metadata={\\\\n \\\\\\\"model\\\\\\\": \\\\\\\"llama3\\\\\\\",\\\\n }\\\\n)\\\\n```\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/observability/sdk/python/evaluation\\\", \\\"title\\\": \\\"Evaluations\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Evaluations with the Langfuse Python SDK - Langfuse\\\\nDocs[Observability](/docs/observability/overview \\\\\\\"Observability\\\\\\\")\\\\n[SDKs](/docs/observability/sdk/overview \\\\\\\"SDKs\\\\\\\")\\\\n[Python](/docs/observability/sdk/python/overview \\\\\\\"Python\\\\\\\")Evaluation\\\\nCopy as Markdown\\\\n# Evaluations\\\\nThe Python SDK provides ways to evaluate your application. You can add custom scores to your traces and observations, or use the SDK to execute Dataset Runs.\\\\nThis page shows the evaluation methods that are supported by the Python SDK. Please refer to the [Evaluation documentation](/docs/evaluation/overview) for more information on how to evaluate your application in Langfuse.\\\\n## Create Scores\\\\nLangfuseSpan / LangfuseGeneration object methodsContext-aware methodsLow-level method\\\\n  * `span_or_generation_obj.score()`: Scores the specific observation object.\\\\n  * `span_or_generation_obj.score_trace()`: Scores the entire trace to which the object belongs.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nwith langfuse.start_as_current_generation(name=\\\\\\\"summary_generation\\\\\\\") as gen:\\\\n # ... LLM call ...\\\\n gen.update(output=\\\\\\\"summary text...\\\\\\\")\\\\n # Score this specific generation\\\\n gen.score(name=\\\\\\\"conciseness\\\\\\\", value=0.8, data_type=\\\\\\\"NUMERIC\\\\\\\")\\\\n # Score the overall trace\\\\n gen.score_trace(name=\\\\\\\"user_feedback_rating\\\\\\\", value=\\\\\\\"positive\\\\\\\", data_type=\\\\\\\"CATEGORICAL\\\\\\\")\\\\n```\\\\n  * `langfuse.score_current_span()`: Scores the currently active observation in the context.\\\\n  * `langfuse.score_current_trace()`: Scores the trace of the currently active observation.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nwith langfuse.start_as_current_span(name=\\\\\\\"complex_task\\\\\\\") as task_span:\\\\n # ... perform task ...\\\\n langfuse.score_current_span(name=\\\\\\\"task_component_quality\\\\\\\", value=True, data_type=\\\\\\\"BOOLEAN\\\\\\\")\\\\n # ...\\\\n if task_is_fully_successful:\\\\n langfuse.score_current_trace(name=\\\\\\\"overall_success\\\\\\\", value=1.0, data_type=\\\\\\\"NUMERIC\\\\\\\")\\\\n```\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n  * Creates a score for a specified `trace_id` and optionally `observation_id`.\\\\n  * Useful when IDs are known, or for scoring after the trace/observation has completed.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nlangfuse.create_score(\\\\n name=\\\\\\\"fact_check_accuracy\\\\\\\",\\\\n value=0.95, ///# Can be float for NUMERIC/BOOLEAN, string for CATEGORICAL\\\\n trace_id=\\\\\\\"abcdef1234567890abcdef1234567890\\\\\\\",\\\\n observation_id=\\\\\\\"1234567890abcdef\\\\\\\", ///# Optional: if scoring a specific observation\\\\n session_id=\\\\\\\"session_123\\\\\\\", ///# Optional: if scoring a specific session\\\\n data_type=\\\\\\\"NUMERIC\\\\\\\", ///# \\\\\\\"NUMERIC\\\\\\\", \\\\\\\"BOOLEAN\\\\\\\", \\\\\\\"CATEGORICAL\\\\\\\"\\\\n comment=\\\\\\\"Source verified for 95% of claims.\\\\\\\"\\\\n)\\\\n```\\\\n**Score Parameters:**\\\\nParameter| Type| Description  \\\\n---|---|---  \\\\n`name`| `str`| Name of the score (e.g., \\\\u201crelevance\\\\u201d, \\\\u201caccuracy\\\\u201d). **Required.**  \\\\n`value`| `Union[float, str]`| Score value. Float for `NUMERIC`/`BOOLEAN`, string for `CATEGORICAL`. **Required.**  \\\\n`trace_id`| `str`| ID of the trace to associate with (for `create_score`). **Required.**  \\\\n`observation_id`| `Optional[str]`| ID of the specific observation to score (for `create_score`).  \\\\n`session_id`| `Optional[str]`| ID of the specific session to score (for `create_score`).  \\\\n`score_id`| `Optional[str]`| Custom ID for the score (auto-generated if None).  \\\\n`data_type`| `Optional[ScoreDataType]`| `\\\\\\\"NUMERIC\\\\\\\"`, `\\\\\\\"BOOLEAN\\\\\\\"`, or `\\\\\\\"CATEGORICAL\\\\\\\"`. Inferred if not provided based on value type and score config on server.  \\\\n`comment`| `Optional[str]`| Optional comment or explanation for the score.  \\\\n`config_id`| `Optional[str]`| Optional ID of a pre-defined score configuration in Langfuse.  \\\\n  \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThe most powerful way to use datasets is by linking your application\\\\u2019s executions (traces) to specific dataset items when performing an evaluation run. The `DatasetItemClient.run()` method provides a context manager to streamline this process.\\\\n**How`item.run()` works:**\\\\nWhen you use `with item.run(run_name=\\\\\\\"your_eval_run_name\\\\\\\") as root_span:`:\\\\n  1. **Trace Creation** : A new Langfuse trace is initiated specifically for processing this dataset item within the context of the named run.\\\\n  2. **Trace Naming & Metadata**: \\\\n    * The trace is automatically named (e.g., \\\\u201cDataset run: your_eval_run_name\\\\u201d).\\\\n    * Essential metadata is added to this trace, including `dataset_item_id` (the ID of `item`), `run_name`, and `dataset_id`.\\\\n  3. **DatasetRunItem Linking** : The SDK makes an API call to Langfuse to create a `DatasetRunItem`. This backend object formally links: \\\\n    * The `dataset_item_id`\\\\n    * The `trace_id` of the newly created trace\\\\n    * The provided `run_name`\\\\n    * Any `run_metadata` or `run_description` you pass to `item.run()`. This linkage is what populates the \\\\u201cRuns\\\\u201d tab for your dataset in the Langfuse UI, allowing you to see all traces associated with a particular evaluation run.\\\\n  4. **Contextual Span** : The context manager yields `root_span`, which is a `LangfuseSpan` object representing the root span of this new trace.\\\\n  5. **Automatic Nesting** : Any Langfuse observations (spans or generations) created _inside_ the `with` block will automatically become children of `root_span` and thus part of the trace linked to this dataset item and run.\\\\n**Example:**\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\ndataset_name = \\\\\\\"qna-eval\\\\\\\"\\\\ncurrent_run_name = \\\\\\\"qna_model_v3_run_05_20\\\\\\\" # Identifies this specific evaluation run\\\\n \\\\n///# Assume 'my_qna_app' is your instrumented application function\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"def my_qna_app(question: str, context: str, item_id: str, run_name: str):\\\\n with langfuse.start_as_current_generation(\\\\n name=\\\\\\\"qna-llm-call\\\\\\\",\\\\n input={\\\\\\\"question\\\\\\\": question, \\\\\\\"context\\\\\\\": context},\\\\n metadata={\\\\\\\"item_id\\\\\\\": item_id, \\\\\\\"run\\\\\\\": run_name}, ///# Example metadata for the generation\\\\n model=\\\\\\\"gpt-4o\\\\\\\"\\\\n ) as generation:\\\\n # Simulate LLM call\\\\n answer = f\\\\\\\"Answer to '{question}' using context.\\\\\\\" # Replace with actual LLM call\\\\n generation.update(output={\\\\\\\"answer\\\\\\\": answer})\\\\n \\\\n # Update the trace with the input and output\\\\n generation.update_trace(\\\\n input={\\\\\\\"question\\\\\\\": question, \\\\\\\"context\\\\\\\": context},\\\\n output={\\\\\\\"answer\\\\\\\": answer},\\\\n )\\\\n \\\\n return answer\\\\n \\\\ndataset = langfuse.get_dataset(name=dataset_name) ///# Fetch your pre-populated dataset\\\\n \\\\nfor item in dataset.items:\\\\n print(f\\\\\\\"Running evaluation for item: {item.id} (Input: {item.input})\\\\\\\")\\\\n \\\\n # Use the item.run() context manager\\\\n with item.run(\\\\n run_name=current_run_name,\\\\n run_metadata={\\\\\\\"model_provider\\\\\\\": \\\\\\\"OpenAI\\\\\\\", \\\\\\\"temperature_setting\\\\\\\": 0.7},\\\\n run_description=\\\\\\\"Evaluation run for Q&A model v3 on May 20th\\\\\\\"\\\\n ) as root_span: ///# root_span is the root span of the new trace for this item and run.\\\\n # All subsequent langfuse operations within this block are part of this trace.\\\\n \\\\n # Call your application logic\\\\n generated_answer = my_qna_app(\\\\n question=item.input[\\\\\\\"question\\\\\\\"],\\\\n context=item.input[\\\\\\\"context\\\\\\\"],\\\\n item_id=item.id,\\\\n run_name=current_run_name\\\\n )\\\\n \\\\n print(f\\\\\\\" Item {item.id} processed. Trace ID: {root_span.trace_id}\\\\\\\")\\\\n \\\\n # Optionally, score the result against the expected output\\\\n if item.expected_output and generated_answer == item.expected_output.get(\\\\\\\"answer\\\\\\\"):\\\\n root_span.score_trace(name=\\\\\\\"exact_match\\\\\\\", value=1.0)\\\\n else:\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n root_span.score_trace(name=\\\\\\\"exact_match\\\\\\\", value=0.0)\\\\n \\\\nprint(f\\\\\\\"\\\\\\\\nFinished processing dataset '{dataset_name}' for run '{current_run_name}'.\\\\\\\")\\\\n```\\\\nBy using `item.run()`, you ensure each dataset item\\\\u2019s processing is neatly encapsulated in its own trace, and these traces are aggregated under the specified `run_name` in the Langfuse UI. This allows for systematic review of results, comparison across runs, and deep dives into individual processing traces.\\\\nLast updated on August 20, 2025\\\\n[Instrumentation](/docs/observability/sdk/python/instrumentation \\\\\\\"Instrumentation\\\\\\\")\\\\n[Advanced usage](/docs/observability/sdk/python/advanced-usage \\\\\\\"Advanced usage\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\", \\\"title\\\": \\\"All new Datasets, Experimentation and Evaluation documentation\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"## New Datasets and Evaluation documentation\\\\nBased on this week\\\\u2019s changes, we have have restructured the documentation to:\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\n### Evaluation\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Evaluation documentation](/docs/scores)\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\n    \\\\n    \\\\n```\\\\nPOST /api/public/scores\\\\n```\\\\nAdd scores via Python or JS SDK.\\\\nExample (Python)\\\\n    \\\\n    \\\\n```\\\\nlangfuse.score(\\\\n trace_id=\\\\\\\"123\\\\\\\",\\\\n name=\\\\\\\"my_custom_evaluator\\\\\\\",\\\\n value=0.5,\\\\n)\\\\n```\\\\n### Datasets\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\", \\\"title\\\": \\\"Remote Dataset Runs\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Task function that answers geography questions\\\\\\\"\\\\\\\"\\\\\\\"\\\\n question = item[\\\\\\\"input\\\\\\\"]\\\\n response = OpenAI().chat.completions.create(\\\\n model=\\\\\\\"gpt-4\\\\\\\",\\\\n messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": question}]\\\\n )\\\\n return response.choices[0].message.content\\\\n \\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\"\\\\\\\"\\\\\\\"\\\\n if expected_output and expected_output.lower() in output.lower():\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=1.0)\\\\n \\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=0.0)\\\\n \\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\"\\\\\\\"\\\\\\\"\\\\n accuracies = [\\\\n eval.value for result in item_results\\\\n for eval in result.evaluations if eval.name == \\\\\\\"accuracy\\\\\\\"\\\\n ]\\\\n \\\\n if not accuracies:\\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=None)\\\\n \\\\n avg = sum(accuracies) / len(accuracies)\\\\n \\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=avg, comment=f\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\")\\\\n \\\\n@pytest.fixture\\\\ndef langfuse_client():\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Initialize Langfuse client for testing\\\\\\\"\\\\\\\"\\\\\\\"\\\\n return get_client()\\\\n \\\\ndef test_geography_accuracy_passes(langfuse_client):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Test that passes when accuracy is above threshold\\\\\\\"\\\\\\\"\\\\\\\"\\\\n result = langfuse_client.run_experiment(\\\\n name=\\\\\\\"Geography Test - Should Pass\\\\\\\",\\\\n data=test_data,\\\\n task=geography_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy_evaluator]\\\\n )\\\\n \\\\n # Access the run evaluator result directly\\\\n avg_accuracy = next(\\\\n eval.value for eval in result.run_evaluations\\\\n if eval.name == \\\\\\\"avg_accuracy\\\\\\\"\\\\n )\\\\n \\\\n # Assert minimum accuracy threshold\\\\n assert avg_accuracy >= 0.8, f\\\\\\\"Average accuracy {avg_accuracy:.2f} below threshold 0.8\\\\\\\"\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\ndef test_geography_accuracy_fails(langfuse_client):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Example test that demonstrates failure conditions\\\\\\\"\\\\\\\"\\\\\\\"\\\\n # Use a weaker model or harder questions to demonstrate test failure\\\\n def failing_task(*, item, **kwargs):\\\\n # Simulate a task that gives wrong answers\\\\n return \\\\\\\"I don't know\\\\\\\"\\\\n \\\\n result = langfuse_client.run_experiment(\\\\n name=\\\\\\\"Geography Test - Should Fail\\\\\\\",\\\\n data=test_data,\\\\n task=failing_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy_evaluator]\\\\n )\\\\n \\\\n # Access the run evaluator result directly\\\\n avg_accuracy = next(\\\\n eval.value for eval in result.run_evaluations\\\\n if eval.name == \\\\\\\"avg_accuracy\\\\\\\"\\\\n )\\\\n \\\\n # This test will fail because the task gives wrong answers\\\\n with pytest.raises(AssertionError):\\\\n assert avg_accuracy >= 0.8, f\\\\\\\"Expected test to fail with low accuracy: {avg_accuracy:.2f}\\\\\\\"\\\\n```\\\\n    \\\\n    \\\\n```\\\\n// test/geography-experiment.test.ts\\\\nimport { describe, it, expect, beforeAll, afterAll } from 'vitest';\\\\nimport { OpenAI } from \\\\\\\"openai\\\\\\\";\\\\nimport { NodeSDK } from \\\\\\\"@opentelemetry/sdk-node\\\\\\\";\\\\nimport { LangfuseClient, ExperimentItem } from \\\\\\\"@langfuse/client\\\\\\\";\\\\nimport { observeOpenAI } from \\\\\\\"@langfuse/openai\\\\\\\";\\\\nimport { LangfuseSpanProcessor } from \\\\\\\"@langfuse/otel\\\\\\\";\\\\n \\\\n// Test data for European capitals\\\\nconst testData: ExperimentItem[] = [\\\\n { input: \\\\\\\"What is the capital of France?\\\\\\\", expected: \\\\\\\"Paris\\\\\\\" },\\\\n { input: \\\\\\\"What is the capital of Germany?\\\\\\\", expected: \\\\\\\"Berlin\\\\\\\" },\\\\n { input: \\\\\\\"What is the capital of Spain?\\\\\\\", expected: \\\\\\\"Madrid\\\\\\\" },\\\\n];\\\\n \\\\nlet otelSdk: NodeSDK;\\\\nlet langfuse: LangfuseClient;\\\\n \\\\nbeforeAll(async () => {\\\\n // Initialize OpenTelemetry\\\\n otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\n otelSdk.start();\\\\n \\\\n // Initialize Langfuse client\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\n    \\\\n    \\\\n```\\\\nimport { Factuality, Levenshtein } from \\\\\\\"autoevals\\\\\\\";\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\"@langfuse/client\\\\\\\";\\\\n \\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\n \\\\n// Use with additional parameters\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\n Factuality,\\\\n { model: \\\\\\\"gpt-4o\\\\\\\" } // Additional AutoEvals parameters\\\\n);\\\\n \\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"AutoEvals Integration Test\\\\\\\",\\\\n data: testDataset,\\\\n task: myTask,\\\\n evaluators: [\\\\n factualityEvaluator,\\\\n levenshteinEvaluator,\\\\n customFactualityEvaluator\\\\n ]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/remote-run\\\", \\\"title\\\": \\\"Remote Dataset Runs\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Remote Dataset Runs via the SDK - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\"Dataset Runs\\\\\\\")Remote Run\\\\nCopy as Markdown\\\\n# Remote Dataset Runs\\\\nOnce you [created a dataset](/docs/evaluation/dataset-runs/datasets), you can use the dataset to test how your application performs on different inputs. Remote Dataset Runs are used to programmatically loop your applications or prompts through a dataset and optionally apply Evaluation Methods to the results.\\\\nThey are called \\\\u201cRemote Dataset Runs\\\\u201d because they can make use of \\\\u201cremote\\\\u201d or external logic and code.\\\\nOptionally, you can also trigger Remote Dataset Runs via the Langfuse UI which will call them via a webhook.\\\\n## Why use Remote Dataset Runs?\\\\n  * Full flexibility to use your own application logic\\\\n  * Use custom scoring functions to evaluate the outputs\\\\n  * Run multiple experiments on the same dataset in parallel\\\\n  * Easy to integrate with your existing evaluation infrastructure\\\\nSequence Diagram\\\\n## Experiment runner SDK\\\\nBoth the Python and JS/TS SDKs provide a high-level abstraction for running an experiment on a dataset. The dataset can be both local or hosted on Langfuse. Using the Experiment runner is the recommended way to run an experiment on a dataset with our SDK.\\\\nThe experiment runner automatically handles:\\\\n  * **Concurrent execution** of tasks with configurable limits\\\\n  * **Automatic tracing** of all executions for observability\\\\n  * **Flexible evaluation** with both item-level and run-level evaluators\\\\n  * **Error isolation** so individual failures don\\\\u2019t stop the experiment\\\\n  * **Dataset integration** for easy comparison and tracking\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThe experiment runner SDK supports both datasets hosted on Langfuse and datasets hosted locally. If you are using a dataset hosted on Langfuse for your experiment, the SDK will automatically create a dataset run for you that you can inspect and compare in the Langfuse UI. For locally hosted datasets not on Langfuse, only traces and scores (if evaluations are used) are tracked in Langfuse.\\\\n### Basic Usage\\\\nStart with the simplest possible experiment to test your task function on local data. If you already have a dataset in Langfuse, see here.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\nfrom langfuse.openai import OpenAI\\\\n \\\\n///# Initialize client\\\\nlangfuse = get_client()\\\\n \\\\n///# Define your task function\\\\ndef my_task(*, item, **kwargs):\\\\n question = item[\\\\\\\"input\\\\\\\"]\\\\n response = OpenAI().chat.completions.create(\\\\n model=\\\\\\\"gpt-4.1\\\\\\\", messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": question}]\\\\n )\\\\n \\\\n return response.choices[0].message.content\\\\n \\\\n \\\\n///# Run experiment on local data\\\\nlocal_data = [\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of France?\\\\\\\"},\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of Germany?\\\\\\\"},\\\\n]\\\\n \\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Geography Quiz\\\\\\\",\\\\n description=\\\\\\\"Testing basic functionality\\\\\\\",\\\\n data=local_data,\\\\n task=my_task,\\\\n)\\\\n \\\\n///# Use format method to display results\\\\nprint(result.format())\\\\n```\\\\nMake sure that OpenTelemetry is properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\n    \\\\n    \\\\n```\\\\nimport { OpenAI } from \\\\\\\"openai\\\\\\\";\\\\nimport { NodeSDK } from \\\\\\\"@opentelemetry/sdk-node\\\\\\\";\\\\n \\\\nimport {\\\\n LangfuseClient,\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n ExperimentTask,\\\\n ExperimentItem,\\\\n} from \\\\\\\"@langfuse/client\\\\\\\";\\\\nimport { observeOpenAI } from \\\\\\\"@langfuse/openai\\\\\\\";\\\\nimport { LangfuseSpanProcessor } from \\\\\\\"@langfuse/otel\\\\\\\";\\\\n \\\\n// Initialize OpenTelemetry\\\\nconst otelSdk = new NodeSDK({ spanProcessors: [new LangfuseSpanProcessor()] });\\\\notelSdk.start();\\\\n \\\\n// Initialize client\\\\nconst langfuse = new LangfuseClient();\\\\n \\\\n// Run experiment on local data\\\\nconst localData: ExperimentItem[] = [\\\\n { input: \\\\\\\"What is the capital of France?\\\\\\\" },\\\\n { input: \\\\\\\"What is the capital of Germany?\\\\\\\" },\\\\n];\\\\n \\\\n// Define your task function\\\\nconst myTask: ExperimentTask = async (item) => {\\\\n const question = item.input;\\\\n \\\\n const response = await observeOpenAI(new OpenAI()).chat.completions.create({\\\\n model: \\\\\\\"gpt-4.1\\\\\\\",\\\\n messages: [\\\\n {\\\\n role: \\\\\\\"user\\\\\\\",\\\\n content: question,\\\\n },\\\\n ],\\\\n });\\\\n \\\\n return response;\\\\n};\\\\n \\\\n// Run the experiment\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Geography Quiz\\\\\\\",\\\\n description: \\\\\\\"Testing basic functionality\\\\\\\",\\\\n data: localData,\\\\n task: myTask,\\\\n});\\\\n \\\\n// Print formatted result\\\\nconsole.log(await result.format());\\\\n \\\\n// Important: shut down OTEL SDK to deliver traces\\\\nawait otelSdk.shutdown();\\\\n```\\\\n **Note for JS/TS SDK** : OpenTelemetry must be properly set up for traces to be delivered to Langfuse. See the [tracing setup documentation](/docs/observability/sdk/typescript/setup#tracing-setup) for configuration details. Always flush the span processor at the end of execution to ensure all traces are sent.\\\\nWhen running experiments on local data, only traces are created in Langfuse - no dataset runs are generated. Each task execution creates an individual trace for observability and debugging.\\\\n### Usage with Langfuse Datasets\\\\nRun experiments directly on datasets stored in Langfuse for automatic tracing and comparison.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\n///# Get dataset from Langfuse\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\ndataset = langfuse.get_dataset(\\\\\\\"my-evaluation-dataset\\\\\\\")\\\\n \\\\n///# Run experiment directly on the dataset\\\\nresult = dataset.run_experiment(\\\\n name=\\\\\\\"Production Model Test\\\\\\\",\\\\n description=\\\\\\\"Monthly evaluation of our production model\\\\\\\",\\\\n task=my_task\\\\n)\\\\n \\\\n///# Use format method to display results\\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\n// Get dataset from Langfuse\\\\nconst dataset = await langfuse.dataset.get(\\\\\\\"my-evaluation-dataset\\\\\\\");\\\\n \\\\n// Run experiment directly on the dataset\\\\nconst result = await dataset.runExperiment({\\\\n name: \\\\\\\"Production Model Test\\\\\\\",\\\\n description: \\\\\\\"Monthly evaluation of our production model\\\\\\\",\\\\n task: myTask\\\\n});\\\\n \\\\n// Use format method to display results\\\\nconsole.log(await result.format());\\\\n \\\\n// Important: shut down OpenTelemetry to ensure traces are sent to Langfuse\\\\nawait otelSdk.shutdown()\\\\n```\\\\nWhen using Langfuse datasets, dataset runs are automatically created in Langfuse and are available for comparison in the UI. This enables tracking experiment performance over time and comparing different approaches on the same dataset.\\\\n### Advanced Features\\\\nEnhance your experiments with evaluators and advanced configuration options.\\\\n#### Evaluators\\\\nEvaluators assess the quality of task outputs at the item level. They receive the input, metadata, output, and expected output for each item and return evaluation metrics that are reported as scores on the traces in Langfuse.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import Evaluation\\\\n \\\\n///# Define evaluation functions\\\\ndef accuracy_evaluator(*, input, output, expected_output, metadata, **kwargs):\\\\n if expected_output and expected_output.lower() in output.lower():\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=1.0, comment=\\\\\\\"Correct answer found\\\\\\\")\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=0.0, comment=\\\\\\\"Incorrect answer\\\\\\\")\\\\n \\\\ndef length_evaluator(*, input, output, **kwargs):\\\\n return Evaluation(name=\\\\\\\"response_length\\\\\\\", value=len(output), comment=f\\\\\\\"Response has {len(output)} characters\\\\\\\")\\\\n \\\\n///# Use multiple evaluators\\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Multi-metric Evaluation\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[accuracy_evaluator, length_evaluator]\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\n// Define evaluation functions\\\\nconst accuracyEvaluator = async ({ input, output, expectedOutput }) => {\\\\n if (expectedOutput && output.toLowerCase().includes(expectedOutput.toLowerCase())) {\\\\n return {\\\\n name: \\\\\\\"accuracy\\\\\\\",\\\\n value: 1.0,\\\\n comment: \\\\\\\"Correct answer found\\\\\\\"\\\\n };\\\\n }\\\\n return {\\\\n name: \\\\\\\"accuracy\\\\\\\",\\\\n value: 0.0,\\\\n comment: \\\\\\\"Incorrect answer\\\\\\\"\\\\n };\\\\n};\\\\n \\\\nconst lengthEvaluator = async ({ input, output }) => {\\\\n return {\\\\n name: \\\\\\\"response_length\\\\\\\",\\\\n value: output.length,\\\\n comment: `Response has ${output.length} characters`\\\\n };\\\\n};\\\\n \\\\n// Use multiple evaluators\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Multi-metric Evaluation\\\\\\\",\\\\n data: testData,\\\\n task: myTask,\\\\n evaluators: [accuracyEvaluator, lengthEvaluator]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Run-level Evaluators\\\\nRun-level evaluators assess the full experiment results and compute aggregate metrics. When run on Langfuse datasets, these scores are attached to the full dataset run for tracking overall experiment performance.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import Evaluation\\\\n \\\\ndef average_accuracy(*, item_results, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Calculate average accuracy across all items\\\\\\\"\\\\\\\"\\\\\\\"\\\\n accuracies = [\\\\n eval.value for result in item_results\\\\n for eval in result.evaluations\\\\n if eval.name == \\\\\\\"accuracy\\\\\\\"\\\\n ]\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n if not accuracies:\\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=None)\\\\n \\\\n avg = sum(accuracies) / len(accuracies)\\\\n \\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=avg, comment=f\\\\\\\"Average accuracy: {avg:.2%}\\\\\\\")\\\\n \\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Comprehensive Analysis\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy]\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst averageAccuracy = async ({ itemResults }) => {\\\\n // Calculate average accuracy across all items\\\\n const accuracies = itemResults\\\\n .flatMap(result => result.evaluations)\\\\n .filter(evaluation => evaluation.name === \\\\\\\"accuracy\\\\\\\")\\\\n .map(evaluation => evaluation.value as number);\\\\n \\\\n if (accuracies.length === 0) {\\\\n return { name: \\\\\\\"avg_accuracy\\\\\\\", value: null };\\\\n }\\\\n \\\\n const avg = accuracies.reduce((sum, val) => sum + val, 0) / accuracies.length;\\\\n \\\\n return {\\\\n name: \\\\\\\"avg_accuracy\\\\\\\",\\\\n value: avg,\\\\n comment: `Average accuracy: ${(avg * 100).toFixed(1)}%`\\\\n };\\\\n};\\\\n \\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Comprehensive Analysis\\\\\\\",\\\\n data: testData,\\\\n task: myTask,\\\\n evaluators: [accuracyEvaluator],\\\\n runEvaluators: [averageAccuracy]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Async Tasks and Evaluators\\\\nBoth task functions and evaluators can be asynchronous.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nimport asyncio\\\\nfrom langfuse.openai import AsyncOpenAI\\\\n \\\\nasync def async_llm_task(*, item, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Async task using OpenAI\\\\\\\"\\\\\\\"\\\\\\\"\\\\n client = AsyncOpenAI()\\\\n response = await client.chat.completions.create(\\\\n model=\\\\\\\"gpt-4\\\\\\\",\\\\n messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": item[\\\\\\\"input\\\\\\\"]}]\\\\n )\\\\n \\\\n return response.choices[0].message.content\\\\n \\\\n///# Works seamlessly with async functions\\\\nresult = langfuse.run_experiment(\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n name=\\\\\\\"Async Experiment\\\\\\\",\\\\n data=test_data,\\\\n task=async_llm_task,\\\\n max_concurrency=5 # Control concurrent API calls\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\nimport OpenAI from \\\\\\\"openai\\\\\\\";\\\\n \\\\nconst asyncLlmTask = async (item) => {\\\\n // Async task using OpenAI\\\\n const client = new OpenAI();\\\\n const response = await client.chat.completions.create({\\\\n model: \\\\\\\"gpt-4\\\\\\\",\\\\n messages: [{ role: \\\\\\\"user\\\\\\\", content: item.input }]\\\\n });\\\\n \\\\n return response.choices[0].message.content;\\\\n};\\\\n \\\\n// Works seamlessly with async functions\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Async Experiment\\\\\\\",\\\\n data: testData,\\\\n task: asyncLlmTask,\\\\n maxConcurrency: 5 // Control concurrent API calls\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Configuration Options\\\\nCustomize experiment behavior with various configuration options.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Configurable Experiment\\\\\\\",\\\\n run_name=\\\\\\\"Custom Run Name\\\\\\\", ///# will be dataset run name if dataset is used\\\\n description=\\\\\\\"Experiment with custom configuration\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[accuracy_evaluator],\\\\n run_evaluators=[average_accuracy],\\\\n max_concurrency=10, ///# Max concurrent executions\\\\n metadata={ ///# Attached to all traces\\\\n \\\\\\\"model\\\\\\\": \\\\\\\"gpt-4\\\\\\\",\\\\n \\\\\\\"temperature\\\\\\\": 0.7,\\\\n \\\\\\\"version\\\\\\\": \\\\\\\"v1.2.0\\\\\\\"\\\\n }\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"Configurable Experiment\\\\\\\",\\\\n runName: \\\\\\\"Custom Run Name\\\\\\\", // will be dataset run name if dataset is used\\\\n description: \\\\\\\"Experiment with custom configuration\\\\\\\",\\\\n data: testData,\\\\n task: myTask,\\\\n evaluators: [accuracyEvaluator],\\\\n runEvaluators: [averageAccuracy],\\\\n maxConcurrency: 10, // Max concurrent executions\\\\n metadata: { // Attached to all traces\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n model: \\\\\\\"gpt-4\\\\\\\",\\\\n temperature: 0.7,\\\\n version: \\\\\\\"v1.2.0\\\\\\\"\\\\n }\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n#### Testing in CI Environments\\\\nIntegrate the experiment runner with testing frameworks like Pytest and Vitest to run automated evaluations in your CI pipeline. Use evaluators to create assertions that can fail tests based on evaluation results.\\\\nPython SDKJS/TS SDK\\\\n    \\\\n    \\\\n```\\\\n///# test_geography_experiment.py\\\\nimport pytest\\\\nfrom langfuse import get_client, Evaluation\\\\nfrom langfuse.openai import OpenAI\\\\n \\\\n///# Test data for European capitals\\\\ntest_data = [\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of France?\\\\\\\", \\\\\\\"expected\\\\\\\": \\\\\\\"Paris\\\\\\\"},\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of Germany?\\\\\\\", \\\\\\\"expected\\\\\\\": \\\\\\\"Berlin\\\\\\\"},\\\\n {\\\\\\\"input\\\\\\\": \\\\\\\"What is the capital of Spain?\\\\\\\", \\\\\\\"expected\\\\\\\": \\\\\\\"Madrid\\\\\\\"},\\\\n]\\\\n \\\\ndef geography_task(*, item, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Task function that answers geography questions\\\\\\\"\\\\\\\"\\\\\\\"\\\\n question = item[\\\\\\\"input\\\\\\\"]\\\\n response = OpenAI().chat.completions.create(\\\\n model=\\\\\\\"gpt-4\\\\\\\",\\\\n messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": question}]\\\\n )\\\\n return response.choices[0].message.content\\\\n \\\\ndef accuracy_evaluator(*, input, output, expected_output, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Evaluator that checks if the expected answer is in the output\\\\\\\"\\\\\\\"\\\\\\\"\\\\n if expected_output and expected_output.lower() in output.lower():\\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=1.0)\\\\n \\\\n return Evaluation(name=\\\\\\\"accuracy\\\\\\\", value=0.0)\\\\n \\\\ndef average_accuracy_evaluator(*, item_results, **kwargs):\\\\n \\\\\\\"\\\\\\\"\\\\\\\"Run evaluator that calculates average accuracy across all items\\\\\\\"\\\\\\\"\\\\\\\"\\\\n accuracies = [\\\\n eval.value for result in item_results\\\\n for eval in result.evaluations if eval.name == \\\\\\\"accuracy\\\\\\\"\\\\n ]\\\\n \\\\n if not accuracies:\\\\n return Evaluation(name=\\\\\\\"avg_accuracy\\\\\\\", value=None)\\\\n \\\\n avg = sum(accuracies) / len(accuracies)\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nThese examples show how to use the experiment runner\\\\u2019s evaluation results to create meaningful test assertions in your CI pipeline. Tests can fail when accuracy drops below acceptable thresholds, ensuring model quality standards are maintained automatically.\\\\n### Autoevals Integration\\\\nAccess pre-built evaluation functions through the [autoevals library](https://github.com/braintrustdata/autoevals) integration.\\\\nPython SDKJS/TS SDK\\\\nThe Python SDK supports AutoEvals evaluators through direct integration:\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse.experiment import create_evaluator_from_autoevals\\\\nfrom autoevals.llm import Factuality\\\\n \\\\nevaluator = create_evaluator_from_autoevals(Factuality())\\\\n \\\\nresult = langfuse.run_experiment(\\\\n name=\\\\\\\"Autoevals Integration Test\\\\\\\",\\\\n data=test_data,\\\\n task=my_task,\\\\n evaluators=[evaluator]\\\\n)\\\\n \\\\nprint(result.format())\\\\n```\\\\nThe JS SDK provides seamless integration with the AutoEvals library for pre-built evaluation functions:\\\\n    \\\\n    \\\\n```\\\\nimport { Factuality, Levenshtein } from \\\\\\\"autoevals\\\\\\\";\\\\nimport { createEvaluatorFromAutoevals } from \\\\\\\"@langfuse/client\\\\\\\";\\\\n \\\\n// Convert AutoEvals evaluators to Langfuse-compatible format\\\\nconst factualityEvaluator = createEvaluatorFromAutoevals(Factuality());\\\\nconst levenshteinEvaluator = createEvaluatorFromAutoevals(Levenshtein());\\\\n \\\\n// Use with additional parameters\\\\nconst customFactualityEvaluator = createEvaluatorFromAutoevals(\\\\n Factuality,\\\\n { model: \\\\\\\"gpt-4o\\\\\\\" } // Additional AutoEvals parameters\\\\n);\\\\n \\\\nconst result = await langfuse.experiment.run({\\\\n name: \\\\\\\"AutoEvals Integration Test\\\\\\\",\\\\n data: testDataset,\\\\n task: myTask,\\\\n evaluators: [\\\\n factualityEvaluator,\\\\n levenshteinEvaluator,\\\\n customFactualityEvaluator\\\\n ]\\\\n});\\\\n \\\\nconsole.log(await result.format());\\\\n```\\\\n## Low-level SDK methods\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\n comment: \\\\\\\"This is a comment\\\\\\\", // optional, useful to add reasoning\\\\n });\\\\n}\\\\n \\\\n// Flush the langfuse client to ensure all score data is sent to the server at the end of the experiment run\\\\nawait langfuse.flush();\\\\n```\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\nfrom langfuse.langchain import CallbackHandler\\\\n///#from .app import my_llm_application\\\\n \\\\n///# Load the dataset\\\\ndataset = get_client().get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\n///# Initialize the Langfuse handler\\\\nlangfuse_handler = CallbackHandler()\\\\n \\\\n///# Loop over the dataset items\\\\nfor item in dataset.items:\\\\n # Use the item.run() context manager for automatic trace linking\\\\n with item.run(\\\\n run_name=\\\\\\\"<run_name>\\\\\\\",\\\\n run_description=\\\\\\\"My first run\\\\\\\",\\\\n run_metadata={\\\\\\\"model\\\\\\\": \\\\\\\"llama3\\\\\\\"},\\\\n ) as root_span:\\\\n # Execute your LLM-app against the dataset item input\\\\n output = my_langchain_chain(item.input, langfuse_handler)\\\\n \\\\n # Update top-level trace input and output\\\\n root_span.update_trace(input=item.input, output=output.content)\\\\n \\\\n # Optionally: Add scores computed in your experiment runner, e.g. json equality check\\\\n root_span.score_trace(\\\\n name=\\\\\\\"<example_eval>\\\\\\\",\\\\n value=my_eval_fn(item.input, output, item.expected_output),\\\\n comment=\\\\\\\"This is a comment\\\\\\\", ///# optional, useful to add reasoning\\\\n )\\\\n \\\\n///# Flush the langfuse client to ensure all data is sent to the server at the end of the experiment run\\\\nget_client().flush()\\\\n```\\\\n    \\\\n    \\\\n```\\\\nimport { LangfuseClient } from \\\\\\\"@langfuse/client\\\\\\\";\\\\nimport { CallbackHandler } from \\\\\\\"@langfuse/langchain\\\\\\\";\\\\n...\\\\n \\\\nconst langfuse = new LangfuseClient()\\\\nconst runName = \\\\\\\"my-dataset-run\\\\\\\";\\\\nfor (const item of dataset.items) {\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/guides/cookbook/example_external_evaluation_pipelines\\\", \\\"title\\\": \\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Evaluate Langfuse LLM Traces with an External Evaluation Pipeline - Langfuse\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\"Cookbooks\\\\\\\")External Evaluation Pipelines\\\\nCopy as Markdown\\\\nThis is a Jupyter notebook\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_external_evaluation_pipelines.ipynb)\\\\n# Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\\\\nThis cookbook explains how to build an external evaluation pipeline to measure the performance of your production LLM application using Langfuse.\\\\nAs a rule of thumb, we encourage you to check first if the [evaluations in the Langfuse UI](https://langfuse.com/docs/scores/model-based-evals) cover your use case. If your needs go beyond these, you can still implement in Langfuse custom evaluation templates without code.\\\\nConsider implementing an external evaluation pipeline if you need:\\\\n  * More control over **when** traces get evaluated. You could schedule the pipeline to run at specific times or responding to event-based triggers like Webhooks.\\\\n  * Greater flexibility with your custom evaluations, when your needs go beyond what\\\\u2019s possible with the Langfuse UI\\\\n  * Version control for your custom evaluations\\\\n  * The ability to evaluate data using existing evaluation frameworks\\\\nIf your use case meets any of this situations, let\\\\u2019s go ahead and implement your first external evaluation pipeline!\\\\n* * *\\\\nBy the end of this cookbook, you\\\\u2019ll be able to:\\\\n  * Create a synthetic dataset to test your models.\\\\n  * Use the Langfuse client to gather and filter traces of previous model runs\\\\n  * Evaluate these traces offline and incrementally\\\\n  * Add scores to existing Langfuse traces\\\\nConceptually, we will implement the following architecture:\\\\n* * *\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n**Note** : While we\\\\u2019re using a Jupyter notebook for this cookbook, in production you\\\\u2019d use your preferred orchestration tool. Just make sure to extract the code into a .py file and ensure all dependencies are available at runtime.\\\\n## (Prep-work) Loading synthetic traces to Langfuse\\\\nIn this demo, we\\\\u2019ll build a mock application: a science communicator LLM that explains any topic in an engaging and approachable way.\\\\nSince we don\\\\u2019t have real user data, our first step is to create a synthetic dataset. We\\\\u2019ll generate a variety of potential questions that real users might ask. While this is a great way to kickstart your LLM development, collecting real user queries as soon as possible is invaluable.\\\\nYou can get your Langfuse API keys [here](https://cloud.langfuse.com/) and OpenAI API key [here](https://platform.openai.com/api-keys)\\\\n    \\\\n    \\\\n```\\\\n%pip install langfuse openai deepeval --upgrade\\\\n```\\\\n    \\\\n    \\\\n```\\\\nimport os\\\\n \\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\nos.environ[\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\"] = \\\\\\\"pk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\"] = \\\\\\\"sk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddea\\\\ud83c\\\\uddfa EU region\\\\n///# os.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://us.cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddfa\\\\ud83c\\\\uddf8 US region\\\\n \\\\n///# Your openai key\\\\nos.environ[\\\\\\\"OPENAI_API_KEY\\\\\\\"] = \\\\\\\"sk-proj-...\\\\\\\"\\\\n```\\\\nLet\\\\u2019s go ahead and generate a list of topic suggestions that we can later query to our application.\\\\n    \\\\n    \\\\n```\\\\nimport openai\\\\n \\\\ntopic_suggestion = \\\\\\\"\\\\\\\"\\\\\\\" You're a world-class journalist, specialized\\\\nin figuring out which are the topics that excite people the most.\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nBATCH_SIZE = 10\\\\nTOTAL_TRACES = 50\\\\n \\\\nlangfuse = get_client()\\\\n \\\\nnow = datetime.now()\\\\nfive_am_today = datetime(now.year, now.month, now.day, 5, 0)\\\\nfive_am_yesterday = five_am_today - timedelta(days=1)\\\\n \\\\ntraces_batch = langfuse.api.trace.list(page=1,\\\\n limit=BATCH_SIZE,\\\\n tags=\\\\\\\"ext_eval_pipelines\\\\\\\",\\\\n from_timestamp=five_am_yesterday,\\\\n to_timestamp=datetime.now()\\\\n ).data\\\\n \\\\nprint(f\\\\\\\"Traces in first batch: {len(traces_batch)}\\\\\\\")\\\\n```\\\\nTraces in first batch: 10\\\\n## 2\\\\\\\\. Run your evaluations\\\\nLangfuse can handle numerical, boolean and categorical (`string`) scores. Wrapping your custom evaluation logic in a function is often a good practice. Evaluation functions should take a `trace` as input and yield a valid score. Let\\\\u2019s begin with a simple example using a categorical score.\\\\n### 2.1. Categoric Evaluations\\\\nWhen analyzing the outputs of your LLM applications, you may want to evaluate traits that are best defined qualitatively, such as sentiment, tonality or text complexity (Grade level).\\\\nWe\\\\u2019re building a science educator LLM that should sound engaging and positive. To ensure it hits the right notes, we\\\\u2019ll evaluate the tone of its outputs to see if they match our intent. We\\\\u2019ll draft an evaluation prompt ourselves (no library) to identify the three main tones in each model output.\\\\n    \\\\n    \\\\n```\\\\ntemplate_tone_eval = \\\\\\\"\\\\\\\"\\\\\\\"\\\\nYou're an expert in human emotional intelligence. You can identify with ease the\\\\n tone in human-written text. Your task is to identify the tones present in a\\\\n piece of <text/> with precission. Your output is a comma separated list of three\\\\n tones. PRINT THE LIST ALONE, NOTHING ELSE.\\\\n \\\\n<possible_tones>\\\\nneutral, confident, joyful, optimistic, friendly, urgent, analytical, respectful\\\\n</possible_tones>\\\\n \\\\n<example_1>\\\\nInput: Citizen science plays a crucial role in research by involving everyday\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\npeople in scientific projects. This collaboration allows researchers to collect\\\\nvast amounts of data that would be impossible to gather on their own. Citizen\\\\nscientists contribute valuable observations and insights that can lead to new\\\\ndiscoveries and advancements in various fields. By participating in citizen\\\\nscience projects, individuals can actively contribute to scientific research\\\\nand make a meaningful impact on our understanding of the world around us.\\\\n \\\\nOutput: respectful,optimistic,confident\\\\n</example_1>\\\\n \\\\n<example_2>\\\\nInput: Bionics is a field that combines biology and engineering to create\\\\ndevices that can enhance human abilities. By merging humans and machines,\\\\nbionics aims to improve quality of life for individuals with disabilities\\\\nor enhance performance for others. These technologies often mimic natural\\\\nprocesses in the body to create seamless integration. Overall, bionics holds\\\\ngreat potential for revolutionizing healthcare and technology in the future.\\\\n \\\\nOutput: optimistic,confident,analytical\\\\n</example_2>\\\\n \\\\n<example_3>\\\\nInput: Social media can have both positive and negative impacts on mental\\\\nhealth. On the positive side, it can help people connect, share experiences,\\\\nand find support. However, excessive use of social media can also lead to\\\\nfeelings of inadequacy, loneliness, and anxiety. It's important to find a\\\\nbalance and be mindful of how social media affects your mental well-being.\\\\nRemember, it's okay to take breaks and prioritize your mental health.\\\\n \\\\nOutput: friendly,neutral,respectful\\\\n</example_3>\\\\n \\\\n<text>\\\\n{text}\\\\n</text>\\\\n\\\\\\\"\\\\\\\"\\\\\\\"\\\\n \\\\n \\\\ntest_tone_score = openai.chat.completions.create(\\\\n messages=[\\\\n {\\\\n \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\",\\\\n \\\\\\\"content\\\\\\\": template_tone_eval.format(\\\\n text=traces_batch[1].output),\\\\n }\\\\n ],\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n model=\\\\\\\"gpt-4o\\\\\\\",\\\\n \\\\n temperature=0\\\\n).choices[0].message.content\\\\nprint(f\\\\\\\"User query: {traces_batch[1].input['args'][0]}\\\\\\\")\\\\nprint(f\\\\\\\"Model answer: {traces_batch[1].output}\\\\\\\")\\\\nprint(f\\\\\\\"Dominant tones: {test_tone_score}\\\\\\\")\\\\n```\\\\nIdentifying human intents and tones can be tricky for language models. To handle this, we used a multi-shot prompt, which means giving the model several examples to learn from. Now let\\\\u2019s wrap our code in an evaluation function for convenience.\\\\n    \\\\n    \\\\n```\\\\ndef tone_score(trace):\\\\n return openai.chat.completions.create(\\\\n messages=[\\\\n {\\\\n \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\",\\\\n \\\\\\\"content\\\\\\\": template_tone_eval.format(text=trace.output),\\\\n }\\\\n ],\\\\n model=\\\\\\\"gpt-4o\\\\\\\",\\\\n temperature=0\\\\n ).choices[0].message.content\\\\n \\\\ntone_score(traces_batch[1])\\\\n```\\\\nGreat! Now let\\\\u2019s go ahead and create a numeric evaluation score.\\\\n### 2.2. Numeric Evaluations\\\\nIn this cookbook, we\\\\u2019ll use the `Deepeval` framework ([docs](https://docs.confident-ai.com/docs/getting-started)) to handle our numeric evaluations. Deepeval provides scores ranging from zero to one for many common LLM metrics. Plus, you can create custom metrics by simply describing them in plain language. To ensure our app\\\\u2019s responses are joyful and engaging, we\\\\u2019ll define a custom \\\\u2018joyfulness\\\\u2019 score.\\\\nYou can use any evaluation library. These are popular ones:\\\\n  * OpenAI Evals ([GitHub](https://github.com/openai/evals))\\\\n  * Langchain Evaluators\\\\n  * [RAGAS](https://docs.ragas.io/en/latest/concepts/metrics/index.html) for RAG applications\\\\n    \\\\n    \\\\n```\\\\nfrom deepeval.metrics import GEval\\\\nfrom deepeval.test_case import LLMTestCaseParams, LLMTestCase\\\\n \\\\ndef joyfulness_score(trace):\\\\n joyfulness_metric = GEval(\\\\n name=\\\\\\\"Correctness\\\\\\\",\\\\n criteria=\\\\\\\"Determine whether the output is engaging and fun.\\\\\\\",\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\\\\n )\\\\n test_case = LLMTestCase(\\\\n input=trace.input[\\\\\\\"args\\\\\\\"],\\\\n actual_output=trace.output)\\\\n \\\\n joyfulness_metric.measure(test_case)\\\\n \\\\n print(f\\\\\\\"Score: {joyfulness_metric.score}\\\\\\\")\\\\n print(f\\\\\\\"Reason: {joyfulness_metric.reason}\\\\\\\")\\\\n \\\\n return {\\\\\\\"score\\\\\\\": joyfulness_metric.score, \\\\\\\"reason\\\\\\\": joyfulness_metric.reason}\\\\n \\\\njoyfulness_score(traces_batch[1])\\\\n```\\\\nUnder the hood, GEval uses chain of thought (CoT) prompting to formulate a set of criteria for scoring prompts. When developing your own metrics, it\\\\u2019s important to review the reasoning behind these scores. This helps ensure that the model evaluates the traces just as you intended when you wrote the evaluation prompt.\\\\nOur eval function returns a dictionary with both the score and the model\\\\u2019s reasoning. We do this as we\\\\u2019ll persist the reasoning with every langfuse score, ensuring interpretability.\\\\nNow we\\\\u2019re done with defining our evaluation functions. Let\\\\u2019s push those scores back to Langfuse!\\\\n## 3\\\\\\\\. Pushing Scores to Langfuse\\\\nNow that we have our evaluation functions ready, it\\\\u2019s time to put them to work. Use the Langfuse client to add scores to existing traces.\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_score(\\\\n trace_id=traces_batch[1].id,\\\\n name=\\\\\\\"tone\\\\\\\",\\\\n value=joyfulness_score(traces_batch[1])[\\\\\\\"score\\\\\\\"],\\\\n comment=joyfulness_score(traces_batch[1])[\\\\\\\"reason\\\\\\\"]\\\\n)\\\\n```\\\\nAnd thus, you\\\\u2019ve added your first externally-evaluated score to Langfuse! Just 49 more to go \\\\ud83d\\\\ude01. But don\\\\u2019t worry \\\\u2014 our solutions are easy to scale.\\\\n## 4\\\\\\\\. Putting everything together\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nUntil now, we went through each of the necessary steps to build an external evaluation pipeline: Fetching traces, running the evaluations, and persisting the scores to Langfuse. Let\\\\u2019s sum it up into a compact script that you could run in your evaluation pipeline.\\\\nWe\\\\u2019ll fetch the data in batches of 10 traces and then iterate through each trace to score it and push the scores back to Langfuse. Note that this batch size is for demonstration purposes. In a production setup, you might want to process multiple batches in parallel to speed things up. Batching not only reduces the memory load on your system but also allows you to create checkpoints, so you can easily resume if something goes wrong.\\\\n    \\\\n    \\\\n```\\\\nimport math\\\\n \\\\nfor page_number in range(1, math.ceil(TOTAL_TRACES/BATCH_SIZE)):\\\\n \\\\n traces_batch = langfuse.api.trace.list(\\\\n tags=\\\\\\\"ext_eval_pipelines\\\\\\\",\\\\n page=page_number,\\\\n from_timestamp=five_am_yesterday,\\\\n to_timestamp=five_am_today,\\\\n limit=BATCH_SIZE\\\\n ).data\\\\n \\\\n for trace in traces_batch:\\\\n print(f\\\\\\\"Processing {trace.name}\\\\\\\")\\\\n \\\\n if trace.output is None:\\\\n print(f\\\\\\\"Warning: \\\\\\\\n Trace {trace.name} had no generated output, \\\\\\\\\\\\n it was skipped\\\\\\\")\\\\n continue\\\\n \\\\n langfuse.create_score(\\\\n trace_id=trace.id,\\\\n name=\\\\\\\"tone\\\\\\\",\\\\n value=tone_score(trace)\\\\n )\\\\n \\\\n jscore = joyfulness_score(trace)\\\\n langfuse.create_score(\\\\n trace_id=trace.id,\\\\n name=\\\\\\\"joyfulness\\\\\\\",\\\\n value=jscore[\\\\\\\"score\\\\\\\"],\\\\n comment=jscore[\\\\\\\"reason\\\\\\\"]\\\\n )\\\\n \\\\n print(f\\\\\\\"Batch {page_number} processed \\\\ud83d\\\\ude80 \\\\\\\\n\\\\\\\")\\\\n```\\\\nIf your pipeline ran successfully, you should see your score in the Langfuse UI. \\\\nAnd that\\\\u2019s it! You\\\\u2019re now ready to integrate these lines into your preferred orchestration tool to ensure they run at the right times.\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nTo achieve our original goal of running the script every day at 5 am, simply schedule a Cron task in your chosen environment with the rule `cron(0 5 * * ? *)`.\\\\nThanks for coding along! I hope you enjoyed the tutorial and found it helpful.\\\\nLast updated on August 20, 2025\\\\n[Example - Tracing and Evaluation for the OpenAI-Agents SDK](/guides/cookbook/example_evaluating_openai_agents \\\\\\\"Example - Tracing and Evaluation for the OpenAI-Agents SDK\\\\\\\")\\\\n[Guide - Building an intent classification pipeline](/guides/cookbook/example_intent_classification_pipeline \\\\\\\"Guide - Building an intent classification pipeline\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"github_discussion\\\", \\\"url\\\": \\\"https://github.com/orgs/langfuse/discussions/7572\\\", \\\"title\\\": \\\"LLM-as-a-Judge Evaluators for Dataset doesn't work as expected.\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\n\\\\n---\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=hallucination) | [Report \\\\ud83d\\\\udc1b](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/636202a2-7fb2-4a3b-bb7a-57bb4a09ca28?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\n\\\\n        Reply #5 to comment #1 from dosubot on June 30, 2025:\\\\n\\\\n        I am using ui and variable mapping is as follows:\\\\r\\\\n\\\\r\\\\n{{search_query}} is mapped to Object: Dataset item, Object Variable: Input, JsonPath: <empty>\\\\r\\\\n{{summary}} is mapped to Object: Dataset item, Object Variable: Metadata, JsonPath: <empty>\\\\r\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"{{expected}} is mapped to Object: Dataset item,  Object Variable: Expected Output, JsonPath: <empty>\\\\r\\\\n\\\\r\\\\nA single row format example from my dataset: \\\\r\\\\nInput column: \\\\\\\"Test input\\\\\\\", Expected Output column: \\\\\\\"Test expected output\\\\\\\", Metadata column: \\\\\\\"Test metadata\\\\\\\"\\\\n\\\\n        Reply #6 to comment #1 from dosubot on June 30, 2025:\\\\n\\\\n        <!-- Answer -->\\\\nYour mapping setup looks correct, but the backend eligibility logic is strict: if any mapped variable can't be resolved (for example, if the field is empty or the type doesn't match expectations), the item is silently skipped and you get \\\\\\\"0 elements\\\\\\\" processed. \\\\n\\\\nOne likely issue is the Metadata field\\\\u2014if it's a plain string (like \\\\\\\"Test metadata\\\\\\\"), but the system expects a JSON object (since many Langfuse examples use objects for metadata), this can cause the mapping to fail even if JsonPath is empty. When JsonPath is empty, the backend uses the top-level value as-is, but if the field is a string and the mapping expects an object (or vice versa), the item is excluded from evaluation\\\\u00a0[see code logic](https://github.com/langfuse/langfuse/blob/98cc1bb7a1759e29de3dd818f6a500df48fb5e82/worker/src/features/evaluation/evalService.ts).\\\\n\\\\nTry this: map all variables to fields you know are plain strings (like Input or Expected Output) and see if the evaluation processes any items. If it works, the issue is with the Metadata field's type. To confirm, create a dataset item where Metadata is a JSON object (e.g., {\\\\\\\"meta\\\\\\\": \\\\\\\"value\\\\\\\"}) and try mapping with and without a JsonPath. This will help isolate the problem.\\\\n\\\\nIf you still get 0 elements, export a sample of your dataset as JSON and share it\\\\u2014there may be a subtle data or mapping mismatch. \\\\n\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\nMore details on dataset item structure and mapping: [Langfuse docs](https://langfuse.com/docs/datasets/get-started).\\\\n\\\\n\\\\n<!-- Dosu Comment Footer -->\\\\n*To reply, just mention [@dosu](https://go.dosu.dev/dosubot).*\\\\n\\\\n---\\\\n<sup>How did I do? [Good](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=great_response) | [Irrelevant](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=irrelevant_answer) | [Incorrect](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=incorrect_sources) | [Verbose](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=too_verbose) | [Hallucination](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=hallucination) | [Report \\\\ud83d\\\\udc1b](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=bug_report) | [Other](https://app.dosu.dev/response-feedback/cad1b9d0-ec72-4d48-bb2f-d42d71dfbb4c?feedback_type=other)</sup>&nbsp;&nbsp;[![Join Discord](https://img.shields.io/badge/join-5865F2?logo=discord&logoColor=white&label=)](https://go.dosu.dev/discord-bot)&nbsp;[![Share on X](https://img.shields.io/badge/X-share-black)](https://twitter.com/intent/tweet?text=%40dosu_ai%20helped%20me%20solve%20this%20issue!&url=https%3A//github.com/orgs/langfuse/discussions/7572)\\\\n\\\\n        Reply #7 to comment #1 from dosubot on June 30, 2025:\\\\n\\\\n        I have created a new dataset with single row in json format, so columns Input, Expected Output and Metadata have following values respectively but still the same issue:\\\\r\\\\n{ \\\\\\\"search_query\\\\\\\": \\\\\\\"What's MLflow?\\\\\\\" }\\\\r\\\\n\\\\r\\\\n{ \\\\\\\"expected\\\\\\\": \\\\\\\"MLflow tracks ML experiments.\\\\\\\" } \\\\r\\\\n\\\\r\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2023-09-25-datasets\\\", \\\"title\\\": \\\"Datasets (beta)\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Datasets (beta) - Langfuse\\\\n[\\\\u2190 Back to changelog](/changelog#2023-09-25-datasets)\\\\nSeptember 25, 2023\\\\n# Datasets (beta)\\\\n[Marc Klingen](https://twitter.com/marcklingen)\\\\nCollect sets of inputs and expected outputs in Langfuse to evaluate your LLM app. Use evaluations to benchmark different experiments.\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\n_Run experiment on dataset_\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\n    \\\\n    \\\\n```\\\\n \\\\ndataset = langfuse.get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\nfor item in dataset.items:\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\n # output also returned as it is used to evaluate the run\\\\n generation, output = my_llm_application.run(item.input)\\\\n \\\\n # link the execution trace to the dataset item and give it a run_name\\\\n item.link(generation, \\\\\\\"<run_name>\\\\\\\")\\\\n \\\\n # optionally, evaluate the output to compare different runs more easily\\\\n generation.score(\\\\n name=\\\\\\\"<example_eval>\\\\\\\",\\\\n # any float value\\\\n value=my_eval_fn(\\\\n item.input,\\\\n output,\\\\n item.expected_output\\\\n )\\\\n )\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst dataset = await langfuse.getDataset(\\\\\\\"<dataset_name>\\\\\\\");\\\\n \\\\nfor (const item of dataset.items) {\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\n // output also returned as it is used to evaluate the run\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\n \\\\n // link the execution trace to the dataset item and give it a run_name\\\\n await item.link(generation, \\\\\\\"<run_name>\\\\\\\");\\\\n \\\\n // optionally, evaluate the output to compare different runs more easily\\\\n generation.score({\\\\n name: \\\\\\\"<score_name>\\\\\\\",\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\n });\\\\n}\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/troubleshooting-and-faq\\\", \\\"title\\\": \\\"Troubleshooting and FAQ\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Custom trace_id for traces created with dataset's `item.run()` method](https://github.com/orgs/langfuse/discussions/8254 \\\\\\\"Langfuse Support: Custom trace_id for traces created with dataset's `item.run\\\\\\\\(\\\\\\\\)` method\\\\\\\")\\\\n[how to create langfuse datasetrun in langfuse-java sdk?](https://github.com/orgs/langfuse/discussions/8184 \\\\\\\"Langfuse Support: how to create langfuse datasetrun in langfuse-java sdk?\\\\\\\")\\\\n[[Experiment] Send items in parallel. Stop experiment.](https://github.com/orgs/langfuse/discussions/7701 \\\\\\\"Langfuse Support: \\\\\\\\[Experiment\\\\\\\\] Send items in parallel. Stop experiment.\\\\\\\")\\\\n[Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing](https://github.com/orgs/langfuse/discussions/6669 \\\\\\\"Langfuse Support: Focused Mode shows only the final ai.generateObject span\\\\u2014earlier generations are missing\\\\\\\")\\\\n[Using scores in data sets from the SDK?](https://github.com/orgs/langfuse/discussions/6016 \\\\\\\"Langfuse Support: Using scores in data sets from the SDK?\\\\\\\")\\\\n[Results for some data items not present when comparing experiments](https://github.com/orgs/langfuse/discussions/5928 \\\\\\\"Langfuse Support: Results for some data items not present when comparing experiments\\\\\\\")\\\\n[Deleting Metrics for Langfuse](https://github.com/orgs/langfuse/discussions/5849 \\\\\\\"Langfuse Support: Deleting Metrics for Langfuse\\\\\\\")\\\\n[Discrepancies between dataset items found in the UI vs retrieved from the SDK/API](https://github.com/orgs/langfuse/discussions/5822 \\\\\\\"Langfuse Support: Discrepancies between dataset items found in the UI vs retrieved from the SDK/API\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Request batch adding of multiple traces to datasets](https://github.com/orgs/langfuse/discussions/8526 \\\\\\\"Langfuse Ideas: Request batch adding of multiple traces to datasets\\\\\\\")\\\\n[How to categorize topics and create a pie chart?](https://github.com/orgs/langfuse/discussions/8512 \\\\\\\"Langfuse Ideas: How to categorize topics and create a pie chart?\\\\\\\")\\\\n[Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window](https://github.com/orgs/langfuse/discussions/8485 \\\\\\\"Langfuse Ideas: Simultaneous Session and Trace Annotation for Context-Rich Feedback in the same annotation window\\\\\\\")\\\\n[feat(LLM-as-a-judge): support stratified sampling by trace property](https://github.com/orgs/langfuse/discussions/8480 \\\\\\\"Langfuse Ideas: feat\\\\\\\\(LLM-as-a-judge\\\\\\\\): support stratified sampling by trace property\\\\\\\")\\\\n[Bulk Evaluation Does not Displays the status of the dataset (Like for E.g. processing, evaluated, executed)](https://github.com/orgs/langfuse/discussions/8410 \\\\\\\"Langfuse Ideas: Bulk Evaluation Does not Displays the status of the dataset \\\\\\\\(Like for E.g. processing, evaluated, executed\\\\\\\\)\\\\\\\")\\\\n[Annotation Queue Creation API](https://github.com/orgs/langfuse/discussions/8372 \\\\\\\"Langfuse Ideas: Annotation Queue Creation API\\\\\\\")\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\"Langfuse Ideas: \\\\\\\\[Langfuse Cloud\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\")\\\\n[feat(evals): allow canceling a running evaluator with pending evaluation jobs](https://github.com/orgs/langfuse/discussions/8310 \\\\\\\"Langfuse Ideas: feat\\\\\\\\(evals\\\\\\\\): allow canceling a running evaluator with pending evaluation jobs\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Support Description on Dataset Items](https://github.com/orgs/langfuse/discussions/6011 \\\\\\\"Langfuse Ideas: Support Description on Dataset Items\\\\\\\")\\\\n[Alta Integration](https://github.com/orgs/langfuse/discussions/5957 \\\\\\\"Langfuse Ideas: Alta Integration\\\\\\\")\\\\n[Add run name to columns when looking at a specific dataset item](https://github.com/orgs/langfuse/discussions/5929 \\\\\\\"Langfuse Ideas: Add run name to columns when looking at a specific dataset item\\\\\\\")\\\\n[Delete multiple dataset runs](https://github.com/orgs/langfuse/discussions/5893 \\\\\\\"Langfuse Ideas: Delete multiple dataset runs\\\\\\\")\\\\n[Navigation between items in a dataset run is confusing - context of the selected dataset run is lost](https://github.com/orgs/langfuse/discussions/5892 \\\\\\\"Langfuse Ideas: Navigation between items in a dataset run is confusing - context of the selected dataset run is lost\\\\\\\")\\\\n[Enhanced score distribution visualization in experiment analysis](https://github.com/orgs/langfuse/discussions/5819 \\\\\\\"Langfuse Ideas: Enhanced score distribution visualization in experiment analysis\\\\\\\")\\\\n[Multi-step Prompt Experiments and Playground](https://github.com/orgs/langfuse/discussions/5812 \\\\\\\"Langfuse Ideas: Multi-step Prompt Experiments and Playground\\\\\\\")\\\\n[Option to add trace to new dataset](https://github.com/orgs/langfuse/discussions/5756 \\\\\\\"Langfuse Ideas: Option to add trace to new dataset\\\\\\\")\\\\n[Simplified UI for Scoring](https://github.com/orgs/langfuse/discussions/5721 \\\\\\\"Langfuse Ideas: Simplified UI for Scoring\\\\\\\")\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Datasets: Add selection of traces to a dataset](https://github.com/orgs/langfuse/discussions/4353 \\\\\\\"Langfuse Ideas: Datasets: Add selection of traces to a dataset\\\\\\\")\\\\n[Multi-user annotation capability in Annotation Queues](https://github.com/orgs/langfuse/discussions/4348 \\\\\\\"Langfuse Ideas: Multi-user annotation capability in Annotation Queues\\\\\\\")\\\\n[Multi-turn / session experiments in datasets](https://github.com/orgs/langfuse/discussions/4208 \\\\\\\"Langfuse Ideas: Multi-turn / session experiments in datasets\\\\\\\")\\\\n[Enable to use variable of prompt on evaluator.](https://github.com/orgs/langfuse/discussions/4121 \\\\\\\"Langfuse Ideas: Enable to use variable of prompt on evaluator.\\\\\\\")\\\\n[Sessions Table: Scores Column](https://github.com/orgs/langfuse/discussions/4120 \\\\\\\"Langfuse Ideas: Sessions Table: Scores Column\\\\\\\")\\\\n[Add new filters for the LLM as a Judge Evaluation (other scores and cost)](https://github.com/orgs/langfuse/discussions/4106 \\\\\\\"Langfuse Ideas: Add new filters for the LLM as a Judge Evaluation \\\\\\\\(other scores and cost\\\\\\\\)\\\\\\\")\\\\n[Export dataset run table](https://github.com/orgs/langfuse/discussions/4077 \\\\\\\"Langfuse Ideas: Export dataset run table\\\\\\\")\\\\n[feat: support adding trace tags in annotation queue view](https://github.com/orgs/langfuse/discussions/4037 \\\\\\\"Langfuse Ideas: feat: support adding trace tags in annotation queue view\\\\\\\")\\\\n[Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times](https://github.com/orgs/langfuse/discussions/4035 \\\\\\\"Langfuse Ideas: Feat: De-dupe and show info when adding the same trace/observation to an annotation queue multiple times\\\\\\\")\\\\n[Diff support for dataset runs view](https://github.com/orgs/langfuse/discussions/4025 \\\\\\\"Langfuse Ideas: Diff support for dataset runs view\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Create Support for gemini models in playground](https://github.com/orgs/langfuse/discussions/4019 \\\\\\\"Langfuse Ideas: Create Support for gemini models in playground\\\\\\\")\\\\n[Change AWS access pattern for Bedrock LLM usage, assume role](https://github.com/orgs/langfuse/discussions/3988 \\\\\\\"Langfuse Ideas: Change AWS access pattern for Bedrock LLM usage, assume role\\\\\\\")\\\\n[Add ability to export and import evaluators between projects](https://github.com/orgs/langfuse/discussions/3970 \\\\\\\"Langfuse Ideas: Add ability to export and import evaluators between projects\\\\\\\")\\\\n[feat: Folder structure for dataset organisation](https://github.com/orgs/langfuse/discussions/3935 \\\\\\\"Langfuse Ideas: feat: Folder structure for dataset organisation\\\\\\\")\\\\n[Model-based evaluations triggered by observations](https://github.com/orgs/langfuse/discussions/3918 \\\\\\\"Langfuse Ideas: Model-based evaluations triggered by observations\\\\\\\")\\\\n[Scores: Conditional Annotation](https://github.com/orgs/langfuse/discussions/3842 \\\\\\\"Langfuse Ideas: Scores: Conditional Annotation\\\\\\\")\\\\n[Annotation Queues: define optional/mandatory score configs by queue](https://github.com/orgs/langfuse/discussions/3841 \\\\\\\"Langfuse Ideas: Annotation Queues: define optional/mandatory score configs by queue\\\\\\\")\\\\n[Scores: support for recording multiple choice selection as score value](https://github.com/orgs/langfuse/discussions/3840 \\\\\\\"Langfuse Ideas: Scores: support for recording multiple choice selection as score value\\\\\\\")\\\\n[Filter by status in dataset items table](https://github.com/orgs/langfuse/discussions/3818 \\\\\\\"Langfuse Ideas: Filter by status in dataset items table\\\\\\\")\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/overview\\\", \\\"title\\\": \\\"Evaluation Overview\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Evaluation of LLM Applications - Langfuse\\\\nDocsEvaluationOverview\\\\nCopy as Markdown\\\\n# Evaluation Overview\\\\nEvaluation is a critical aspect of developing and deploying LLM applications. Usually, teams use a multitude of different evaluation methods to score the performance of their AI application depending on the use case and the stage of the development process.\\\\n\\\\ud83c\\\\udfa5\\\\n[ **Watch this walkthrough**](/watch-demo?tab=evaluation) of Langfuse Evaluation and how to use it to improve your LLM application.\\\\n## Why use LLM Evaluation?\\\\nLLM evaluation is crucial for improving the accuracy and robustness of language models, ultimately enhancing the user experience and trust in your AI application. Here are the key benefits:\\\\n  * **Quality Assurance** : Detect hallucinations, factual inaccuracies, and inconsistent outputs to ensure your AI app delivers reliable results\\\\n  * **Performance Monitoring** : Measure response quality, relevance, and user satisfaction across different scenarios and edge cases\\\\n  * **Continuous Improvement** : Identify areas for enhancement and track improvements over time through structured evaluation metrics\\\\n  * **User Trust** : Build confidence in your AI application by demonstrating consistent, high-quality outputs through systematic evaluation\\\\n  * **Risk Mitigation** : Catch potential issues before they reach production users, reducing the likelihood of poor user experiences or reputational damage\\\\n## Online & Offline Evaluation\\\\n**Offline Evaluation involves**\\\\n  * Evaluating the application in a controlled setting\\\\n  * Typically using curated test Datasets instead of live user queries\\\\n  * Heavily used during development (can be part of CI/CD pipelines) to measure improvements / regressions\\\\n  * Repeatable and you can get clear accuracy metrics since you have ground truth.\\\\n**Online Evaluation involves**\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Python SDK: Pass ScoreConfig instead of config_id when creating scores](https://github.com/orgs/langfuse/discussions/8623 \\\\\\\"Langfuse Ideas: Python SDK: Pass ScoreConfig instead of config_id when creating scores\\\\\\\")\\\\n[[Langfuse Cloud] Missing SessionId and Author in exported Scores](https://github.com/orgs/langfuse/discussions/8346 \\\\\\\"Langfuse Ideas: \\\\\\\\[Langfuse Cloud\\\\\\\\] Missing SessionId and Author in exported Scores\\\\\\\")\\\\n[Score Configs: Allow editing the categories of a categorical score](https://github.com/orgs/langfuse/discussions/8259 \\\\\\\"Langfuse Ideas: Score Configs: Allow editing the categories of a categorical score\\\\\\\")\\\\n[UI-LLM as a Jury](https://github.com/orgs/langfuse/discussions/8195 \\\\\\\"Langfuse Ideas: UI-LLM as a Jury\\\\\\\")\\\\n[Enable Immediate Score Management for User Feedback](https://github.com/orgs/langfuse/discussions/7686 \\\\\\\"Langfuse Ideas: Enable Immediate Score Management for User Feedback\\\\\\\")\\\\n[Filter by scores in session view](https://github.com/orgs/langfuse/discussions/7528 \\\\\\\"Langfuse Ideas: Filter by scores in session view\\\\\\\")\\\\n[Support break lines on evaluation run tooltip hint](https://github.com/orgs/langfuse/discussions/7452 \\\\\\\"Langfuse Ideas: Support break lines on evaluation run tooltip hint\\\\\\\")\\\\n[Support new lines when storing / displaying score comments](https://github.com/orgs/langfuse/discussions/6473 \\\\\\\"Langfuse Ideas: Support new lines when storing / displaying score comments\\\\\\\")\\\\n[Evaluator: Filter for Scores](https://github.com/orgs/langfuse/discussions/6236 \\\\\\\"Langfuse Ideas: Evaluator: Filter for Scores\\\\\\\")\\\\n[Code-based custom evaluators](https://github.com/orgs/langfuse/discussions/6087 \\\\\\\"Langfuse Ideas: Code-based custom evaluators\\\\\\\")\\\\n[More scoring configurations on UI](https://github.com/orgs/langfuse/discussions/5719 \\\\\\\"Langfuse Ideas: More scoring configurations on UI\\\\\\\")\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Session-level scores](https://github.com/orgs/langfuse/discussions/2728 \\\\\\\"Langfuse Ideas: Session-level scores\\\\\\\")\\\\n[Scoring dataset runs, e.g. precision, recall, f-value](https://github.com/orgs/langfuse/discussions/2511 \\\\\\\"Langfuse Ideas: Scoring dataset runs, e.g. precision, recall, f-value\\\\\\\")\\\\n[Adding userId / author to score (custom metadata)](https://github.com/orgs/langfuse/discussions/2469 \\\\\\\"Langfuse Ideas: Adding userId / author to score \\\\\\\\(custom metadata\\\\\\\\)\\\\\\\")\\\\n[Add string data type in score config](https://github.com/orgs/langfuse/discussions/2402 \\\\\\\"Langfuse Ideas: Add string data type in score config\\\\\\\")\\\\n[API to delete scores](https://github.com/orgs/langfuse/discussions/1133 \\\\\\\"Langfuse Ideas: API to delete scores\\\\\\\")\\\\nGitHubSupportGitHubIdeas\\\\nUpvotes[GitHubNew](https://github.com/orgs/langfuse/discussions/new/choose)\\\\n  * 9votes\\\\n[Update/delete score using python sdk](https://github.com/orgs/langfuse/discussions/1486)\\\\nmsanand\\\\u20223/25/2024\\\\u2022\\\\n2Resolved\\\\n  * 4votes\\\\n[Filter Categorical Score Values](https://github.com/orgs/langfuse/discussions/3797)\\\\nalabrashJr\\\\u202210/17/2024\\\\u2022\\\\n3Resolved\\\\n  * 3votes\\\\n[How to get experiment run scores programmatically?](https://github.com/orgs/langfuse/discussions/8590)\\\\nanuras\\\\u20228/18/2025\\\\u2022\\\\n1Resolved\\\\n  * 3votes\\\\n[Support for Metric Calculation (Precision@K, Recall@K) and Adding Custom Metrics Use Case Overview](https://github.com/orgs/langfuse/discussions/5215)\\\\nsrimantacse\\\\u20221/27/2025\\\\u2022\\\\n2Resolved\\\\n  * 2votes\\\\n[Scoring using span object or using trace id doesn't seem to work in remote runs for dataset evaluations](https://github.com/orgs/langfuse/discussions/8556)\\\\nflabbergastedbd\\\\u20228/15/2025\\\\u2022\\\\n1\\\\n  * 1votes\\\"}], \\\"type\\\": \\\"content\\\"}}]}\"}],\"isError\":false}",
      "modelParameters": {
        "maxSteps": null,
        "toolChoice": null,
        "maxTokens": null,
        "finishReason": null,
        "system": null,
        "maxRetries": null,
        "mode": null,
        "temperature": null
      },
      "completionStartTime": null,
      "promptId": null,
      "createdAt": "2025-09-19T06:51:30.170Z",
      "updatedAt": "2025-09-19T06:51:30.170Z",
      "usageDetails": {},
      "costDetails": {},
      "providedCostDetails": {},
      "model": null,
      "internalModelId": null,
      "promptName": null,
      "promptVersion": null,
      "latency": 4800,
      "timeToFirstToken": null,
      "inputCost": null,
      "outputCost": null,
      "totalCost": 0,
      "inputUsage": 0,
      "outputUsage": 0,
      "totalUsage": 0,
      "internalModel": null
    },
    {
      "id": "0f202199b1c8adf7",
      "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "SPAN",
      "environment": "default",
      "parentObservationId": "2126822ce3db7c07",
      "startTime": "2025-09-19T06:51:14.796Z",
      "endTime": "2025-09-19T06:51:15.715Z",
      "name": "create-mcp-client",
      "metadata": "{}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "modelParameters": null,
      "completionStartTime": null,
      "promptId": null,
      "createdAt": "2025-09-19T06:51:19.874Z",
      "updatedAt": "2025-09-19T06:51:19.875Z",
      "usageDetails": {},
      "costDetails": {},
      "providedCostDetails": {},
      "model": null,
      "internalModelId": null,
      "promptName": null,
      "promptVersion": null,
      "latency": 919,
      "timeToFirstToken": null,
      "inputCost": null,
      "outputCost": null,
      "totalCost": 0,
      "inputUsage": 0,
      "outputUsage": 0,
      "totalUsage": 0
    },
    {
      "id": "86931488451a1240",
      "traceId": "ac29be521ce79d5df06d9081b50f2cb3",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "SPAN",
      "environment": "default",
      "parentObservationId": "2126822ce3db7c07",
      "startTime": "2025-09-19T06:51:14.795Z",
      "endTime": "2025-09-19T06:51:14.795Z",
      "name": "get-langfuse-prompt",
      "metadata": "{\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"langfuse-sdk\",\"version\":\"4.0.0\",\"attributes\":{}}}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "input": "langfuse-docs-assistant-text",
      "output": "\"{\\\"name\\\":\\\"langfuse-docs-assistant-text\\\",\\\"prompt\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., ‚ÄúWhich SDK are you using - Python or JS/TS?‚Äù).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can‚Äôt confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* ‚Üí *Setup* ‚Üí *Minimal Example* ‚Üí *Run* ‚Üí *Troubleshooting* ‚Üí *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ‚â§ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"version\\\":10,\\\"isFallback\\\":false,\\\"tags\\\":[],\\\"labels\\\":[\\\"production\\\",\\\"latest\\\"],\\\"type\\\":\\\"text\\\",\\\"config\\\":{\\\"model\\\":\\\"gpt-5\\\",\\\"reasoningSummary\\\":\\\"detailed\\\",\\\"textVerbosity\\\":\\\"low\\\",\\\"reasoningEffort\\\":\\\"low\\\"}}\"",
      "modelParameters": null,
      "completionStartTime": null,
      "promptId": null,
      "createdAt": "2025-09-19T06:51:19.874Z",
      "updatedAt": "2025-09-19T06:51:19.875Z",
      "usageDetails": {},
      "costDetails": {},
      "providedCostDetails": {},
      "model": null,
      "internalModelId": null,
      "promptName": null,
      "promptVersion": null,
      "latency": 0,
      "timeToFirstToken": null,
      "inputCost": null,
      "outputCost": null,
      "totalCost": 0,
      "inputUsage": 0,
      "outputUsage": 0,
      "totalUsage": 0,
      "internalModel": null
    }
  ]
}
